        <!DOCTYPE html>
        <html lang="en">
        <head><title>A discussion on virtual-memory topics [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/705758/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/705482/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/705758/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>A discussion on virtual-memory topics</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>November 9, 2016</br>
           <hr>
<a href="/Articles/KernelSummit2016/">2016 Kernel Summit</a>
</div>
The Kernel Summit "technical topics" day was a relatively low-key affair in
2016; much of the interesting discussion had been pulled into the adjoining
Linux Plumbers Conference instead.  That day did feature one brief
discussion involving several core virtual-memory (VM) developers, though.  While
a number of issues were discussed, the overall theme was clear: the growth of
persistent memory is calling into question many of the assumptions on which
the kernel's VM subsystem was developed.
<p>
The half-hour session begin with self-introductions from the four
developers at the head of the 
room.  Mel Gorman, who works in the performance
group at SUSE, said that his current interest is in improving small-object
allocation, a task which he hopes will be relatively straightforward.  Rik
van Riel has spent much of his time on non-VM topics recently, but has a
number of projects he wants to get back to.  Those include using the
persistent-memory support in QEMU to shift the page cache out of
virtualized guest systems and into the host, where it can be better managed
and shared.  Johannes Weiner has been working on scalability issues, and is
working on improving page-cache thrashing detection.  Vlastimil Babka is
concerned with high-order (larger than one page) allocations.  With the
merging of the <a href="/Articles/692208/">virtually mapped kernel
stacks</a> work, stack allocation no longer requires high-order allocations, but
there are other users in the kernel.  So his current focus is on making the
compaction and anti-fragmentation code work better.
<p>
<blockquote>
<a href="/Articles/705761/"><img src="https://static.lwn.net/images/conf/2016/ks/vm-group-sm.jpg"
alt="[The virtual memory panel]" title="Rik van Riel, Mel Gorman, Johannes
Weiner, Vlastimil Babka" border=0 class="photo"></a>
</blockquote>
<p>
Mel opened the discussion by asking whether anybody in the room had
problems with the VM subsystem.  Laura Abbott replied that she is working
on slab sanitization as a way of hardening the kernel; it zeroes memory
when it is freed to prevent memory leaks.  The initial code came from the
PaX/grsecurity patch set, but that code is not acceptable in mainline due
to the performance 
hit it adds to the memory allocator's hot paths.  She has been pursuing a
suggestion to use the SLUB allocator's debug path, but there is little joy
to be found there; SLUB's slow path is <i>very</i> slow.
<p>
Mel replied that he would expect that the performance hit from sanitization
would be severe in any case; thrashing the memory caches will hurt even if
nothing 
else does.  If a particular environment cares enough about security to want
sanitization, it will have to accept the performance penalty; this would
not be the first time that such tradeoffs have had to be made.  That said,
he has no fundamental opposition to the concept.  Laura does believe that
the hit can be reduced, though; much of the time spent in the slow path
goes to lock contention, so perhaps lockless algorithms should be
considered.  Mel concurred, noting that the slow path was misnamed; it
should be the "glacial path."
<p>
<h4>Swap woes</h4>
<p>
The next topic was brought up by a developer who is working on
next-generation memory, and swapping to hardware-compressed memory in
particular.  In the absence of the actual hardware, he is working on
optimizing swapping to a RAM disk using zram.  There are a number of
problems he is running into, including out-of-memory kills while there is
still memory available.  But he's concerned about performance; with zram,
about ⅔ of the CPU's time is spent on compression, while the other ⅓ is
consumed by overhead.  When the compression moves to hardware, that ⅓ will
be the limiting factor, so he would like to find ways to improve it.
<p>
Johannes replied that there are a lot of things to fix in the swap path
when fast devices are involved, starting with the fact that the swapout
path uses global locks.  Because swapping to rotational devices has always
been terrible, the system is biased heavily against swapping in general.  A
workload can be thrashing the page cache, but the VM subsystem will still
only reclaim page-cache pages and not consider swapping.  He has been
working on <a href="/Articles/690079/">a patch set</a> to improve the
balance between swapping and the page cache; it tries to reclaim memory
from whichever of the two is thrashing the least.  There are also problems
with the swapout path splitting huge pages on the way out, with a
consequent increase in overhead.  Adding batching to the swap code will
hopefully help here.
<p>

Mel suggested the posting of profiles showing where the overhead is in the
problematic workload.  Getting representative workloads is hard
for the VM developers; without those workloads, they cannot easily
reproduce or address the problems.  In general, he said, swapping is
"running into walls" and needs to be rethought.  Patience will be required,
though; it could be 6-24 months before the problems are fixed.
<p>
<h4>Shrinker shenanigans</h4>
<p>
Josef Bacik is working, as he has for years, on improving the Btrfs
filesystem.  He has observed a problematic pattern: if the system is using
vast amounts of slab memory, everything will bog down.  He has workloads
that can fill 80% of memory with cached inodes and dentries.  The size of
those caches should be regulated by the associated shrinkers, but that is not
working well.  Invocation of shrinkers is tied to page-cache scanning,
but this workload has almost no page cache, so that scanning is not
happening and the shrinkers are not told to free as much memory as they
should.  As more subsystems use the 
shrinker API, he said, we will see more cases where it is not working as
desired.
<p>
Ted Ts'o said that he has seen similar problems with the extent status slab
cache in the ext4 filesystem.  That cache can get quite large; it can also
create substantial spinlock contention when multiple shrinkers are running
concurrently.  The problems are not limited to Btrfs, he said.
<p>
Rik asked whether it would make sense to limit the size of these caches to
some more reasonable value.  There are quite a few systems out there now
that do not really have a page cache, and their number will grow as the use
of persistent memory spreads.  Persistent memory is nice in that it can
make terabytes worth of file data instantly accessible, but that leads to
the storing of a lot of metadata in RAM.
<p>
Christoph Hellwig replied that blindly limiting the size of metadata caches
is not a good solution; it might be a big hammer that is occasionally
needed, but it should not be relied upon in a well-functioning system.
What is needed is better balancing, he said, not strict limits.  The VM
subsystem has been built around the idea that filesystems store much of
their metadata in the page cache, but most of them have shifted that
metadata over to slab-allocated memory now.  So, he said, there needs to be
more talk between the VM and filesystem developers to work out better
balancing mechanisms.
<p>
Rik answered that the only thing the VM code can do now is to call the
shrinkers.  Those shrinkers will work through a global list of objects and
free them, but there is a problem.  Slab-allocated objects are packed many
to a page; all objects in a page must be freed before the page itself can
be freed.  So, he said, a shrinker may have to clear out a large fraction
of the cache before it is able to free the first whole page.  The cache is
wiped out, but little memory is made available to the rest of the system.
<p>
Christoph said that shrinkers are currently designed around a
one-size-fits-all model.  There needs to be a way to differentiate between
clean objects (which can be freed immediately) and dirty objects (that must
be written back to persistent store first).  There should also be
page-based shrinkers that can try to find pages filled with clean objects
that can be quickly freed when the need arises.
<p>
Mel suggested that there might be a place for a helper that a shrinker can
call to ask for objects that are on the same page; it could then free them
all together.  The problem of contention for shrinker locks could be
addressed by limiting the number of threads that can be running in direct
reclaim at any given time.  Either that, or shrinkers should back off
quickly when locks are unavailable on the assumption that other shrinkers
are running and will get the job done.
<p>
Ted said that page-based shrinkers could make use of a shortcut by which
they could indicate that a particular object is pinned and cannot be
freed.  The VM subsystem would then know that the containing page cannot be
freed until the object is unpinned.  Jan Kara suggested that there could be
use in having a least-recently-used (LRU) list for slab pages to direct reclaim
efforts, but Linus Torvalds responded that such a scheme would not work
well for the 
dentry cache, which is usually one of the largest caches in the system.
<p>
The problem is that some objects will pin others in memory; inodes are
pinned by their respective dentries, and dentries can pin the dentries
corresponding to their parent directories.  He suggested that it might
make sense to segregate the dentries for leaves (ordinary files and such)
from those for directories.  Leaf dentries are much easier to free, so
keeping them together will increase the chances of freeing entire pages.
There's just one little problem: the kernel often doesn't know which type a
dentry will be when it is allocated, so there is no way to know where to
allocate it.  There are places where heuristics might help, but it is not
an easy problem.  Mel suggested that the filesystem code could simply
allocate another dentry and copy the data when it guesses wrong; Linus said
that was probably doable.
<p>
<h4>Some final details</h4>
<p>
Linus said that there is possible trouble coming with the merging of slab
caches in the SLUB allocator.  SLUB normally does that merging for objects of
similar size, but many developers don't like it.  Slab merging would also
obviously complicate the task of freeing entire pages.  That merging
currently doesn't happen when there is a shrinker associated with a cache,
but that could change in the future; disabling merging increases the memory
footprint considerably.  We need to be able to do merging, he said, but
perhaps need to be more intelligent about how it is done.
<p>
Tim Chen talked briefly about his <a href="/Articles/704478/">swap
optimization work</a>.  In particular, he is focused on direct access to
swap when persistent memory is used as the swap device.  Since persistent
memory is directly addressable, the kernel can map swapped pages into a
process's address space, avoiding the need to swap them back into RAM.
There will be a performance penalty if the pages are accessed frequently,
though, so some sort of decision needs to be 
made on when a page should be swapped back in.  Regular RAM has the LRU
lists to help with this kind of decision, but all that is available for
persistent memory is the "accessed" bit in the page-table entry.
<p>
Johannes pointed out that the NUMA code has a page-table scanner that uses
the accessed bit; perhaps swap could do something similar, but Rik said
that this mechanism is too coarse for swap use.  Instead, he said, perhaps
the kernel could use the system's performance-monitoring unit (PMU) to
detect situations where pages in persistent memory are being accessed too
often.  The problem with that approach, Andi Kleen pointed out, is that
developers generally want the PMU to be available for performance work;
they aren't happy when the kernel grabs the PMU for its own use.  So it's
not clear what form the solution to this problem will take.
<p>
All of the above was discussed in a mere 30 minutes.  Mel closed the
session by thanking the attendees, noting that some good information had
been shared and that there appeared to be some low-hanging fruit that could
be addressed in the near future.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Conference_sessions">Memory management/Conference sessions</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Kernel_Summit-2016">Kernel Summit/2016</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/705758/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor706167"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A discussion on virtual-memory topics</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 11, 2016 0:44 UTC (Fri)
                               by <b>JanC_</b> (guest, #34940)
                              [<a href="/Articles/706167/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;  Laura Abbott replied that she is working on slab sanitization as a way of</font><br>
<font class="QuotedText">&gt; hardening the kernel; it zeroes memory when it is freed to prevent memory</font><br>
<font class="QuotedText">&gt; leaks.</font><br>
<p>
<font class="QuotedText">&gt; Mel replied that he would expect that the performance hit from sanitization</font><br>
<font class="QuotedText">&gt; would be severe in any case; thrashing the memory caches will hurt even if</font><br>
<font class="QuotedText">&gt; nothing else does.</font><br>
<p>
Isn't it possible to zero memory without using the CPU?  At least on some architectures?  Maybe using DMA or the memory controller or something like that?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/706167/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor706276"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A discussion on virtual-memory topics</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 11, 2016 19:31 UTC (Fri)
                               by <b>excors</b> (subscriber, #95769)
                              [<a href="/Articles/706276/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That's certainly possibly on some architectures, but I suspect it may not be worthwhile on many of them.<br>
<p>
I think DMA is usually not cache-coherent with the CPUs, so you'd have to flush the relevant lines from the CPU cache before zeroing the RAM. That will have some cost even if the cache is empty (I think many architectures require one flush instruction per cache line, you can't pass them an arbitrary range), and if the whole range is dirty in the cache then you'll end up doing twice as many writes to RAM (wasting bandwidth and power). Then if you immediately reallocate and reuse that range, it won't be in the cache and you'll have the extra cost of reading it all back from RAM.<br>
<p>
The overhead of sending control messages to the DMA engine and waiting for completion might be significant if you're only processing 4KB pages. Larger DMA transactions should reduce the overhead, but might not be possible if RAM is very fragmented. There's also a risk of contention with other users of the DMA hardware, which might be copying huge chunks of memory that won't finish any time soon.<br>
<p>
On some systems a single CPU core is able to saturate the memory bandwidth by itself, so DMA might not complete faster anyway.<br>
<p>
And if you want to avoid cache pollution, some CPUs have non-temporal stores that will avoid pulling lines into the cache unnecessarily but will still guarantee cache consistency, which might be a reasonable approach here.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/706276/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2016, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
