        <!DOCTYPE html>
        <html lang="en">
        <head><title>NUMA nodes for persistent-memory management [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/787418/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/787286/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/787418/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>NUMA nodes for persistent-memory management</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 6, 2019</br>
           <hr>
<a href="/Articles/lsfmm2019/">LSFMM</a>
</div>
While persistent memory is normally valued for its persistence, there is
also a subcurrent of interest in using it in settings where persistence is
not important.  In particular, the fact that this memory is relatively
inexpensive makes it appealing to use instead of ordinary RAM in
budget-conscious settings.  At the 2019 Linux Storage, Filesystem, and
Memory-Management Summit, two sessions in the memory-management track
looked at how the kernel's NUMA mechanism could be pressed into service to
manage non-persistent uses of persistent memory. 
<p>
<h4>Persistent memory as RAM</h4>
<p>
Yang Shi led the first session, which was focused on
a scheme to use persistent memory configured into CPUless NUMA nodes as RAM
for low-cost virtual machines.  The idea seems to have some merit, but
there are a number of details to be worked out yet.
<p>
The motivation behind this work is simple enough: offer cheaper virtual
machines for customers who are willing to put up with slower memory.  It is
an idea that has been <a href="/Articles/777212/">making the rounds</a> for

<a href="/Articles/787431/"><img
src="https://static.lwn.net/images/conf/2019/lsfmm/YangShi-sm.jpg" alt="[Yang Shi]"
title="Yang Shi" class="rthumb"></a>

some months already.  Ideally, it would be possible to give each virtual
machine a certain amount of RAM for its hottest data, while filling in the
rest with persistent memory, maintaining a set ratio of the amounts of each
type of memory.  Virtual machines would preferentially use their available
RAM up to their quota, after which data would spill over into persistent
memory; the kernel would migrate data back and forth in an attempt to keep
the most frequently used memory in RAM.
<p>
Needless to say, there are some challenges involved in implementing a
scheme like this.  Workloads can have random and frequently changing access
patterns that are hard to keep up with.  Maintaining the right ratio
between the two types of memory will require continuous scanning of the
virtual machine's address space.  It's even hard to define a service-level
agreement so that customers know what sort of performance to expect.
<p>
Michal Hocko asked what sort of API was envisioned to control this
memory-type ratio.  There is no control planned for customers;
administrators would have a <tt>/proc</tt> file where the ratio could be
set.  Customers would be allowed some influence over where data is placed
by using the <tt>madvise()</tt> system call to indicate which pages should
be in fast memory.  Dave Hansen asked about that aspect of things; given
that the memory types will be split into different NUMA nodes, the existing
NUMA mechanisms could be used to control placement.  Mel Gorman agreed,
saying that, if the kernel's NUMA awareness could be augmented to allow the
specification of a percentage-based allocation strategy, the needed
flexibility would be there.
<p>
Hocko then wondered about explicit placement of pages, something that user
space can request now.  If the kernel then tries to shuffle things around,
the situation could get messy.  He would like to avoid reproducing the sort
of problems we see now with CPU sets conflicting with explicit NUMA
placement, which he described as a "giant mess".
<P>
Gorman suggested that perhaps the memory controller could be augmented with
per-node counters, though it would be a non-trivial problem to get right.
It's not clear how memory reclaim could be handled.  But it is important to
realize, he said, that there are two different problems to be solved here:
implementing the memory ratio and migrating pages between memory types.
The first step is to get the accounting right so that the ratio can be
implemented; after that, it will make sense to worry about locality.
Proper accounting will require tracking memory usage on a per-node,
per-process (or, more likely, per-control-group) basis, which the kernel
doesn't do now.  When this is implemented, he said, it's important to not
make it specific to persistent memory; this mechanism could
be used to express policies for different classes of memory in general.
<p>
The discussion wandered around the accounting issue for some time without
making any real progress, but it was agreed that getting the accounting
working was an important first step.  Control groups already have some of
the needed support; finishing the job should be feasible.  The
data-migration task might prove harder, though; Hocko said that it would
probably have to be implemented in user space.  Gorman added that, once the
accounting is available, the kernel provides everything that is needed to
implement a brute-force migration solution in user space, though he
described it with terms like "brutal" and "horrible".
<p>
That still leaves the problem of kernel memory, though, which is not
accounted in the same way and which cannot generally be migrated at all,
much less from user space.  Where, Hansen asked, would the dentry cache live on
such a system?  Putting it into slow memory would be painful.  Hansen
argued for supporting placement in a general way, for now.  He also noted
that the memory situation is becoming more complicated; slow memory may
have fast caches built into it, for example.  Christoph Lameter mentioned
the possibility of high-end memory of the type currently found on GPUs
coming to CPUs in the near future.  The kernel will need as much
flexibility as possible to be able to handle the more complex memory
architectures on the horizon.
<p>
The discussion returned to the ratio-enforcement problem, with Gorman
repeating that control groups offer some of the needed statistics now.
Hocko agreed, saying that this support is not complete, but it's a place to
start.  The charging infrastructure can be made more complete, and
kernel-memory accounting could be added.  If the result turns out not to be
usable for this task, it least it would be a starting point from which
developers could figure out what the real solution should look like.
Gorman cautioned against premature optimism, noting the previous ideas
along these lines have not succeeded.
<p>
This kind of policy would bring some new challenges to the accounting code,
Hocko said.  The current memory controller works by first allocating memory
in response to a request, then attempting to charge the memory to the
appropriate control group.  If the charge fails (the group is above its
limit), the newly allocated memory is freed rather than handed over and the
allocation request fails.  In a ratio-based scheme, a charging failure
would have to lead to an allocation being retried with a different policy
instead.
<p>
Gorman said that might not necessarily be the case; instead, if a group has
moved away from its configured ratio, memory could be reclaimed from that
group to bring things back into balance.  There would be problems with such
a solution, but it would be a place to start.  Dan Williams, instead,
suggested a scheme where allocations come from a random node, with the
choice being biased toward the desired ratio.  The session ran out of time
at this point, ending with no conclusions but with a number of ideas for
developers to try.
<p>
<h4>Persistent memory in NUMA nodes</h4>
<p>
At this point, leadership of the discussion shifted over to Hocko and the
topic moved to the use of the NUMA infrastructure for the management of
persistent memory in general.  There are schemes floating around to do
things like configure persistent memory as a "distant" NUMA node and
implement proactive reclaim algorithms that would automatically move cold
pages over to that "node", which would be hidden from the memory-management
subsystem otherwise.  Many of these proposals, Hocko said, make the mistake
of treating persistent memory as something special — an approach that he
has been pushing back on.  They risk creating a solution that works for
today's technology but not tomorrow's.
<p>
Instead, he would really rather just think in terms of NUMA workloads; the
system is already built to handle memory with different performance
characteristics, after all.  Perhaps all that is really needed is to create a

<a href="/Articles/787432/"><img
src="https://static.lwn.net/images/conf/2019/lsfmm/MichalHocko-sm.jpg" alt="[Michal Hocko]"
title="Michal Hocko" class="lthumb"></a>

new memory-reclaim mode that migrates memory rather than reclaiming its
pages; the NUMA balancing code could then handle the task of ensuring that
pages are in the right places.
<p>
Hansen wondered how the ordering of nodes would be handled.  Currently,
NUMA balancing works in terms of distance — how long it takes to access
memory on any given node.  But that might not be optimal for the sorts of
things people want to do with persistent memory.  Perhaps the kernel needs
to create a fallback list of places a page could be put, using the node
distance as the metric initially.  Gorman, though, said that was too
complicated; memory placement should fall back to exactly one node at the
outset.  That would avoid traps like page-migration cycles, where pages
would shift around the system but never find a permanent home.
<p>
Hocko asked if the migration-based reclaim mode seemed like a good idea; it
could be implemented without big changes in a simple mode to see how well
it works.  All that would be needed, he said, would be a hook into one
place in the reclaim code.  Gorman thought it made sense, but he
repeated that there should only be one node to which pages could be
migrated.  If all nodes in the system were on a fallback list, a page would
have to move through every possible option — each RAM-based node and each
persistent-memory node — before actually being reclaimed.  It would be
necessary to maintain the history of where each page has been, and would be
likely to disrupt other workloads on the system.
<p>
Hansen thought that perhaps the problem could be addressed by creating
directional paths between nodes, so that pages would only migrate in one
direction.  Hocko said that there was no real need for that, since the
kernel would not move pages from a CPUless persistent-memory node to a RAM
node, but Gorman argued that this was an unfortunate application of
special-casing.  One should not, he said, assume that persistent-memory
nodes will never have CPUs in them.
<p>
Once migration is solved, there is still the problem of moving pages in the
other direction: how will frequently used pages be promoted to fast memory?
Gorman said that the NUMA-balancing code does that now, so things should
just work, but Rik van Riel argued that NUMA balancing would not work in
this case; when pages are stored on a CPUless node, <i>all</i> accesses
will be remote, so NUMA balancing will naturally pull every page off of
that node.  Gorman conceded the point, but said that this behavior might be
reasonable; pages that are being used should move to faster memory, but Van
Riel said that there is no way to tell between occasional uses and frequent
uses.
<p>
Hocko was less worried about this scenario.  A page that has found its way
to a persistent-memory node has been demoted there by the memory-management
code, meaning that it fell off the far end of the least-recently-used
list.  That means it hasn't been used for a while and is unlikely to be
accessed again unless the access pattern changes.  But Van Riel was still
worried that the system could thrash through the demoted memory, and that
some sort of rate limiting might be needed.  Gorman said that there may be
a need to alter the two-reference rule that is currently used to move pages
between NUMA nodes, but that is not a radical change; the idea of using
NUMA balancing is still sound, he said.
<p>
The session wound down with a general agreement that the migrate-on-reclaim
mode was an idea worth pursuing, and that relying on NUMA balancing to
promote pages back to RAM should be tried.  Once the simplest solution has
been implemented it can be observed with real workloads; developers can go
back and improve things if this approach behaves poorly.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Nonvolatile_memory">Memory management/Nonvolatile memory</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2019">Storage, Filesystem, and Memory-Management Summit/2019</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/787418/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor787646"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 6, 2019 21:44 UTC (Mon)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/787646/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
Dan Williams, instead, suggested a scheme where allocations come from a random node, with the choice being biased toward the desired ratio.
</blockquote>
Wouldn't this lead to random pages of the customer workload being allocated from the slow memory? This seems unlikely to be what anyone wants -- though if you wait long enough the frequently-used pages would be NUMA-balanced back to fast memory, I suppose. But it does mean that startup would lead to perhaps 50% of the active pages being moved to or from memory that was, after all, specifically chosen because it was slow (and all those moves would of course be moves the process would be blocked on). This seems undesirable for performance.
      
          <div class="CommentReplyButton">
            <form action="/Articles/787646/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787665"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 6, 2019 23:41 UTC (Mon)
                               by <b>djbw</b> (subscriber, #78104)
                              [<a href="/Articles/787665/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
If the guest needs guarantees about memory speed and is explicitly managing fast/slow then yes, of course this will not work. However, as I understood the problem the hypervisor is making a performance vs capacity tradeoff and the guest has no direct control. With those assumptions biased allocations might achieve the same effect as active capacity management.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787665/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor787671"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 5:01 UTC (Tue)
                               by <b>zev</b> (subscriber, #88455)
                              [<a href="/Articles/787671/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I imagine this was sufficiently obvious (or sufficiently well-trodden ground) to the people involved in the sessions in question here that it didn't merit explicit discussion, but for the sake of laypersons among us for whom that's not the case: how would these NUMA-based approaches compare to a simple swap-based arrangement?<br>
<p>
A brd device over a persistent memory region could presumably be plugged in as a swap device completely transparently, and should provide the general property of shuffling infrequently-accessed pages out to pmem and keeping the working set in DRAM.  Aside from dragging the block layer into the access path somewhat spuriously, would that have some major disadvantage?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787671/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787675"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 6:35 UTC (Tue)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/787675/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You're right that it's well-trodden ground.  The Linux swap code is not in good shape, and few people are willing to give it the thorough overhaul it needs.<br>
<p>
One of the new features it would need is the ability to leave the page on the swap device and allow DAX-style accesses to it (rather than page it back in to DRAM)<br>
<p>
It's a lot of work, and it's not clear that it's feasible.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787675/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor787698"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 13:51 UTC (Tue)
                               by <b>Baughn</b> (subscriber, #124425)
                              [<a href="/Articles/787698/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I'm curious about the specific type of hardware being used here. If I wanted to experiment with it myself, what should I look at?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787698/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787731"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 15:19 UTC (Tue)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/787731/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You can rent NVDIMM or Optane DIMM machines on public clouds these days. QEMU also has a NVDIMM emulation mode that works quite well for initial investigations. For more on that check out the SNIA Persistent Memory Hackathon information on GitHub and the PMDK wiki. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787731/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787787"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2019 6:03 UTC (Wed)
                               by <b>nilsmeyer</b> (guest, #122604)
                              [<a href="/Articles/787787/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Where? I presume to properly test this one needs access to bare metal, that leaves out most proprietary cloud firms. Some have announced persistent memory, but I have yet to see it in the wild. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787787/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor787742"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 17:35 UTC (Tue)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/787742/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<a href="https://www.tomshardware.com/news/intel-optane-dimm-pricing-performance,39007.html">https://www.tomshardware.com/news/intel-optane-dimm-prici...</a><br>
<p>
There really needs to be an asterisk by the capacities though, and I pointed this out when I worked there.  That is the total amount of memory on the DIMM.  You don't get it all; some of it is reserved for the DIMMs own use.  You will get more usable bytes if you buy a 128GB DRAM DIMM than a 128GB Optane DIMM.  I mean, it's a fairly small percentage, but it's something people should be aware of.<br>
<p>
They'll probably say something like "well a gigabyte is 10^9 bytes, you shouldn't expect 2^30 bytes", but that's not how DIMMs work today.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787742/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor788075"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">NUMA nodes for persistent-memory management</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2019 15:56 UTC (Fri)
                               by <b>droundy</b> (subscriber, #4559)
                              [<a href="/Articles/788075/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It seems to me that persistent memory has another advantage besides price: power usage.  Unlike DRAM, persistent memory has no (or little) power use when the data is not being accessed. This suggests that it could benefit laptops, which leads to another possible synergy which could benefit from the persistence: a possibility of a very performant hibernation: if all memory were migrated to persistent memory then on reboot the system could begin running as soon as kernel memory has been copied back to RAM, and user memory could gradually migrate back to RAM.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788075/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2019, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
