        <!DOCTYPE html>
        <html lang="en">
        <head><title>In-kernel memory compression [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/545244/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/544793/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/545244/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>In-kernel memory compression</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>We're bad at marketing</b>
<p>
We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please <a href="/Promo/nsn-bad/subscribe">subscribe today</a> to help us keep doing that, and so
we don’t have to get good at marketing.
</blockquote>
<div class="GAByline">
           <p>April 3, 2013</p>
           <p>This article was contributed by Dan Magenheimer</p>
           </div>
Amdahl's law tells us that there is always a bottleneck in any computing
system. Historically, 
the bottleneck in many workloads on many systems is the CPU and
so system designers have made CPUs faster and more efficient and
also continue to increase the number of CPU cores even in low-end
systems.  So now, increasingly, RAM is
the bottleneck; CPUs wait idly while data is moved
from disk to RAM and back again.  Adding
more RAM is not always a timely or cost-effective option and sometimes
not an option at all.  Faster I/O buses and solid-state disks reduce
the bottleneck but don't eliminate it.
<p>
Wouldn't it be nice if it were possible to increase the effective amount
of data stored in RAM?  And, since those CPUs are waiting anyway,
perhaps we could use those spare CPU cycles to contribute towards
that objective?  This is the goal of in-kernel compression:  We keep more
data — compressed — in RAM and use otherwise idle CPU cycles to execute
compression and decompression algorithms.
<p>
With the recent posting of <a href="/Articles/537422/">zswap</a>, there are
now three in-kernel compression 
solutions proposed for merging into the kernel memory management (MM)
subsystem: zram, zcache, and zswap.  While a casual observer might
think that only one is necessary, the three have significant differences
and may target different user bases.  So, just as there are many filesystems
today co-existing in the kernel, someday there may be multiple
compression solutions.  Or maybe not... this will be decided by Linus
and key kernel developers.  To help inform that decision,
this article compares and contrasts the different solutions, which
we will call, for brevity, the "zprojects".  We will first introduce
some of the key concepts and challenges of compression.  Then
we will identify three design layers and elaborate on the different
design choices made by the zprojects.  Finally, we will discuss how
the zprojects may interact with the rest of the kernel and then
conclude.
<p>
<h4>Compression basics</h4>
<p>
For in-kernel compression to work, the kernel must take byte sequences
in memory and compress them, and then keep the compressed version in RAM
until some point in the future when the data is needed again.
While the data is in a compressed state, it is impossible to
read any individual bytes from it or write any individual bytes
to it.   Then, when the data is needed again,
the compressed sequence must be decompressed so that individual bytes
can again be directly accessed. 
<p>
It is possible to compress any number of sequential bytes but
it is convenient to use a fixed "unit" of compression.  A
standard storage unit used throughout the kernel is a "page"
which is made up of a fixed constant PAGE_SIZE bytes (4KB on
most architectures supported by Linux).  If this page is aligned at a
PAGE_SIZE address boundary, it is called a "page frame";
the kernel maintains a corresponding "<tt>struct&nbsp;page</tt>" for every
page frame in system RAM.  All three zprojects use a page as
the unit of compression and allocate and manage page frames
to store compressed pages.
<p>
There are many possible compression algorithms.   In general,
achieving a higher "compression ratio" takes a larger number
of CPU cycles, whereas less-effective compression can be done faster.
It is important to achieve a good balance between time and
compression ratio.  All three zprojects, by default, use the
LZO(1X) algorithm in the kernel's <tt>lib/</tt> directory, which is
known to deliver a good balance.  However, it is important that
the choice of algorithm remain flexible; possibly the in-CPU
algorithm might be replaced in some cases by an architecture-specific
hardware compression engine. 
<p>
In general, the number of cycles required to compress, and to
later decompress, a sequence of bytes is roughly proportional
to the number of bytes in the sequence.  Since the size of a page
is fairly large, page compression and decompression are expensive
operations and, so, we wish to limit the number of those operations.  Thus
we must carefully select which pages 
to compress, choosing pages that are likely to be used again but
not likely to be used in the near future, lest we spend all the
CPU's time repeatedly compressing and then decompressing pages.
Since individual bytes in a compressed page are inaccessible,
we must not only ensure that normal CPU linear byte addressing
is not attempted on any individual byte, but also ensure that the
kernel can clearly identify the compressed page so that it can
be found and decompressed when the time comes to do so.
<p>
When a page is compressed, the compression algorithm is applied
and the result is a sequence of bytes which we can refer to as a
"zpage".
The size of a zpage, its "zsize", is highly data-dependent,
hard to predict, and highly variable.  Indeed the expected distribution of
zsize drives a number of key zproject design decisions so it
is worthwhile to understand it further.  The "compression ratio"
for a page is zsize divided by PAGE_SIZE.  For nearly all pages,
zsize is less than PAGE_SIZE so the compression ratio is less than one,
but odd cases may occur where compression actually results in
the zsize being larger than PAGE_SIZE.  A good compression solution must
have a contingency plan to deal
with such outliers.  At the other extreme, a data page containing
mostly zeroes or ones may compress by a factor of 100x or more.
For example, LZO applied to an all-zero page results in a
zpage with zsize equal to 28, for a compression ratio of 0.0068.
<p>
As a rough rule of thumb, "on average", compression has a ratio of about
0.5 (i.e. it reduces a page of data by about a factor of two),
so across a wide set of workloads, it is useful to envision the distribution
as a bell curve, centered at PAGE_SIZE/2.
We will refer to zpages with zsize less than PAGE_SIZE/2 as "thin"
zpages, and zpages with zsize greater than PAGE_SIZE/2 as "fat" zpages.
For any given workload, of course, the distribution may "skew thin",
(if many mostly-zero pages are compressed, for example) or "skew fat",
as is true if the data is difficult to compress (already-compressed data
like a JPEG image would be an example).
A good compression
solution must thus be able to handle zsize distributions from a wide range of
workloads.
<p>
With this overview and terminology, we can now compare and
contrast the three zprojects.  At a high level, each uses
a "data source layer" to provide pages to compress and a
"data management layer" to organize and store the zpages.
This data management layer has two sublayers, the first
for data organization, which we will call the "metadata layer",
and the second for determining how best to utilize existing
kernel memory and kernel allocation code (such as <tt>alloc_page()</tt>)
to optimally store the zpages, which we will call the
"zpage allocator layer".
<p>
<h4>Data source layer — overview</h4>
<p>
As previously noted, the nature of compression limits the
types and quantities of pages to which it can be applied.
It doesn't make sense to compress a page for which there is
a high probability of imminent reuse.  Nor does it make
sense to compress a page for which there is a high probability
the data it contains will never again be reused.  All three
zprojects assume one or more data sources that provide
a "pre-qualified" sequence of data pages.  In one zproject,
an existing kernel subsystem is the data source.  For
the other two zprojects, previous kernel hooks added as part of
the "<a href="/Articles/454795/">transcendent memory</a>" projects
known as cleancache and frontswap,
source the data page stream.
<p>
Each data source must also provide a unique identifier,
or "key", for each page of data so that the page, once stored
and associated with that unique key, can be later retrieved by
providing the key.
It's also important to note that different data sources
may result in streams of pages with dramatically different
contents  and thus, when compressed, different
zsize distributions.
<p>
<h4>Swap/anonymous pages as a data source</h4>
<p>
The Linux swap subsystem provides a good opportunity for compression
because, when the system is under memory pressure, swap acts as a
gatekeeper between (a)&nbsp;frequently used anonymous pages which are kept
in RAM, and (b)&nbsp;presumably less frequently used anonymous pages that
"overflow", or can be "swapped", to a very-much-slower-than-RAM swap disk.
If, instead of using a slow swap disk, we can cause the gatekeeper to
store the overflow pages in RAM, compressed, we may be
able to avoid swapping them entirely.  Further, the swap subsystem already
provides a unique key for each page so that the page
can be fetched from the swap disk.
So, unsurprisingly, all three zprojects consider swap pages as
an ideal data source for compression.
<p>
One zproject, zram, utilizes the existing swap device model.  A zram swap
device is explicitly created and enabled in 
user space (via <tt>mkswap</tt> and <tt>swapon</tt>) and this zram device is prioritized
(presumably highest) against other configured swap devices.  When the swap subsystem sends
a data page to the zram device, the page goes through the
block I/O subsystem to the zram "driver".  All swap pages sent
to zram are compressed and associated with a key created by concatenating
the swap device identifier and page offset.  Later, when the swap subsystem
determines (via a page fault) that the page must be brought back
in from the swap device to fill a specific page frame, the zram
device is notified; it then decompresses the zpage matching the swap
device number and page offset into that page frame.
<p>
The other two zprojects, zcache and zswap, use the kernel's frontswap
hooks (available since Linux 3.5) in the swap subsystem.  Frontswap
acts as a "fronting store" — a type of a cache — to an existing
swap device.  It avoids the block I/O subsystem completely; 
swapped pages instead go directly to
zcache/zswap where they are compressed.  On the fetch path, when the
page fault occurs, the block I/O subsystem is again bypassed and then,
as with zram, the page is decompressed directly into the faulting page frame.
<p>
There are some subtle but important side effects of the different
approaches:
<p>
<ul>
<p>

<li> The block I/O subsystem is designed to work with fixed-size block
     devices and is not very happy when a block write results in a failure.
     As a result, when a poorly-compressible page ("very fat" zpage) is
     presented to zram, all PAGE_SIZE bytes of the uncompressed page are
     stored by zram in RAM, for no net space savings.  The frontswap-based
     zprojects have more flexibility; if a very fat zpage is presented,
     zcache/zswap can simply reject the request, in which case the swap subsystem
     will pass on the original data page to the real swap disk.
<p>

<li> Since zram presents itself as a swap disk, user-space configuration is
     required, but this also means that zram can work on systems with no
     physical swap device, a configuration common for embedded Linux
     systems.  On the other hand, zcache and zswap depend entirely on
     already-configured swap devices and so are not functional unless at
     least one physical swap device is configured and sized adequately.  As
     a result, one might think of zram as being a good match for an
     embedded environment, while the others are better suited for a more
     traditional server/data-center environment.

<p>
<li> It is important to note that all three zprojects must never discard
     any of these data pages unless explicitly instructed, otherwise
     user-space programs may experience data loss.  Since all three
     zprojects have a finite capacity on any given system, it is prudent to
     have a "pressure relief valve".  Both zcache and zswap have the
     ability to "writeback" data to the swapdisk to which the data was
     originally intended to be written; this cannot be done with zram.
</ul>
<p>
<h4>Clean page cache pages as a data source</h4>
<p>
It is not uncommon for the vast majority of page frames in a running
system to contain data pages that are identical
to pages already existing on
a disk in a filesystem.  In Linux, these pages are stored in
the page cache; the data in these pages is kept in RAM in anticipation
that it will be used again in the future.
When memory pressure occurs and the kernel is looking to free 
RAM, the kernel will be quick to
discard some of this duplicate "clean" data because it can be fetched from
the filesystem later if needed.
A "cleancache" hook, present in the kernel since Linux 3.0, can
divert the data in these page frames, sending the pages to zcache
where the data can be compressed and preserved in RAM. Then, if the
kernel determines the data is again needed in RAM, a page fault
will be intercepted by another cleancache hook
which results in the decompression of the data.
<p>
Because modern filesystems can be large, unique identification
of a page cache page is more problematic than is unique identification of
swap pages.  Indeed, the key provided via cleancache must uniquely
identify: a filesystem; an "exportable" inode value representing a file
within that filesystem; and a page offset within the file.  This
combination requires up to 240 bits.
<p>
Zcache was designed to handle cleancache pages, including the full
range of required keys.  As a result, the data management layer
is much more complex, consisting of a combination of different data
structures which we will describe below.  Further, the location
of some cleancache hooks in the VFS part of the kernel results
in some calls to the data management layer with interrupts disabled 
which must be properly handled.
<p>
Even more than with swap data, filesystem data can proliferate
rapidly and, even compressed, can quickly fill up RAM.  So,
as with swap data, it is prudent to have a pressure-relief valve
for page cache data.  Unlike swap data, however, we know that
page cache data can simply be
dropped whenever necessary.  Since zcache
was designed from the beginning to manage page cache pages, its
data management layer was also designed to efficiently handle
the eviction of page cache zpages.
<p>
Zram can maintain entire fixed-size filesystems in compressed form
in RAM, but not as a cache for a non-RAM-based filesystem.  In essence,
part of RAM can be pre-allocated as a "fast disk" for a filesystem;
even filesystem metadata is compressed and stored in zram.
While zram may be useful for small filesystems that can entirely fit
in RAM, the caching capability provided via cleancache combined with
the compression provided by zcache is useful even for large filesystems.
<p>
Zswap is singularly focused on swap and so does not make use of the
page cache at all or filesystem data as a data source.  As a result, its
design  is simpler because it can ignore the complexities required
to handle the tougher page cache data source.
<p>
<h4>The metadata layer</h4>
<p>
Once the zproject receives a data page for compression, it must
set up and maintain data structures so that the zpage can be stored
and associated with a key for that page, and then later found
and decompressed.  Concurrency must also be considered to ensure
that unnecessary serialization bottlenecks do not occur.  The three
zprojects use very different data structures for different reasons,
in part driven by the requirements of data sources to be maintained.
<p>
Since the number of swap devices in a system is small, and the
number of pages in each is predetermined and fixed, a small, unsigned
integer representing the swap device combined with a page offset is sufficient to identify
a specific page.  So zram simply uses a direct table lookup (one
table per configured zram swap device) to find and manage its data.
For example, for each data page stored, the zram table contains a
zsize, since zsize is useful for decompression validation, as well
as information to get to the stored data.  Concurrency is restricted,
per-device, by a reader-writer semaphore.
<p>
Zswap must manage the same swap-driven address space, but it utilizes
one dynamic <a href="/Articles/184495/">red-black tree</a> (rbtree) per swap device
to manage its 
data.  During tree access, a spinlock on the tree must be held; this
is not a bottleneck because simultaneous access within any one
swap device is greatly limited by other swap subsystem factors.
Zswap's metadata layer is intentionally minimalist.
<p>
Zcache must manage both swap data and pagecache data, and the latter
has a much more extensive key space which drives the size and
complexity of zcache data structures.  For each filesystem,
zcache creates a "pool" with a hash table of red-black tree root
pointers; each node in the rbtree represents a filesystem
inode — the inode space in an exportable filesystem is
often very sparsely populated which lends itself to management
by an rbtree.  Then each rbtree node contains the head of a <a
href="/Articles/175432/">radix 
tree</a> — the data structure used throughout the kernel for page
offsets — and this radix tree is used to look up a specific page
offset.  The leaf of the radix tree points to a descriptor which,
among other things, references the zpage.
Each hash table entry has a spinlock to manage concurrency; unlike
swap and frontswap, filesystem
accesses via cleancache may be highly concurrent so contention must
be aggressively avoided.
<p>
<h4>Impact of writeback on the metadata layer</h4>
<p>
Earlier it was noted that zcache and zswap support "writeback";
this adds an important twist to the data structures in that,
ideally, we'd like to write back pages in some reasonable order, such as
least recently
used (LRU).  To that end, both zcache and
zswap maintain a queue to order the stored data; zswap queues
each individual zpage and zcache queues the page frames used to store
the zpages.  While lookup requires a key and searches the data
structures from the root downward, writeback chooses one or more zpages from
the leaves of the data structure and then must not only remove or
decompress the data, but must also remove its metadata from the leaf upward.
This requirement has two ramifications: First, the leaf nodes of
data structures must retain the key along with information necessary
to remove the metadata.  Second, if writeback is allowed to execute
concurrently with other data structure accesses (lookup/insert/remove),
challenging new race conditions arise which must be understood
and avoided.
<p>
Zswap currently implements writeback only as a side effect of
storing data (when kernel page allocation fails) which limits the
possible race conditions.  Zcache implements writeback as a shrinker
routine that can be run completely independently and, thus,
must handle a broader set of potential race conditions.  Zcache must also
handle this same broader set when discarding page cache pages.
<p>
<h4>Other metadata layer topics</h4>
<p>
One clever data management technique is worth mentioning here:  Zram
deduplicates "zero pages" — pages that contain only zero bytes —
requiring no additional storage.  In some zsize distributions,
such pages may represent a significant portion of the zpages
to be stored.  While the data of the entire page may need to be
scanned to determine if it is a zero page, this overhead may
be small relative to that of compression and, in any case,
it pre-populates the cache with bytes that the compression algorithm
would need anyway.  Zcache and zswap should add this feature.
More sophisticated deduplication might also be useful.  Patches are
welcome.
<p>
<h4>Zpage allocator layer</h4>
<p>
Memory allocation for zpages can present some unique challenges.
First, we wish to maximize the number of zpages stored in a given
number of kernel page frames, a ratio which we will call "density".
Maximizing the density
can be challenging as we do not know <i>a priori</i> the number of
zpages to be stored, the distribution of zsize of those zpages,
or the access patterns which insert and remove those zpages.  As a result,
fragmentation can be a concern.  Second, allocating memory to store
zpages may often occur in an environment of high-to-severe memory
pressure; an effective allocator must handle frequent failure and
should minimize large (i.e. multiple-contiguous-page) allocations.  Third,
if possible, we wish the allocator to support some mechanism to enable
ordered writeback and/or eviction as described above.
<p>
One obvious alternative is to use an allocator already present and
widely used in the kernel: <tt>kmalloc()</tt>, which is optimized for
allocations that nicely fit into default sizes of 2<sup>N</sup> byte chunks.
For allocations of size 2<sup>N</sup>+x, however, as much as 50% of the
allocated memory is wasted.  <tt>kmalloc()</tt> provides an option for
"odd" sizes but this requires pre-allocated caches, often of
contiguous sets of pages ("higher order" pages) which are difficult
to obtain under memory pressure.  Early studies showed that <tt>kmalloc()</tt>
allocation failures were unacceptably frequent and density was insufficient,
so <tt>kmalloc()</tt> was discarded and other options were pursued, all based
on <tt>alloc_page()</tt>, which always allocates a single page frame.
<p>
To improve density, the xvmalloc allocator was written, based on
the two-level sequential fit (TLSF) algorithm.  Xvmalloc provided
high density
for some workloads and was the initial default allocator for zram and for
the frontswap-driven portion of the initial zcache.  It was found, however,
that the high density led to poor fragmentation characteristics,
especially for zsize distributions that skewed fat.  Xvmalloc has
since been discarded (though remnants of it survive through Linux 3.8.)
<p>
A new allocator, zbud, was written for the cleancache-driven portion of the
original zcache code.  The initial version of zbud had less focus on high
density and more on the ability to efficiently evict cleancache-provided
zpages.  With zbud, no more than two zpages are contained
in a page frame, buddying up a pair of zpages, one fat and one thin.
Page frames, rather than zpages, are entered into an LRU queue so
that entire page frames can be easily "freed" to the system.
On workloads where zsize skews thin, a significant amount of space
is wasted, but fragmentation is limited, and eviction is still easy.
<p>
Zsmalloc, an allocator intended to "rule them all", was then written.
Zsmalloc takes the best of <tt>kmalloc()</tt> but with the ability to provide
the complete range
of "size classes" suitable to store zpages; zsmalloc also never requires
higher-order allocations.  All zpages with zsize within
a "class" (e.g. between 33-48 bytes) are grouped together and stored
in the same "zspage".  A clever feature (especially useful for fat zpages)
allows discontiguous page frames to be "stitched" together, so that
the last bytes in the first page frame contain the first part of the zpage
and the first bytes of the second page frame contain the second part.
A group of N of these discontiguous pages are allocated on demand
for each size class, with N chosen by the zsmalloc code
to optimize density.  
<p>
As a result of these innovations, zsmalloc achieves great density
across a wide range of zsize distributions: balanced, fat, and thin.
Alas, zsmalloc's strengths also proves to be its Achilles heel:
high density and page crossings make it very difficult for eviction
and writeback to concurrently free page frames, resulting in a different
kind of fragmentation (and consequent lower density) and rendering it
unsuitable for  management of cleancache-provided pages in zcache.
So zbud was revised to utilize some of the techniques pioneered
by zsmalloc.
Zbud is still limited to two zpages per page frame, but when under
heavy eviction or writeback, its density compares favorably
with zsmalloc.  The decision by zcache to use zbud for swap pages
sadly led to a fork of zcache, first to "old zcache" (aka zcache1)
and "new zcache" (aka zcache2) and then to a separate zproject
entirely, zswap, because the author prefers the density of zsmalloc
over the ability to support cleancache pages and the predictability
and page frame-reclaim of zbud.
<p>
So, today, there are two zpage allocators, both still evolving.
The choice for best zpage allocator depends
on use models and unpredictable workloads.  It may be possible that
some hybrid of the two will emerge, or perhaps some yet unwritten
allocator may arise to "rule them all".
<p>
<h4>Memory management subsystem interaction</h4>
<p>
In some ways, a zproject is like a cache for pages temporarily removed
from the MM subsystem.  However it is a rather unusual cache in that,
to store its data, it steals capacity (page frames) from its backing
store, the MM subsystem.  From the point-of-view of the MM subsystem,
the RAM is just gone, just as if the page frames were absorbed by
a RAM-voracious device driver.  This is a bit unfortunate, given that
both the zproject and the MM subsystem are respectively, but separately,
managing compressed and uncompressed anonymous pages (and possibly
compressed and uncompressed page cache pages as well).
So it is educational to consider how
a zproject may "load balance" its memory needs with the MM subsystem,
and with the rest of the kernel.  
<p>
Currently, all three zprojects obtain page frames using the standard
kernel <tt>alloc_page()</tt>
call, with the "GFP flags" parameter chosen to ensure that the kernel's
emergency reserve pages are never used.
So each zproject must be (and is) resilient to the fairly frequent
situation where an <tt>alloc_page()</tt> call returns failure.
<p>
Zram makes no other attempt to limit its total use of page frames but,
as we've seen,
it is configured by the administrator as a swap device, and swap parameters do
limit the number of swap pages, and thus zpages, that can be accepted.
This indirectly
but unpredictably limits the number of page frames used.  Also,
the configuration is independent of and not cognizant of the total RAM
installed in the system, so configuration errors are very possible.
<p>
Zswap uses a slightly modified version of zsmalloc to track the
total number of page frames allocated.  It ensures this number does
not exceed a certain percent of total RAM in the system, and this
limit is configurable via sysfs.  As previously noted, using
writeback, zswap can reduce the number of (anonymous) zpages stored
and this, indirectly, can reduce the number of page frames,
though at the likely cost of fragmentation.
<p>
Zcache and zbud were designed from the beginning with much more dynamic
constraints on page frame utilization in mind, intended eventually to
flexibly mesh with the dramatically time-varying memory balancing algorithms
that the MM subsystem uses.
While the exact interaction model and best future control policy are not
yet determined, zcache's and zbud's underlying page frame-based writeback
and eviction mechanisms support a shrinker-like interface that can
be told to, for example, reduce the number of page frames used for
storing anonymous zpages from 10000 to 8937 and the number of page frames
used for storing page cache zpages from 3300 to 463, and zcache can do that.
Since zbud is much more predictable (two zpages per page frame), the
MM subsystem can estimate and manage both the total number of anonymous
and pagecache pages (compressed and uncompressed) in the system and the
total number of page frames used to manage them.
<p>
<h4>Status and conclusions</h4>
<p>
Zram, zcache, and zswap are all advancing the concept of in-kernel compression
in different ways.  Zram and zcache are in-tree but both still in
the staging tree.  While the staging tree has served to expose
the concept of in-kernel compression to a wide group of kernel
developers and even to the users of a few leading-edge distributions,
as Andrew Morton <a href="https://lkml.org/lkml/2013/1/29/565">says</a>,
the staging tree "<q>is where code goes to be ignored</q>".
Staging tree maintainer Greg Kroah-Hartman has become reluctant to accept any new
zproject functionality into the staging tree.  This creates a
conundrum for these zprojects:  evolution in the staging tree
has resulted in rapid improvements in the design and implementation
and exposure of the zprojects,  but because of this evolution
they are not stable enough for promotion into the core kernel
and further evolution is now stymied.
<p>
Zswap is attempting to circumvent the staging tree entirely by
proposing what is essentially a simplified frontswap-only fork of
zcache using zsmalloc instead of zbud, for direct merging into the
MM subsystem.   Since it is also much simpler than zcache, it has
garnered more reviewers
which is a valuable advantage for to-be-merged code.  But it is
entirely dependent on the still-in-staging zsmalloc, does not
support page cache pages at all, and does not support page-frame-based
writeback, so its simplicity comes at a cost.  If zswap is
merged, it remains to be seen if it will ever be extended
adequately.
<p>
So, in-kernel compression has clear advantages and real users.
It remains unclear though when (or if) it will be merged
into the core kernel and how it should interact with
the core kernel.  If you are intrigued, the various zproject
developers encourage your ideas and contributions.
<p>
<h4>Acknowledgments</h4>
<p>
Nitin Gupta was the original author of compcache and I
was the original author of transcendent memory.  While both projects
were initially targeted to improve memory utilization in a multi-tenant
virtualized environment, their application to compression in a single
kernel quickly became apparent.  Nitin designed and wrote the Linux
code for zram, xvmalloc, and zsmalloc, and wrote an early prototype
implementation of zcache.  I wrote frontswap and cleancache (with the
cleancache hook placement originally done by Chris Mason), and the
Linux code for zcache, zbud, and <a href="/Articles/481681/">ramster</a>.
Seth Jennings contributed 
a number of improvements to zcache and is the author of zswap.
Kernel mm developer Minchan Kim was a helpful influence for all the
zprojects, and none of the zprojects would have been possible without
Greg Kroah-Hartman's support — and occasional whippings — as
maintainer of the staging drivers tree.
<p>
As with any open-source project, many others have contributed ideas,
bug fixes, and code improvements and we are thankful for everyone's
efforts.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management">Memory management</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Transcendent_memory">Transcendent memory</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#zswap">zswap</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Magenheimer_Dan">Magenheimer, Dan</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/545244/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor545878"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 8:45 UTC (Thu)
                               by <b>SLi</b> (subscriber, #53131)
                              [<a href="/Articles/545878/">Link</a>] (9 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
LZO indeed delivers a good balance for this and many other use cases. It generally bothers me that people tend to treat compression algorithms in a one-size-fits-all fashion. Especially LZO tends to be underrated, but the important thing is choosing the right algorithm for the job.<br>
<p>
LZO excels in one thing, and one thing only: It is blazingly fast. Specifically it is fast enough that in most applications it is not the bottleneck. LZO-compressing 1 GiB of zeros (or mostly any other data) takes less than a second on a modern computer, while gzip takes closer to 10 seconds. The result, when compressing zeros, is about five times the size of the size of gzip output; generally expect much worse compression ratio for LZO compared to any slower algorithm.<br>
<p>
This has an interesting consequence: In very many cases it pays off to LZO-compress your data, even temporarily. With current disk and CPU speeds, it usually pays off to LZO-compress data before writing it to disk and uncompress it after reading: The time of reading the compressed data from the disk and uncompressing it is in most cases less than the time to read the uncompressed data from the disk. Another thing LZO (and the gzip-like filter program 'lzop') is great for is moving large pieces of data over the network. Even with modern CPUs and gzip you still are close to the point where gzip may end up being a bottleneck on a 100 Mbit/s network. Doing LZO compression on the sending end and decompression on the receiving end, on the other hand, is usually a pure win, unless your data is entirely uncompressible, in which case it hardly makes a difference.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/545878/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor545916"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 14:08 UTC (Thu)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/545916/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thanks for the interesting addition to the article!  As some have pointed out though, LZO doesn't seem to do well with text (~0.6 compression ratio as opposed to ~0.4 for gzip).  While the ~10x speed may still be worth it, it would be *really* nice if there were a way to do a quick analysis on the first N bytes of an uncompressed page and, from that analysis, adaptively select one of the several kernel compression algorithms to use.<br>
<p>
Separately re LZO, Markus Oberhumer (the "O" in "LZO") has provided an updated better LZO for the Linux kernel, but so far there has been no maintainer to volunteer to shepherd it through to merge it (including syntactic changes so that it gets by checkpatch).  If someone would step up and help with that (and serve as its maintainer), that would be very helpful to the longterm success of the zprojects.  Volunteers?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/545916/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor545930"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 14:53 UTC (Thu)
                               by <b>SLi</b> (subscriber, #53131)
                              [<a href="/Articles/545930/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
On the contrary, from the numbers you quote, it seems to me that LZO is doing great with text! I would have expected an even larger discrepancy between the compression ratios, something like 0.7 to 0.4, and my gut feeling is that 0.4 to 0.6 is as good as it gets for nearly any compressible data in a competition between LZO and gzip.<br>
<p>
LZO is just not optimized for compression ratio so much as for speed. That's why it's good we have both algorithms. One is good for cases where speed matters more, and the other is better for cases where compression ratio matters more.<br>
<p>
It's simply a question of what bounds your performance. The more you are constrained about I/O speed, whether disk or network or something else, the better choice a CPU-heavy algorithm is. But even with modern CPUs, gzip is going to tax your CPU at disk or network speeds, while lzop &lt;file1 &gt;file2 is generally faster than cp file1 file2 for most storage media if the file in question is compressible.<br>
<p>
Here are some quick numbers from a few rounds of compressing a kernel tarball (50.6 MB uncompressed):<br>
<p>
lzop: 1.62 seconds to compress, 18.0 MB, ratio=.355<br>
gzip: 19.13 seconds, 10.8 MB, ratio=.213<br>
bzip2: 60.81 seconds, 8.46 MB, ratio=.176<br>
xz: 311.65 seconds, 7.33 MB, ratio=.145<br>
...<br>
<p>
It's always possible to compress better by using more time, and you cannot just claim that one of the algorithms here is "better" than any of the others without reference to a specific scenario. Some algorithms are also designed to have significantly faster decompression than compression.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/545930/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor545976"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 15:58 UTC (Thu)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/545976/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Right... I was specifically referring to the impact in the context of the zprojects.  There's a big difference between whether a page compresses to a zsize of (PAGE_SIZE/2)-1 with algorithm A which is slow; vs a zsize of (PAGE_SIZE/2)+1 with algorithm B which is fast:<br>
In the former case, two zpages can be stored in one pageframe; in the latter, only three zpages can be stored in TWO pageframes.<br>
So if one could know these respective zsizes without running BOTH algorithms,  running A might be worth the extra cycles.<br>
Exact knowledge of zsizes is highly unlikely, but there might be some useful patterns that could drive adaptive algorithm selection.  OTOH, pattern analysis might take more cycles than the difference between A and B!<br>
<p>
Related, compression algorithms may have very different strengths and weaknesses when the uncompressed size is limited to a smaller value (such as PAGE_SIZE) as opposed to a larger value (such as the 50MB in a kernel tarball) so be careful drawing conclusions from the latter.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/545976/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor592545"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 31, 2014 6:28 UTC (Mon)
                               by <b>masta</b> (guest, #59664)
                              [<a href="/Articles/592545/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
IMHO a good solution would be to use a fast (LZO) algorithm first and then from time to time recompress the oldest pages with a higher compressing algorithm (gzip). This way the kernel could reclaim bigger chunks of memory fast to keep the system running without interruptions. But in the end it is able to carve out some more extra memory by putting some more compression effort into pages which will not be used for some time.<br>
<p>
Just my 2c<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/592545/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor546098"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 9:05 UTC (Fri)
                               by <b>epa</b> (subscriber, #39769)
                              [<a href="/Articles/546098/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<a href="http://code.google.com/p/snappy/">http://code.google.com/p/snappy/</a> looks like an interesting alternative to LZO.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546098/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546170"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 15:38 UTC (Fri)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546170/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
See Andi Kleen's comment at <a href="https://lkml.org/lkml/2012/1/24/498">https://lkml.org/lkml/2012/1/24/498</a> and the long thread before it.<br>
<p>
If snappy can be shown to be consistently better than the existing LZO (on PAGE_SIZE-sized streams, specifically anonymous pages and file pages) AND if it is merged into the kernel, the zprojects should be able to switch to it very easily.  Actually, snappy should also be compared to the not-yet-merged (see other comment about not having a LZO maintainer) newer version of LZO, see <a href="https://lkml.org/lkml/2012/1/18/232">https://lkml.org/lkml/2012/1/18/232</a><br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546170/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546698"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 9, 2013 22:29 UTC (Tue)
                               by <b>zt</b> (guest, #90306)
                              [<a href="/Articles/546698/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
LZO 2.05 uses the same tricks as Google Snappy to achieve roughly the same performance. A C-only port of Snappy was submitted to the kernel a long time ago, before LZO was sped-up, but didn't get much traction. With LZO 2.05 getting into the kernel (eventually), I don't see a reason to bother.<br>
<p>
<a rel="nofollow" href="http://driverdev.linuxdriverproject.org/pipermail/devel/2011-April/015577.html">http://driverdev.linuxdriverproject.org/pipermail/devel/2...</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546698/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor546569"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 9, 2013 9:24 UTC (Tue)
                               by <b>etienne</b> (guest, #25256)
                              [<a href="/Articles/546569/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
LZO is very good for big files, but does it fit the problem space: we are talking of compressing 4 Kbytes pages.<br>
Compressing basically means building a data dictionary and then building an index of reference into that dictionary - so any repeated sequence is reduced because the second and third times you just insert a reference to the first sequence.<br>
If you have a big file, and you compress independently each of its 4 K blocks, then you basically rewrite the same dictionary times and times again.<br>
Now if you re-write the wheel^Walgorithm compressing a 4 Kbytes buffer, you would use references (i.e. windows) max out at 4 Kbytes.<br>
Because there is a way out if the compression is not too effective, and because the speed is paramount, maybe it is better to find repeated sequences stating at 32 bits boundary instead of 8 bits for standard algorithm or 1 bit for slow as hell algorithm.<br>
IHMO LZO would be very good for large page (2 Mbytes), or if someone find a way to keep an out-of-band dictionary... but any general compression algorithm would not be so good for 4 Kbytes.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546569/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor548094"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">LZO</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 22, 2013 16:24 UTC (Mon)
                               by <b>turrini</b> (guest, #90157)
                              [<a href="/Articles/548094/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Why not just LZ4? It's extremely fast.<br>
<p>
<a href="http://code.google.com/p/lz4/">http://code.google.com/p/lz4/</a><br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/548094/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor545998"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 17:06 UTC (Thu)
                               by <b>ssam</b> (guest, #46587)
                              [<a href="/Articles/545998/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
it seems to me that there are 2 ways that compression could give speed benefits. here by compressing pages in RAM we can fit more in it, and so avoid costly fall backs to using slow disk. This will give a huge speed up on setups where you hit swap often, or where you benefit from having files in the RAM cache.<br>
<p>
But compression can also (in some cases) give you speedups by increasing bandwidth. For example if i my hard disk can manage 100MB/s, and i enable the compression option in my filesystem, then i might be able to get 200MB/s of data out of it (assuming an average 0.5 compression ratio). This works when the interface is slower than (de)compression.<br>
<p>
So i wonder if there is any opportunity that compression could help get around the problem of limited memory bandwidth. If memory bandwidths are of the order 10s of GB/s, shared between multiple CPUs, can the CPU decode fast enough for this to be any use? or is latency to RAM more of an issue than bandwidth?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/545998/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546024"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 19:50 UTC (Thu)
                               by <b>alankila</b> (guest, #47141)
                              [<a href="/Articles/546024/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It seems to me that tricks with compression will always lose when dealing with limited memory bandwidth, because the compressed form generally enters the main memory somehow first, and must be read, and then written in uncompressed form again. Even if the compressed data didn't have to be read from the main memory (somehow), then you'd still end up writing the uncompressed form once, so the memory bandwidth utilization would be unchanged with or without compression.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546024/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546041"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 4, 2013 22:18 UTC (Thu)
                               by <b>ssam</b> (guest, #46587)
                              [<a href="/Articles/546041/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
maybe the uncompressed would be written to the CPU cache, compressed there, and then pushed out to RAM. this may be impossible with current cache architecture.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546041/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor546073"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 2:45 UTC (Fri)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546073/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Probably not quite what you had in mind, but its been suggested that for microblade clusters (like Facebook's Group Hug), moving compressed pages between systems is kind of the next step beyond NUMA... obviously NOT cache-coherent though.<br>
Ramster (see link in the article) is a layer built on top of zcache which is a first step in this direction... it is in the staging tree in 3.9 as a subdirectory of zcache and works today over kernel sockets.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546073/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546284"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 6, 2013 22:10 UTC (Sat)
                               by <b>jospoortvliet</b> (guest, #33164)
                              [<a href="/Articles/546284/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
A question triggered by the thread parent: on a system with a compressed filesystem (think btrfs with lzo), does the cleancache side of zcache make much sense? Is data from the filesystem cached on compressed or uncompressed form? Thinking of it, the answer is probably 'uncompressed'...<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546284/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546292"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 6, 2013 23:28 UTC (Sat)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546292/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Uncompressed.  Pages in the page cache are usually mapped into processes so must be uncompressed.  When kswapd decides to evict (reclaim) a page from the page cache, the page either goes the cleancache-&gt;zcache route or filesystem-&gt;blockio route.  For the former, the page is compressed and kept in RAM, for the latter it is compressed by btrfs before being sent to blockio and to the disk.  (Note I am somewhat ignorant of btrfs internals so btrfs experts can correct me if I got the btrfs part wrong.)<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546292/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor546160"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 14:51 UTC (Fri)
                               by <b>cesarb</b> (subscriber, #6266)
                              [<a href="/Articles/546160/">Link</a>] (12 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Instead of comparing the whole page to zero and then compressing it, why not do the opposite?<br>
<p>
If the compression is deterministic, all zero pages will compress to the exact same byte string. So first do the compression, them check the size of the result. If it is the same as the size of a compressed zero page, compare its compressed form with the compressed form of a zero page. Since it is a small byte string (around 28 bytes as mentioned in the article), the comparison will be fast.<br>
<p>
I do not know which way would be faster. If the comparison to a zero page bails out fast (as soon as the first non-zero byte is found), most non-zero pages have the non-zero bytes early, and zero pages are common, it can be much faster to compare first and compress; if the non-zero bytes are at the end, or zero pages are rare, it might be faster to compress first and compare the compressed result. Only a benchmark could tell.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546160/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546172"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 15:49 UTC (Fri)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546172/">Link</a>] (11 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It's been awhile since I measured LZO compression and it was on a Core 2 Duo.  I should probably re-measure on a newer machine, but the numbers I vaguely remember are tha,t on average, compression was about 20K cycles and decompression was about 8K? (x86_64 so PAGE_SIZE=4K).<br>
<p>
However, if you think about it, to compress a page, any algorithm must certainly examine every byte and then do some other action which must include at least some arithmetic and/or a table lookup.  So I can't imagine a case where compression would be faster than comparing every byte (or word or doubleword) against a register containing zero.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546172/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546196"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 18:21 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/546196/">Link</a>] (9 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
True, but given that the CPU is so much faster than the ram, it probably makes sense to do this zero checking as part of the compression routine. Something along the lines of<br>
<p>
set a flag allzero=true<br>
<p>
then in your loop<br>
<p>
if data<br>
  then allzero=false<br>
<p>
this should be a very fast (and small) check on modern CPUs.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546196/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546224"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 23:12 UTC (Fri)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546224/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I agree it would be more convenient (and possibly faster) if the check for zero-filled could be done in the compression algorithm.  I'm not signing up for that though... the LZO algorithm/code is incredibly dense!<br>
<p>
But even though the CPU is much faster than the memory bus, much of the cost for scanning a page (or even copying it) is due to the cache misses incurred getting all the data bytes in the page in from RAM to cache.  So scanning a page for zero-filled also serves as an effective pre-fetch.   Once the page has been scanned, and assuming the data cache is not tiny, the compression algorithm will be reading pre-cached data.<br>
<p>
Lots of good discussion, would be good to measure the different options.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546224/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546226"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2013 23:20 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/546226/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The CPU caches are rather small, especially if there are other active processes on the box that you have to share it with.<br>
<p>
I don't think that considering it a pre-fetch is really a good way of looking at it.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546226/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546291"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 6, 2013 23:22 UTC (Sat)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546291/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hmmm... on what recent architectures is the data cache small relative to PAGE_SIZE?  (Maybe on ARM or embedded archs?  I'm not very familiar with those.)<br>
<p>
I'm not sure I understand why you wouldn't think the check for zero-filled is a good pre-fetch?  The code looks basically like:<br>
<p>
if (!is_zero_filled(page))<br>
        compress(page);<br>
<p>
which is high in both address locality and temporal locality for all the bytes in the page.  Seems like a perfect match for what data caches are designed for.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546291/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546294"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 7, 2013 0:20 UTC (Sun)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/546294/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
the problem is that !is_zero_filled(page) is really not that simple<br>
<p>
when a page is 4K bytes this boils down to is more like<br>
<p>
while !endofpage<br>
for 1..32<br>
  fetch 128 bytes<br>
  wait for the fetch to complete<br>
  check if they are zero<br>
<p>
and during the time that this is running, other programs are also running, fetching data from other places in ram that may evict some of these 128 byte chunks.<br>
<p>
if you have a multi-core chip, processes running on these other cores are fetching memory that can evict chunks<br>
<p>
If you have a hyperthreading chip, while you are stalled waiting for the fetch to complete, other programs may run on the same core, generating more fetches that can evict chunks of the page<br>
<p>
And then the OS can decide that you have used your share of time and let several other programs run for a while before you get the CPU back, and they all can generate memory fetches that could evict this from the cache.<br>
<p>
<p>
so, you may luck out and find that the data is still in the cache when you come back to re-use it for the compression, but it's not a reliable thing to depend on.<br>
<p>
But if you instead do<br>
<p>
while !done<br>
  fetch 128 bytes<br>
  check if they are zero<br>
  compress them<br>
<p>
the odds of having to fetch data multiple times are much lower (not zero, but much lower)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546294/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546296"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 7, 2013 1:47 UTC (Sun)
                               by <b>djm1021</b> (guest, #31130)
                              [<a href="/Articles/546296/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Heh... good discussion, but I still have to disagree with you.<br>
<p>
Any data cache that can't hold onto the first cache line of a sequentially-read page until after the last cache line of a page has been read is either very very small or was poorly designed for its CPU.  Data caches for multi-core hyperthreaded CPUs should be either large or N-way associative or both.<br>
<p>
And the is_zero_filled() and compress() calls are in the kernel, so kernel pre-emption must occur to cause cache evictions from the same hyperthread.  Possible but presumably rare.<br>
<p>
And in your algorithm, if is_zero_filled() proves true, you've spent CPU cycles compressing almost the entire page which is totally unnecessary.<br>
<p>
So IMHO for some mostly older or tiny CPUs you may be right.  And for some workloads you may be right.   If you want to provide a patch to LZO to add is_zero_filled() checking and you can demonstrate non-synthetic workloads where your patch is faster, I will owe you a beer.  Else I think the algorithm in zram and the zcache patch recently posted by Wanpeng Li should work just fine.<br>
<p>
P.S. If your understanding of the LZO code in the kernel is good enough to provide the patch, please also update LZO to the latest version from Markus Oberhumer and volunteer to serve as kernel maintainer for lzo.c!<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546296/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546299"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 7, 2013 2:05 UTC (Sun)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/546299/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
caches are getting larger, but last I looked, the speed ratio between the CPU and memory was continuing to grow (although not as fast as it was).<br>
<p>
you can spend quite a few CPU cycles between the time that you tell the cpu to fetch the next chunk of ram into the cache and the time it's available. I don't know how many, and it will vary from chip to chip (with the increasing impact of ARM/MIPS and other non-X86 chips, I expect that this is getting to be a more interesting concern than it ever has been). The ability to vary the speed of the CPU without varying the speed of the memory makes this even more interesting :)<br>
<p>
I don't know how many CPU cycles you spend in the compression. It would be interesting to find out how this compares to the 'free' cycles that are available on current CPUs when walking though memory.<br>
<p>
I have never looked at any LZO code, so I'm not in the position to make modifications there.<br>
<p>
<font class="QuotedText">&gt; Else I think the algorithm in zram and the zcache patch recently posted by Wanpeng Li should work just fine.</font><br>
<p>
I expect they work just fine as well, I'm just suggesting a possible improvement (and then explaining why I'm doing so when it sounds like I'm not getting the point across)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546299/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor546418"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 8, 2013 14:50 UTC (Mon)
                               by <b>intgr</b> (subscriber, #39733)
                              [<a href="/Articles/546418/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I think cache eviction is a red herring, but I agree that this talk about "let's write a fast loop to synchronously prefetch the data, then process this data in a slow loop" is a big fallacy.<br>
<p>
The real performance killer is that the zero-page check is very fast CPU-wise, but it has to wait synchronously for the next cacheline to arrive from the slow RAM. Your CPU will be stalled most of the time, waiting for the data to arrive from RAM or a lower-level cache. (And let's not forget that it only makes sense to compress rarely-accessed data -- which is unlikely to already be in cache).<br>
<p>
In contrast, any compression algorithm is likely to be slow enough that by the time it needs to access the next cacheline, the hardware prefetcher will already have that data in cache -- or at least reduced the time your CPU is stalled behind memory. Thus RAM (pre)fetching and CPU compression can work in parallel.<br>
<p>
And let's not forget that hardware prefetchers are so good these days that even *asynchronous* prefetch instructions can frequently hurt performance: <a href="https://lwn.net/Articles/444336/">https://lwn.net/Articles/444336/</a><br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546418/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor546751"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 10, 2013 12:25 UTC (Wed)
                               by <b>eras</b> (guest, #90319)
                              [<a href="/Articles/546751/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How about combining the check for zero-pages and compression?<br>
<p>
First check if the first 8 bytes are zero (or whatever convenient size); if not, goto compression. If they are zero, check the next 8 bytes.<br>
<p>
And here is the different step: if they are not zero, lookup pre-generated state representing the compressed 8 zeroes. Otherwise keep on doing the check (possibly later looking up a 8*n-byte zero compression data).<br>
<p>
This would require 511 lookup table entries, and if would be blazingly fast to compress pages that begin with zeroes - or pages that are fully zero, in which case you jump to the compression routine. You also never throw any work away.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546751/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor546794"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 10, 2013 15:39 UTC (Wed)
                               by <b>etienne</b> (guest, #25256)
                              [<a href="/Articles/546794/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; How about combining the check for zero-pages and compression?</font><br>
<p>
Note that under Linux and a filesystem which supports holes, the check for pages full of zero bytes will not often succeed because they are mapped in memory as the special zero-page (and marked copy on write).<br>
But the ultra-simplified compression algorithm looks like: read the first byte, read the second byte and see if it equal to the first byte, read the third byte and see if it is equal to either the first one or the second one or both, ... so you will catch quickly a page full of the same byte.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546794/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor546659"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Fast zero page deduplication?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 9, 2013 18:43 UTC (Tue)
                               by <b>bluss</b> (subscriber, #47454)
                              [<a href="/Articles/546659/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
No, not every algorithm out there examines every byte. If you see a long stretch of incompressible data you'll skip forward in increasing steps, skipping over some parts. If you do this, you'll scan through incompressible files pretty fast, and you have a reasonable go at skipping over incompressible parts and landing in compressible parts where you go back to normal scan.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/546659/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor547138"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 12, 2013 10:27 UTC (Fri)
                               by <b>Duncan</b> (guest, #6647)
                              [<a href="/Articles/547138/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thanks for answering the simple sysadmin's perspective question "Compressed memory, great, but which zproject should I enable in my kernel config and why would I choose it over the others?"<br>
<p>
LWN has previously covered the three in various levels of detail, but this was the first coverage that really had a practical answer to that simple question.  =:^)<br>
<p>
Duncan<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/547138/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor547459"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-kernel memory compression</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 17, 2013 7:49 UTC (Wed)
                               by <b>dag-</b> (guest, #30207)
                              [<a href="/Articles/547459/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It's probably too early to do performance comparisons (especially when done by one of the authors of one of the projects). However as a sysadmin, that's one of the metrics that would influence my decision ;-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/547459/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2013, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
