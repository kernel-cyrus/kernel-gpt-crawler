        <!DOCTYPE html>
        <html lang="en">
        <head><title>Lockless patterns: some final topics [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/850202/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/850504/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/850202/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Lockless patterns: some final topics</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</blockquote>
<div class="GAByline">
           <p>March 29, 2021</p>
           <p>This article was contributed by Paolo Bonzini</p>
           <hr>
<a href="/Articles/844224/">Lockless patterns</a>
</div>
So far, this series has covered five common lockless patterns in the Linux
kernel; those are probably the five that you will most likely encounter
when working 
on Linux.  Throughout this series, some details have been left out and some
simplifications were made in the name of clarity.  In this final
installment, I will sort out some of these loose ends and try to answer
what is arguably the
most important question of all: when should you use the lockless
patterns that have been described here?

<h4>Data, address, and control dependencies</h4>

<p>
In the code examples that have been presented so far, the ordering of
instructions 
in one thread was enforced either by acquire and release semantics or by
memory barriers.  Sometimes, however, it is simply impossible for the
processor and/or the compiler to reorder instructions, because an
instruction has a <em>dependency</em> on another instruction that comes before.
In these cases, ordering between pairs of instructions is ensured even
for relaxed loads and stores.  These dependencies come in three forms.
<p>
<b>Data dependencies</b> exist when a write stores a value that comes from
a previous load, or is based on such a value:

  <pre>
    int x = READ_ONCE(a);
    WRITE_ONCE(b, <b>x</b> + 1);
</pre>
<p>

Data dependencies are the simplest of these three cases; the store
cannot execute before the load, or the processor would not know
the value that the store must write.  Therefore, in this case,
the read from <tt>a</tt> behaves as a load-acquire, at least
with respect to the following <tt>WRITE_ONCE()</tt> statement.
Unlike <tt>smp_load_acquire()</tt> or <tt>smp_rmb()</tt>, an 
ordering created by a data dependency is not guaranteed against
<em>all</em> following memory 
operations.  In the following example, the read of <tt>c</tt>
might be reordered before the read of <tt>a</tt>, but the write of
<tt>b</tt> cannot be:

</p><pre>
    int x = READ_ONCE(a);
    WRITE_ONCE(b, x + 1);
    int y = READ_ONCE(c);
</pre>

<p>
However, we've already seen that, most of the time, lockless code only cares
about the ordering between specific memory operations; in some cases, loads
can use <tt>READ_ONCE()</tt> safely because the only users of the value and
the only operations that need to be ordered
are data-dependent stores.  That said, none of the patterns presented
throughout the series have a store that follows a load in the same thread,
so this optimization is not applicable.
<p>

<b>Address dependencies</b> exist when a read or a write operates on a
location whose address comes from a previous load, or is based on such a
value:

<p>
<pre>
    int x = READ_ONCE(a);
    int y = READ_ONCE(b[<b>x</b>]);

    struct s *x = READ_ONCE(a);
    WRITE_ONCE(<b>x</b>-&gt;f, 1);
</pre>
<p>

Address dependencies are, in theory, pretty simple as well.
In the first example, the read of <tt>b[x]</tt> cannot occur prior to the
read of <tt>x</tt>; in the second, the address to be written to cannot be
known until the read is complete.
<p>
Similarly, in this code from part&nbsp;1,
the address of <tt>datum-&gt;x</tt> is only known after <tt>datum</tt>
is loaded from <tt>message</tt>:

</p><pre>
    thread 1                            thread 2
    --------------------------------    ------------------------------------
    a.x = 1;
    smp_store_release(&amp;message, &amp;a);    datum = smp_load_acquire(&amp;message);
                                        printk("%x\n", datum-&gt;x);
</pre>

<p>
How could the CPU fetch a value from memory, or store it, without
knowing which location to operate on?  Therefore, one would expect
that it would be possible to write the first line of thread&nbsp;2
as follows, without a load-acquire instruction:
<p>
<pre>
    datum = READ_ONCE(message)
</pre>
<p>
This is a reasonable 
expectation, and it is also true at the assembly language level for all
processors that Linux has been ported to&mdash;except the Alpha, due to a
peculiar cache architecture.  As of version 4.15, Linux developers decided
that it's okay to make <tt>READ_ONCE()</tt> more expensive on the Alpha,
and therefore address-dependent loads are now always ordered after the
loads they depend on.  Note that address-dependent stores
have never been a problem.
<p>
Finally, <b>control dependencies</b> exist when the value obtained by a
read may cause a read or a write to not execute at all:
<p>
<pre>
    int y = 0;
    if (READ_ONCE(a))
        y = READ_ONCE(b);        // *no* ordering here

    if (READ_ONCE(a))
        WRITE_ONCE(b, 1);        // write is ordered here
</pre>
<p>
When a control dependency exists from one load to another load (as in the
first example above), it will never introduce any ordering.  A CPU can
always perform the load speculatively before the test and ignore the result
if the "then" branch turns out not to be executed.  Control dependencies
from a load to a store are the tricky ones this time.  The CPU is not a
problem, because the cache cannot store the new datum before it knows
whether the store should actually happen; the problem is the compiler.
Consider code like this:

</p><pre>
    x = READ_ONCE(a);
    smp_store_release(&amp;b, 1);
    if (x)
        do_something();
    else
        do_something_else();
</pre>

<p>A clever programmer might want to write it like this:

</p><pre>
    x = READ_ONCE(a);
    if (x) {
        WRITE_ONCE(b, 1);
  	do_something();
    } else {
        WRITE_ONCE(b, 1);
  	do_something_else();
    }
</pre>

and, indeed, the control dependency would cause the CPU to impose an
ordering between reading <tt>a</tt> 
and writing <tt>b</tt>.  However, the compiler could decide to hoist
the writes before the conditional, and only test <tt>x</tt> afterward,
leading to generated code that looks like:

</p><pre>
    x = READ_ONCE(a);
    WRITE_ONCE(b, 1);
    if (x)
        do_something();
    else
        do_something_else();
</pre>

<p>Now the control dependency is gone and both the CPU and the compiler
<em>can</em> move the store before 
<tt>a</tt> is read.  As is often the case, these issues with control
dependencies are solved simply by avoiding them and annotating each
store properly as either acquire or release.

</p><p>
Of all these cases, address-dependent loads are probably
the only ones that you will encounter in practice.  The most common
and self-explanatory case is retrieving a data structure with
<tt>rcu_dereference()</tt> and <tt>srcu_dereference()</tt> and then
reading its contents; on the Alpha, these RCU and SRCU
primitives include the required memory barriers even on Linux
versions prior to 4.15.
However, you should be alert in the occasional case where RCU is not
used and therefore both memory accesses use <tt>READ_ONCE()</tt>.
</p>

<h4>Optimized memory barriers</h4>

<p>
<a href="/Articles/849237/">Part 5</a> introduced atomic
read-modify-write operations such as <tt>atomic_inc()</tt>.  Linux
defines operations that do not return a value  
to have relaxed semantics.  Failed compare-and-swap operations also
do not imply any memory barrier, while a successful compare-and-swap
behaves as if the programmer had written a memory barrier on each
side of the operation.

</p><p>
Some processors, however, do not have an instruction for a relaxed,
read-modify-write operation.  On those processors, writing something
like this (a variation on the full memory barrier pattern from
<a href="/Articles/847481/">part&nbsp;3</a>) would be wasteful,
with <tt>set_bit()</tt> (which must read the target location in order to
change only the specified bit) 
and <tt>smp_mb()</tt> providing back-to-back
memory barriers:

</p><pre>
    set_bit(&amp;flag1, FLAG_DONT_SLEEP);
    smp_mb();
    if (READ_ONCE(wake_me))
        wake(thread2);
</pre>

<p>
For this purpose, Linux defines the optimized memory barriers
<tt>smp_mb__before_atomic()</tt> and <tt>smp_mb__after_atomic()</tt>.
They compile as either compiler-only barriers or full memory barriers,
depending on architecture-specific details in the implementation of
atomic read-modify-write operations.  For example, all x86 read-modify-write
operations imply a barrier on each side, therefore these optimized
memory barriers are a compiler-only barrier on that architecture.
On ARM, instead, it is possible to define relaxed read-modify-write
operations and, therefore, the optimized memory barriers will emit a
<a
href="https://developer.arm.com/documentation/dui0489/c/arm-and-thumb-instructions/miscellaneous-instructions/dmb--dsb--and-isb"><tt>dmb</tt></a>
instruction.  They are used as a drop-in replacement for 
<tt>smp_mb()</tt>:

</p><pre>
    set_bit(&amp;flag1, FLAG_DONT_SLEEP);
    <b>smp_mb__after_atomic();</b>
    if (READ_ONCE(wake_me))
        wake(thread2);
</pre>

<p>Another optimized memory barrier is <tt>smp_store_mb()</tt>, which
is a replacement for <tt>WRITE_ONCE()</tt> followed by <tt>smp_mb()</tt>.
For example:

</p><pre>
    thread 1                               thread 2
    -------------------                    --------------------------
    <b>smp_store_mb(dont_sleep, 1);</b>           <b>smp_store_mb(wake_me, 1);</b>
    if (READ_ONCE(wake_me))                if (!READ_ONCE(dont_sleep))
        wake(thread2);                         sleep();
</pre>

<h4>When to use lockless patterns?</h4>

<p>
Even though the previous installments of this series tried to
use actual Linux kernel code in the examples, one could say that they
only showed the theory.  While the material in the articles
should be enough to understand existing code, there was very
little explanation of when to employ these patterns and why.

</p><p>
One thing that I did try to stress throughout the series is
that lockless techniques are not an alternative to traditional
synchronization primitives.  They are only a means to an end, which
is to limit the cost of synchronization (cacheline contention is
also a kind of synchronization; it just happens in the processor rather
than in your code).  <a href="/Articles/423994/">Expensive
synchronization in concurrent code cannot be eliminated</a>, but
it is possible to limit the number of expensive instructions, or
to move them out of the hottest paths.  To this end, some
design points that you should consider are:

</p><ul class="spacylist">
<li> The interaction with existing locks and shared data accesses.</li>

<li> The frequency of writes to shared data: every time a thread writes to
     a shared location, cache coherency traffic can end up creating a
     potential scalability bottleneck.</li>

<li> The frequency of synchronization: the best way to keep
     multiple threads going is for them to be as independent as possible,
     because synchronization will always introduce overhead.</li>
</ul>

<p>
Let's say you want to gather some statistics while your code runs, for
example counting how many packets are sent through a network interface,
and you want the overhead to be minimal.  Before rushing to implement
the counter in a lockless manner, for example with an atomic increment
instruction, you should investigate the ways in which sending a packet
can be serializing.  If sending a packet already takes a spinlock or
mutex, for example, there is likely no performance to gain from a fancy
implementation of the counter: if you ensure that the counter resides in a
cache line that is already needed when sending a packet, incrementing a
single memory location while the lock is already held will be almost free.

</p><p>
If there is no single lock that is always taken when sending a packet, lockless
techniques may indeed be beneficial, but there's much more to them
than atomic read-modify-write operations.  For example, you could
use multiple counters, so that you can increment them without a lock
(making them per-CPU) or under a lock that you already take (say, per
network queue).  The counters can be summed in the (presumably rare) event
of someone reading the statistics: this solution avoids concurrent writes
to shared data and does not need any additional synchronization on the
hot path.

</p><p>
In the example above, a coarse lock&mdash;for example, a lock that covers
the operation of a network queue&mdash;does not necessarily imply loss of
scalability: in a system that is designed to keep the threads mostly independent,
contention on coarse locks will often be rare or nonexistent.
Conversely, papering over an excessive amount of shared-data access with
fine-grained locks can increase the cost of synchronization substantially,
and bring performance down.

</p><p>
The cost of fine-grained locking is especially visible with read/write
locks, where even the read side needs to write to the lock in order
to take it.  When writing scalable code, it can be useful to think
of read/write locks as "shared/exclusive" locks.  You can use
coarse read/write locks to make sure that only hot paths have to handle
concurrent execution: if less frequently executed code takes the lock for
exclusive access, any lockless fast paths need not take that
code into account  at all.  An example of this is the Linux <tt>mmap_sem</tt>; Linux
performs many page table manipulations while holding it for "reading".
But still, the relatively high cost of taking <tt>mmap_sem</tt> for
reading has made it <a
  href="/Articles/787629/">a known problem</a> for
scalability.

</p><p>
If fine-grained locking is needed for specific data structures,
writes to these data structures will usually be rare in a
scalable system.  Instead of a read/write lock, you can try to
protect the read-side critical sections with mechanisms such as seqlocks
or RCU.  <a
href="/Articles/202847/">SRCU</a>
can also be an interesting alternative to RCU whenever writers
cannot bear the cost of <tt>synchronize_rcu()</tt> (think of it as
<em>subsystem</em> RCU, not just <em>sleepable</em> RCU).

</p><p>
Even if the threads operate independently, there may be rare cases
where they have to interact.  In an implementation that needs
to check a flag thousands or millions of times a second, taking
a fine-grained lock around each check, the cost might become visible
no matter how small and rare these interactions are.  In these cases,
applying lockless techniques to the fast path can be valuable.

</p><p>
For example, you could give each thread a queue of requests from other
threads and manage them through
<a href="/Articles/847973/">single-consumer linked
lists</a>.  Perhaps you can trigger the processing of requests using the
<a href="/Articles/847481/">cross-thread notification</a> pattern
from the article on full memory barriers.  However, these techniques
only make sense because the design of the whole system supports them.
In other words, in a system that is designed to avoid scalability bottlenecks,
common sub-problems tend to arise and can often be solved efficiently
using the patterns that were presented here.

</p><p>
When seeking to improve the scalability of a system with lockless
techniques, it is also important to distinguish between <em>lock-free</em>
and <em>wait-free</em> algorithms.  Lock-free algorithms guarantee that
the system as a whole will progress, but do not guarantee that each
thread will progress; lock-free algorithms are rarely fair, and
if the number of operations per second exceeds a certain threshold,
some threads might end up failing so often that the result is a
livelock.  Wait-free algorithms additionally ensure per-thread
progress.  Usually this comes with a significant price
in terms of complexity, though not always; for example <a
  href="/Articles/844224/">message passing</a> and
cross-thread notification are both wait-free.

</p><p>
Looking at the Linux <tt>llist</tt> primitives, <tt>llist_add()</tt>
is lock-free; on the consumer side, <tt>llist_del_first()</tt> is lock-free,
while <tt>llist_del_all()</tt> is wait-free.  Therefore, <tt>llist</tt> may
not be a good choice if many producers are expected to contend on calls to
<tt>llist_add()</tt>; and using <tt>llist_del_all()</tt> is likely better
than <tt>llist_del_first()</tt> unless constant-time consumption
is an absolute requirement.  For some architectures, the
instruction set does not allow read-modify-write operations to be
written as wait-free code; if that is the case, <tt>llist_del_all()</tt>
will only be lock-free (but still preferable, because it lets
the consumer perform fewer accesses to the shared data structure).

</p><p>
In any case, the definitive way to check the performance characteristics
of your code is to benchmark it.  Intuition and knowledge of
some well-known patterns can guide you in both the design and the
implementation phase, but be ready to be proven wrong by the numbers.

</p><p>
I'll conclude this series with a quote of Dave Chinner's
<a href="/Articles/849239/">excellent critique</a>:
</p>

<blockquote class="bq">
  This is the art of concurrent programming&mdash;it's not enough just
  to know what a lockless algorithm is, you need to understand the
  data access patterns those algorithms result in and when those access
  patterns are going to become a limitation to the software. Of course,
  knowing when not to use a lockless algorithm because there are better
  ways to reduce shared data access is also part of the art.
</blockquote><p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Lockless_algorithms">Lockless algorithms</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Bonzini_Paolo">Bonzini, Paolo</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/850202/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor851102"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lockless patterns: some final topics</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 29, 2021 23:13 UTC (Mon)
                               by <b>dvdeug</b> (guest, #10998)
                              [<a href="/Articles/851102/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
As for &quot;In the first example, the read of b[x] cannot occur prior to the read of x;&quot; if b is small enough or x is a char, b could be loaded prior to the read of x and then b[x] loaded from the core cache; in fact, that might save quite a few cycles getting b from off-cpu memory. I&#x27;m not sure I understand this well enough, but loading stuff from memory early so the processor isn&#x27;t waiting on it is a standard tactic that seems like it could mess up assumptions like that.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/851102/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor851105"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lockless patterns: some final topics</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 29, 2021 23:24 UTC (Mon)
                               by <b>pbonzini</b> (subscriber, #60935)
                              [<a href="/Articles/851105/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The caches promise not to make use of tricks like that one except on the Alpha.<br>
<p>
At the compiler level, however, it would be a legal optimization to load a whole 32-bit word and extract a byte using bit masks, if b is declared as a char[4]. That would appear as reordering READ_ONCE(b[x]) before READ_ONCE(x). That&#x27;s why READ_ONCE still needs to include a compiler barrier.<br>
<p>
Ironically that specific optimization would only really make sense if the machine language lacks instructions to address memory by byte... like first generation Alpha again! :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/851105/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2021, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
