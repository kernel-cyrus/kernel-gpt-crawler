        <!DOCTYPE html>
        <html lang="en">
        <head><title>Issues around discard [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/787272/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/787286/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/787272/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Issues around discard</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>This article brought to you by LWN subscribers</b>
<p>
Subscribers to LWN.net made this article &mdash; and everything that
       surrounds it &mdash; possible.  If you appreciate our content, please
       <a href="/Promo/nst-nag3/subscribe">buy a subscription</a> and make the next
       set of articles possible.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>May 6, 2019</br>
           <hr>
<a href="/Articles/lsfmm2019/">LSFMM</a>
</div>
<p>
In a combined filesystem and storage session at the 2019 Linux Storage, Filesystem,
 and Memory-Management Summit (LSFMM), Dennis Zhou wanted to talk
about discard, which is the process of sending commands
(e.g. <tt>TRIM</tt>)  to block devices to indicate
blocks that are no longer in use.  Discard is a "serious black box", he
said; it is a third way to interact with a drive, but Linux developers have
no real insight into what its actual effects will be.  That can lead to
performance and other problems.
</p>

<p>
Zhou works for Facebook, where discard is enabled, but the results are not
great; there is a fair amount of latency observed as the flash
translation layer (FTL) shuffles blocks around for wear leveling and/or
garbage collection. 
Facebook runs a periodic <a
href="http://man7.org/linux/man-pages/man8/fstrim.8.html"><tt>fstrim</tt></a>
to discard unused blocks; that is something that the <a
href="http://cassandra.apache.org/">Cassandra database</a> recommends for
it users. 
The company also has an internal delete scheduler that slowly deletes files,
but the FTL can take an exorbitant amount of time when gigabytes of files
are deleted; read and write performance can be affected.  It is "kind of 
insane" that applications need to recognize that they can't issue a bunch
of discards at one time; he wondered if there is a better way forward. 
</p>

<a href="/Articles/787511/">
<img src="https://static.lwn.net/images/2019/lsf-zhou-sm.jpg" border=0 hspace=5 align="right"
alt="[Dennis Zhou]" title="Dennis Zhou" width=212 height=280>
</a>

<p>
An approach that Facebook is trying with Btrfs is to simply preferentially reuse
logical-block addresses (LBAs), rather than discarding them.  That requires
cooperation between the filesystem and block layer, since the block layer
knows when it can afford to do discard operations.  What the FTL is
doing is unknown, however, which could have implications on the lifetime of
the device.  He is looking for something that can be run in production.
</p>

<p>
Erik Riedel asked what is special about a 1GB discard versus a 1GB read or
write.  You are asking the device to do a large operation, so it may take
some time.  But James Bottomley noted that discard had been sold as a "fast"
way to clean up blocks that are no longer needed.  Riedel suggested that
vendors need to be held accountable for their devices; if <tt>TRIM</tt> has
unacceptable characteristics, those should be fixed in the devices.
</p>

<p>
Ric Wheeler said that there should be a tool that can show the problems
with specific devices; naming and shaming vendors is one way to get them to
change their behavior.  Chris Mason noted that the kernel team at Facebook
did not choose the hardware; it would likely choose differently.  This is
an attempt to do the best the team can with the hardware that it gets.
</p>

<p>
Ted Ts'o said that he worries whenever the filesystem tries to second-guess
the hardware.  Different devices will have different properties; there is
at least one SSD where the <tt>TRIM</tt> operation is handled in memory, so
it is quite fast. 
Trying to encode heuristics for different devices at the filesystem layer
would just be hyper-optimizing for today's devices; other devices will roll
out and invalidate that work.
</p>

<p>
The current predicament came about because the kernel developers gave the
device makers an "out" with the <tt>discard</tt> mount flag, Martin
Petersen said.  If performance was bad for a device, the maker could
recommend mounting without enabling discard; if the kernel developers had
simply consistently enabled discard, vendors would have fixed their devices
by now.  Beyond that, some vendors have devices that advertise the
availability of the <tt>TRIM</tt> command, but do nothing with it; using it simply
burns a queue slot for no good reason.
</p>

<p>
Riedel suggested that "name and shame" is the right way to handle this
problem.  If a device claims to have a feature that is not working, or not
working well, that should be reported to provide the right incentives to
vendors to fix things.
</p>

<p>
Bottomley wondered if filesystems could just revert to their old behavior
and not do discards at all.  Wheeler said that could not work since discard
is the only way to tell the block device that a block is not in use
anymore.  The bigger picture is that these drives exist and Linux needs to
support them,  Mason said.
</p>

<p>
Part of the problem is that filesystems are "exceptionally inconsistent" in
how they do discard, Mason continued.  XFS does discard asynchronously,
while ext4 and Btrfs 
do it synchronously.  That means Linux does not have a consistent story to
give to the vendors; name and shame requires that developers 
characterize what, exactly, filesystems need.
</p>

<p>
The qualification that is done on the devices is often cursory, Wheeler
said.  The device is run with some load for 15 minutes or something like
that before it is given a "thumbs up"; in addition, new, empty devices are
typically tested.  Petersen said that customers provide much better data on
these devices; even though his employer, Oracle, has an extensive
qualification cycle, field-deployed drives provide way more information.
</p>

<p>
Zhou said that tens or hundreds of gigabytes can be queued up for discard,
but there is no need to issue it all at once.  Mason noted that filesystems
have various types of rate limiting, but not for discard.  Ts'o said it
would be possible to do that, but it should be done in a single place; each
filesystem should not have to implement its own.  There was some
discussion of whether these queued discards could be "called back" by the
filesystem, but attendees thought the complexity of that was high for not
much gain.  However, if the queue is not going to empty frequently,
removing entries might be required.
</p>

<p>
In the end, Fred Knight said, the drive has to do the same amount of work
if a block is discarded or just overwritten.  Writing twice to the same LBA
will not go to the same place in the flash.  The FTL will erase the current
location of the LBA's data, then write a new location for the block.
All discard does is allow the erasure to happen earlier, thus saving time
when the data is written.
</p>

<p>
The problem is that kernel developers do not know what a given FTL will
do, Ts'o said.  For some vendors, writing the same LBA will be problematic,
especially for the "cheap crappy" devices.  In some cases, reusing an LBA
is preferable to a discard, but the kernel would not know that is the case;
it would simply be making assumptions.
</p>

<p>
To a certain degree, Zhou said, "discard doesn't matter until it does";
until the device gets past a certain use level, writing new blocks without
discarding the old ones doesn't
impact performance.  But then there is a wall at, say, 80% full, where the
drive goes past the "point of forgiveness" and starts doing garbage
collection on every write.  There is a balance that needs to be struck for
discard so that there are "enough" erasure blocks available to keep it out
of that mode.
</p>

<p>
That is hard for SSD vendors, however, because they cannot
reproduce the problems that are seen, so they cannot fix them, an attendee
said.  The kernel developers need to work more closely with the device
firmware developers and provide workloads, traces, and the like.  Wheeler
said that reporting workloads and traces is the "oldest problem in the
book".  We have to do better than we are doing now, he said, which is to
provide nothing to the vendors, he said.
</p>

<p>
Bottomley pushed back on the idea that preferentially reusing LBAs was the
right path.  If there are blocks available to be written, reusing the LBA
is worse, as it will fragment extent-based filesystems.  If there is an
erase block available, no erase need be done on a write to a new LBA, and if there
isn't, it is the same as reusing the LBA; so rewriting LBAs actually
compounds the problem.
</p>

<p>
The exact behavior is specific to the filesystem and workload, however.
The sizes of erase blocks are not known to the kernel, or even by kernel
developers, because the drive vendors have decided to keep them secret,
Bottomley said.  So every decision the kernel makes is based on an
assumption of one kind or another.
</p>

<p>
Riedel still believes this is a qualification problem at its core.  The
right solution is for the drives to do a good job with the expected
workloads.  But Petersen reiterated that by giving the vendors an out with
the <tt>discard</tt> flag, nothing will ever change.  Fixes will "never happen".
</p>

<p>
The core of the problem is that reads and writes need to happen
immediately, Zhou said, while discards can wait a bit without causing much
of a problem.  But there are different viewpoints on the problem itself,
Ts'o said; desktop distributions will be different from servers or mobile
devices.  Mason noted that if you asked ten people in the room how discard
should work, you would get something like 14 answers.
</p>

<p>
The session wound down without much in the way of resolution.  There was
talk of some kind of tool that could be used to reproduce the problems and
gather traces.  There was also talk of rate-limiting discards, but no one
wants to do a "massive roto-tilling" of the block layer given all of the
unknowns, Ts'o said; he suggested any change be done in an optional module
between the filesystems and the block layer, which could be revisited in a
few years. 
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer-Discard_operations">Block layer/Discard operations</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2019">Storage, Filesystem, and Memory-Management Summit/2019</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/787272/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor787620"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 6, 2019 18:01 UTC (Mon)
                               by <b>fuhchee</b> (guest, #40059)
                              [<a href="/Articles/787620/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Did the possibility of running online measurements of TRIM performance come up?  The filesystem could learn the actual contemporaneous characteristics of various size TRIMs during lulls in operation.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787620/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor787680"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 8:09 UTC (Tue)
                               by <b>kdave</b> (subscriber, #44472)
                              [<a href="/Articles/787680/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <blockquote>Martin Petersen said. If performance was bad for a device, the maker could recommend mounting without enabling discard; if the kernel developers had simply consistently enabled discard, vendors would have fixed their devices by now.</blockquote>

... <strike>vendors would have fixed their devices</strike> users would simply bug filesystem developers until discard is off by default, vendors doing nothing. No matter how much I'd like this approach to work, it does not work as expected in practice. I think vendors respond to $$$ and big companies asking for things, but for example see where we are with the erase block size. From our view it is a simple thing yet there has been no change AFAIK with the answers ranging from "trade secret" to "you don't need to know". And I'm afraid this won't change.
      
          <div class="CommentReplyButton">
            <form action="/Articles/787680/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor787688"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">We should not look at discard as an uniform feature in the first place</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 11:47 UTC (Tue)
                               by <b>hmh</b> (subscriber, #3838)
                              [<a href="/Articles/787688/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Sometimes it feels like the real use for "TRIM" ("discard") on FLASH-based, old-style-storage devices with advanced FTLs (i.e. SATA or SAS-attached SSDs) is being forgotten.  It is there to *reduce needless copying of stale pages of data* by the SSD itself, i.e. to reduce the need for background block writes.  It is not an speedy way to delete data blocks, or if it is, someone forgot to properly notify the device vendors about it -- it is a FLASH endurance saver.<br>
<p>
When you either TRIM or overwrite an LBA, the SSD gets the implied information that the old block is not going to be reused, and can be scheduled to be *erased*.<br>
<p>
OTOH, when a filesystem prefers to direct writes to a new LBA and TRIM/discard is never done on the old, now-freed blocks, those old blocks are going to be copied around by the SSD firmware to free up erase blocks (much like memory compaction tries to do to create huge pages).  This wastes FLASH write circles, increases on-device fragmentation, and reduces the number of "erased and ready to be used" FLASH pages.  It also eventually renders the SSD into the dreaded "slow as an old floppy drive" state.<br>
<p>
So, what "discard" is really useful for on [old-style non-NVMe?] SSDs is vastly different on why one would use "discard" on, e.g., a thin-provisioned volume.  And it is *not* any less important.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787688/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788204"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">We should not look at discard as an uniform feature in the first place</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2019 12:39 UTC (Mon)
                               by <b>Fowl</b> (subscriber, #65667)
                              [<a href="/Articles/788204/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
What happens if a write and a discard race?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788204/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788313"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">We should not look at discard as an uniform feature in the first place</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2019 2:23 UTC (Tue)
                               by <b>hmh</b> (subscriber, #3838)
                              [<a href="/Articles/788313/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It entirely depends on the competence of the vendor that wrote the device firmware, and that whomever designed the queue protocol was not crazy enough to forget about write collisions between queues.<br>
<p>
A discard really is just a write as far as ordering and races/collisions go.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788313/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor787697"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 13:52 UTC (Tue)
                               by <b>shentino</b> (guest, #76459)
                              [<a href="/Articles/787697/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Discard isn't just for SSDs.<br>
<p>
In essence, discard is at this point a fundamental storage operation just like reads and writes.<br>
<p>
LVM thin pools for example use high level discards as cues to deallocate committed pool space, which may well provoke the thin pool itself to cascade discards to its own storage.<br>
<p>
It's also used in virtualization.<br>
<p>
VMs that issue discards to virtual block devices can likewise provoke the hypervisor into deallocating storage or space occupied by whatever it stores on the host to back the device.  A guest OS issuing discards to its virtual drives, even ones presented as "spinning rust", can help a hypervisor optimize how it manages the storage on the host.<br>
<p>
Discards are a big opportunity for higher layers like this to give lower layers housekeeping opportunities beyond just letting an SSD garbage collect.<br>
<p>
They should be liberally sent at every opportunity.  If anything the overhead in managing them should encourage lower layers to take advantage of the information.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787697/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor787735"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">it is the O_PONIES issue again!</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 16:27 UTC (Tue)
                               by <b>walex</b> (subscriber, #69836)
                              [<a href="/Articles/787735/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <blockquote><p>“but the FTL can take an exorbitant amount of time when gigabytes of files are deleted; read and write performance can be affected.”</p></blockquote>

<p>This is alluded to in the text by C Mason and others, but that is typical of devices that don't have a supercapacitor-backed cache/buffer: they must commit every delete to flash.</p>

<p>So called "enteprise" devices have supercapacitor backed caches, and can do deletes (and random writes) a lot faster. The situation is rather similar to RAID host-adapters with a cache, where a BBU makes a huge difference.</p>
<p>It is the famous <tt>O_PONIES</tt> and <q>eternal september</q> issue that never goes away, because every year there is a new batch of newbie sysadms and programmers who don't get persistence and caches, and just want <tt>O_PONIES</tt>.</p>

<p>People familiar with using SSDs for journaling in a Ceph storage layer know how enormous the difference made by having a supercapacitor backed SSD cache...</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/787735/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787738"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">it is the O_PONIES issue again!</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 17:11 UTC (Tue)
                               by <b>naptastic</b> (guest, #60139)
                              [<a href="/Articles/787738/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I inherited a Samsung 950 Evo after it was retired from service after ~2 years. Once it was installed, I checked the smart data. Couldn't believe the "Total LBAs written" number.<br>
<p>
"How the hell did you write SEVEN HUNDRED TERABYTES to this drive in two years‽"<br>
<p>
It was the Ceph journal drive.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787738/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787764"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">it is the O_PONIES issue again!</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 21:23 UTC (Tue)
                               by <b>walex</b> (subscriber, #69836)
                              [<a href="/Articles/787764/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <blockquote><p>«"How the hell did you write SEVEN HUNDRED TERABYTES to this drive in two years‽"</p>
<p>It was the Ceph journal drive.»</p>
</blockquote>

<p>And that is also because the 950 EVO does not have a persistent (supercapacitor backed) cache, and thus all 700 TB will have hit the flash chips, even if a lot of it probably was just ephemeral. Anyhow using the 950 EVO as a Ceph journal device, especially with that high rate of journaling (38GB/hour), probably cost a lot in latency to Ceph.</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/787764/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788008"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">it is the O_PONIES issue again!</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2019 0:01 UTC (Fri)
                               by <b>miquels</b> (guest, #59247)
                              [<a href="/Articles/788008/">Link</a>] 
      </p>
      
      </div>
      </summary>
      I have several SSDs in production that have written not 700 TB, but 7600 TB. In 4 years' time.  Datacentre SSDs FTW :)

<Pre>
=== START OF INFORMATION SECTION ===
Device Model:     Samsung SSD 845DC PRO 800GB
User Capacity:    800,166,076,416 bytes [800 GB]
Sector Size:      512 bytes logical/physical

SMART Attributes Data Structure revision number: 1
Vendor Specific SMART Attributes with Thresholds:
ID# ATTRIBUTE_NAME          FLAGS    VALUE WORST THRESH FAIL RAW_VALUE
  5 Reallocated_Sector_Ct   PO--CK   099   099   010    -    3
  9 Power_On_Hours          -O--CK   093   093   000    -    34281
 12 Power_Cycle_Count       -O--CK   099   099   000    -    4
177 Wear_Leveling_Count     PO--C-   076   076   005    -    9158
179 Used_Rsvd_Blk_Cnt_Tot   PO--C-   099   099   010    -    3
180 Unused_Rsvd_Blk_Cnt_Tot PO--C-   099   099   010    -    7037
241 Total_LBAs_Written      -O--CK   094   094   000    -    16410885339592
242 Total_LBAs_Read         -O--CK   097   097   000    -    7734700749043
250 Read_Error_Retry_Rate   -O--CK   100   100   001    -    0
</pre>
      
          <div class="CommentReplyButton">
            <form action="/Articles/788008/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor787736"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">There are two very different types of TRIM command</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 16:43 UTC (Tue)
                               by <b>walex</b> (subscriber, #69836)
                              [<a href="/Articles/787736/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <blockquote><p>“XFS does discard asynchronously, while ext4 and Btrfs do it synchronously.“</p></blockquote>

<p>The discussion throughout the article and here is made less useful by a vital omission: there is no mention that the first edition of the TRIM command for SATA was "blocking" ("synchronous"), but there is now a variant that is non-blocking ("asynchronous").</p>

<p>Essentially all the problems reported with 'discard' are due to the use of the first "blocking" variant, which unfortunately is the only one that has been implemented on most of the SATA flash SSD installed base so far. <a href="https://en.wikipedia.org/wiki/Trim_(computing)#Shortcomings">Wikipedia says</a>:

<blockquote><p>“The original version of the TRIM command has been defined as a non-queued command by the T13 subcommittee, and consequently can incur massive execution penalty if used carelessly, e.g., if sent after each filesystem delete command. The non-queued nature of the command requires the driver to first wait for all outstanding commands to be finished, issue the TRIM command, then resume normal commands.”</p></blockquote>

<p>SAS/SCSI and NVME have similar commands with different semantics, I particularly like the "write zeroes" command of NVME.</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/787736/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787763"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">There are two very different types of TRIM command</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 21:19 UTC (Tue)
                               by <b>masoncl</b> (subscriber, #47138)
                              [<a href="/Articles/787763/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I was talking about async discards in a slightly different context.  Btrfs and ext4 will block transaction commit until we've finished trimming the things we deleted during the transaction.  XFS will allow the commit to finish and let the trims continue floating down in the background, while making sure not to reuse the blocks until the trim is done.<br>
<p>
Depending on the device, the async approach can be much faster, but it can also lead to a very large queue of discards, without any way for the application to wait for completion.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787763/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787774"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">There are two very different types of TRIM command</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2019 23:44 UTC (Tue)
                               by <b>walex</b> (subscriber, #69836)
                              [<a href="/Articles/787774/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <blockquote><p>“XFS will allow the commit to finish and let the trims continue floating down in the background”</p></blockquote>

<p>Indeed, but a discard is pretty much like a write, so it could be handled the same way, wo why would XFS do that complicated stuff? The obvious reason is that if discarding is handled synchronously, like Btrfs and <tt>ext4</tt> do, then issuing blocking (non queued) TRIM can cause long freezes, which is indeed why many people don't use the <tt>discard</tt> mount option but use <tt>fstrim</tt> every now and then at quiet times.</p>

<p>That's why mentioning that there are both blocking and nonblocking TRIMs matters so much, because if non-blocking TRIM is available it effectively works like a write as to queueing too, thus there is very little to be gained from backgrounding TRIMs like XFS does. Apart from love of overcomplexity, which seems rather common in the design of XFS.</p>

<p>Put another way pretty much the entire TRIM debate has been caused by the predominance of blocking TRIM in the SATA installed base of consumer flash SSDs (and the other minor reason has been the numerous TRIM related bugs in many models of flash SSDs, which are just part the numerous bugs of many models of flash SSDs).</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/787774/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787813"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">There are two very different types of TRIM command</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2019 15:26 UTC (Wed)
                               by <b>masoncl</b> (subscriber, #47138)
                              [<a href="/Articles/787813/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
"That's why mentioning that there are both blocking and nonblocking TRIMs matters so much, because if non-blocking TRIM is available it effectively works like a write as to queueing too, thus there is very little to be gained from backgrounding TRIMs like XFS does. Apart from love of overcomplexity, which seems rather common in the design of XFS."<br>
<p>
Actual queueing support for discards does change the math a bit, but the fundamental impact on latency of other operations is still a problem.  Sometimes it's worse because you're just allowing the device to saturate itself with slow operations.<br>
<p>
The XFS async trim implementation is pretty reasonable, and it can be a big win in some workloads.  Basically anything that gets pushed out of the critical section of the transaction commit can have a huge impact on performance.  The major thing it's missing is a way to throttle new deletes from creating a never ending stream of discards, but I don't think any of the filesystems are doing that yet.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787813/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788038"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">There are two very different types of TRIM command</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2019 13:20 UTC (Fri)
                               by <b>GoodMirek</b> (subscriber, #101902)
                              [<a href="/Articles/788038/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
In my eyes, there is a significant difference between write and discard.<br>
If a write fails it can cause a data loss, which is a critical issue and therefore requires transaction safety.<br>
If a discard fails, the worst thing that can happen is some performance and wear deterioration, which is negligible issue.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788038/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor787789"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2019 6:41 UTC (Wed)
                               by <b>nilsmeyer</b> (guest, #122604)
                              [<a href="/Articles/787789/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There are a lot of websites doing hardware testing already, however few of them do testing with Linux (Phoronix and Servethehome come to mind). I think they could and should add discard testing, with phoronix at least the procedure is somewhat standardized. Another test I would really like is fsync() performance, since that shows you the actual, durable write performance of the drive. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787789/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788322"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2019 8:57 UTC (Tue)
                               by <b>roblucid</b> (guest, #48964)
                              [<a href="/Articles/788322/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
May be the kernel needs to give user space the ability to give it some hints, so something like a drivecap file plus a utility which then configures the kernel on drive characterisics policy?  This is how BSD used to tune to HDD's, when sectors per cylinder mattered (way before HDDs switched to LBA addressing).<br>
<p>
Then SSD vendors (or large customers) can characterise how their drive is expected to be used, LBA reuse, discard penalties, phantom discards and the like.  You might even be able to tune for service life, with user space logging expected degraded performance.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788322/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788598"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 16, 2019 4:05 UTC (Thu)
                               by <b>scientes</b> (guest, #83068)
                              [<a href="/Articles/788598/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The problem is that the firmware is non-free software. I've had SSDs that just fail with TRIM for example, and you are given a windows-only update tool with a binary blob firmware. I just threw it out and decided to never buy from the company again.<br>
<p>
These firmware run ARM M and they should be free software.<br>
<p>
There is a project in this direction: <a rel="nofollow" href="http://www.openssd.io/">http://www.openssd.io/</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788598/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788600"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Issues around discard</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 16, 2019 5:40 UTC (Thu)
                               by <b>zdzichu</b> (subscriber, #17118)
                              [<a href="/Articles/788600/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
For firmware, I think Linux Vendor Firmware Service (<a href="https://fwupd.org/">https://fwupd.org/</a>) can be used to update SSDs firmware under Linux.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788600/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2019, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
