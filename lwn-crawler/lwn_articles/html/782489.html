        <!DOCTYPE html>
        <html lang="en">
        <head><title>Controlling device peer-to-peer access from user space [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/782489/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/782582/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/782489/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Controlling device peer-to-peer access from user space</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="GAByline">
           <p>March 7, 2019</p>
           <p>This article was contributed by Marta Rybczyńska</p>
           </div>
<p>The recent <a href="https://lwn.net/Articles/767281/">addition of 
support for direct (peer-to-peer) operations between PCIe devices</a> in the
kernel has opened the door for different use cases. The initial work
concentrated on in-kernel support and the NVMe subsystem; it also
added support for memory regions that can be used for such transfers.
Jérôme Glisse recently <a
href="/ml/linux-kernel/20190129174728.6430-1-jglisse@redhat.com/">proposed
two extensions</a> that would allow the mapping of those regions into user
space and mapping device files between two devices. 
The resulting discussion surprisingly led to consideration of the
future of core kernel structures dealing with memory management.</p>


<p>Some PCIe devices can perform direct data transfers to other devices without
involving the CPU; support for these peer-to-peer transactions was added
to the kernel for the 4.20 release.
The rationale behind
the functionality is that, if the data is passed between two devices without
modification, there is no need to involve the CPU, which can perform other
tasks instead. The peer-to-peer feature was developed to allow <a
href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">Remote
Direct
Memory Access (RDMA)</a> network interface cards to pass data
directly to NVMe drives in the NVMe fabrics subsystem. Using peer-to-peer
transfers lowers the memory bandwidth needed (it avoids one copy operation
in the standard path from device to system memory, then to another device)
and CPU 
usage (the devices set up the DMAs on their own).  While not considered directly
in the initial work, graphics processing  
units (GPUs) and RDMA interfaces have been able to use that functionality in
out-of-tree modules for years.</p>

<p>The merged work concentrated on support at the PCIe layer. It
included setting up special memory regions and the devices that will export and
use those regions. It also allows finding out if the PCIe topology allows
the peer-to-peer transfers.</p>

<p>The new work by Glisse adds support for peer-to-peer memory regions
managed from user space. An important aspect is adding support for <a
href="/Articles/684916/">heterogeneous memory management</a> (HMM), of
which Glisse is the author, to access such 
memory, but the functionality also covers devices without HMM.
The current prototype implementation connects a network card (Mellanox
mlx5) to a 
GPU (AMD); Glisse expects to use it in <a
href="https://github.com/RadeonOpenCompute/ROCnRDMA#start-of-content">AMD
ROCm 
RDMA</a>. He mentioned a number of possible use cases,  including one
device controlling another device's command queue. An example of this
situation is a network card accessing a block device command queue so that
it can submit storage transactions without the CPU's involvement. Another
is direct memory access between two devices, where the
memory is not even mapped to the CPU. In this case, the computation on one
device runs without interaction with the other one. Examples include a
network device monitoring the progress of a device and avoiding use of
the system memory, or a network device streaming results from an
accelerator.</p>

<p>The patch set in its current state does not include any drivers that
actually use the feature.  This resulted in other developers having
difficulties in understanding how the code is expected to be used and <a
href="/ml/linux-kernel/20190130075256.GA29665@lst.de/">commenting</a>
that users are needed before the code can be merged. Similar
comments appeared elsewhere in the discussion. Glisse provided some
code examples from other branches, but it seems that examples of drivers
using the feature will have to appear in future versions of the patch
set so that it can be seriously considered for a merge. A part of this
related to that fact that many developers remember <a
href="/Articles/757124/">the difficulties</a> after HMM was initially  
merged and would rather avoid a repeat.</p>

<h4>Interconnect topologies</h4>

<p>Currently, PCIe peer-to-peer transfers are only allowed when two devices
are attached to the same bridge. Glisse <a
href="/ml/linux-kernel/20190129174728.6430-2-jglisse@redhat.com/">added</a>
helper functions to simplify the checks in drivers and support any other
interconnect in the process. In the discussion that followed,  Greg
Kroah-Hartman <a
href="/ml/linux-kernel/20190129194605.GC32069@kroah.com/">commented</a>
that there is no peer-to-peer concept in the device model; the
implementation only covers PCIe for now, so he thought that the changes were
premature. Glisse <a
href="/ml/linux-kernel/20190129195651.GK3176@redhat.com/">agreed
to concentrate on PCIe for now</a>. It seems likely that the device model
will get richer when other interconnects start using the peer-to-peer
functionality.</p>

<h4>Extending <tt>vm_operations_struct</tt></h4>

<p>One of the patches added two new callbacks to <tt><a
href="https://elixir.bootlin.com/linux/latest/source/include/linux/mm.h#L420">struct
vm_operations_struct</a></tt>, a core kernel structure collecting callbacks
for virtual memory area (VMA) operations. The two proposed callbacks are
<tt>p2p_map()</tt> and <tt>p2p_unmap()</tt>; respectively, they map and unmap
a peer-to-peer region.  Rather than mapping the region into a process's
address space, though, these functions instruct the device to make the
regions available for peer-to-peer access.  A common use case would be for
a user-space process to map a device memory region with <tt>mmap()</tt>,
then pass that region to a second device to set up the peer-to-peer
connection.  That second device would then call the new methods to manage
that mapping.

<p>


Logan Gunthorpe <a
href="/ml/linux-kernel/c2c02af7-1d6f-e54f-c7fb-99c5b7776014@deltatee.com/">asked</a> 
about the possible use of the existing <tt>fault()</tt> callback that would
map peer-to-peer memory if appropriate.
Glisse <a
href="/ml/linux-kernel/20190129191120.GE3176@redhat.com/">answered</a>
that the two new callbacks should be called by the <tt>mmap()</tt>
callbacks of the exporting device driver. Instead of mapping to a typical
process space (as <tt>fault()</tt> would do), the goal is to map memory
between two device drivers. This
task may include complex conditions and allow accessing some memory by
a peer device, but not all of it.
<p>
The resulting memory region works as a
connection between the two sides of the peer-to-peer setup. It will not
necessarily be available to user space (or even the CPU itself).
Glisse explained that, in the GPU case, it is easier to add a
specific callback: the exporting device can check if the peer-to-peer
operation is possible and then allow this operation, or use main memory
instead. Setting up a fault handler, instead, would require numerous
additional flags and a 
fake page table to be filled and then interpreted by the other device,
according to Glisse. 

<h4>Views of device memory</h4>

<p>The discussion highlighted some differences in the handling of special
memory 
in the GPU and RDMA subsystems. It started with Glisse <a
href="/ml/linux-kernel/20190129205749.GN3176@redhat.com/">explaining
the GPU point of view</a>: the driver often updates the device page tables
that map memory within the GPU itself,  meaning that the CPU's mappings of
GPU memory are invalid most of the 
time.  There is a need for a method to tell the exporting driver that
another device wants to map one of its memory regions. If possible, the GPU
driver should 
avoid remapping the affected zone while it is mapped. The exporting device
must be able to decide 
whether it wants to authorize that mapping. Glisse noted that he also wants
to use 
the API in case of interconnected GPUs. In this case, the CPU page tables
are invalid and the physical addresses are the only meaningful ones;
the kernel has no information about what the addresses are. Glisse also gave an
overview of the <a
href="/ml/linux-kernel/20190129234752.GR3176@redhat.com/">GPU 
usage of HMM</a> and how things work <a
href="/ml/linux-kernel/20190130024851.GB10462@redhat.com/">without HMM</a>.</p>

<p>
One significant subthread of the discussion had to do with whether device
peer-to-peer memory should be represented in the CPU's memory map with <a
href="/Articles/565097/"><tt>page</tt> structures</a>.  For hardware where
HMM is in use, device memory behaves (mostly) like ordinary memory and,
thus, has those structures.  Jason Gunthorpe <a
href="/ml/linux-kernel/20190130041841.GB30598@mellanox.com/">commented</a>,
though, that in the case where HMM is not applicable (many RDMA settings,
for example), there are no <tt>page</tt> structures for this memory; he
would like things to stay that way.  Attempts to use <tt>page</tt>
structures for this memory, he said, always run into trouble.
<p>
Christoph
Hellwig <a href="/ml/linux-kernel/20190130080006.GB29665@lst.de/">replied</a>
that not having <tt>page</tt> structures can create even more trouble; some
functionalities, like <tt>get_user_pages()</tt> or
direct I/O to the underlying device memory, just do not work without
them. Deeper in the discussion, he <a 
href="/ml/linux-kernel/20190131081355.GC26495@lst.de/">listed
three uses</a> of <tt>struct page</tt> in the kernel: to keep the physical
address, to keep a reference of memory so that it does not go away while it
is still needed, and to set the dirty flag on the PTE after
writing to that memory.  Any solution that avoids <tt>struct page</tt>
would have to solve those problems first, he said.

<p><tt>get_user_pages()</tt>, which maps user-space memory into the
kernel's address space, is frequently called by drivers to access that
memory.  Whether it works or not will have a significant impact on how
peer-to-peer memory is used.  Some developers would like to see this memory
act like ordinary memory, with associated <tt>page</tt> structures, that
would be mappable with <tt>get_user_pages()</tt>.  
However, peer-to-peer memory is <a
href="/ml/linux-kernel/20190130233021.GD25486@mellanox.com/">I/O
memory that requires special handling</a>, Jason Gunthorpe noted, so it can
never look entirely like ordinary system memory.

Glisse would rather avoid supporting <tt>get_user_pages()</tt> for that
memory altogether. In the case of GPUs, it is not needed, he
noted.  Things turn out to be more complicated for the other potential user,
the RDMA layer, though. The developers <a
href="/ml/linux-kernel/35bad6d5-c06b-f2a3-08e6-2ed0197c8691@deltatee.com/">discussed</a>
other options like a special version of <tt>get_user_pages()</tt> for
DMA-only mappings.</p>

<p>Jason Gunthorpe <a
href="/ml/linux-kernel/20190130185652.GB17080@mellanox.com/">commented</a>
on the current state of the peer-to-peer implementation, which implements a type
of scatter-gather list (SGL) that references DMA memory only — there is no
mapping for the CPU. This is
different than the standard in-kernel SGLs that hold both the CPU and DMA
addresses. The NVMe and RDMA layers got fixes to support the special SGLs, but
he fears that some RDMA drivers may still break because they won't
understand those specific SGL semantics. Making <tt>get_user_pages()</tt>
work for all those cases will be a big job and there are conflicting
requirements, he said. He <a
href="/ml/linux-kernel/20190131190202.GC7548@mellanox.com/">also
suggested</a> promoting the peer-to-peer, DMA-only, scatter-gather lists to
general-use kernel structures.

<h4>Next steps</h4>

<p>The developers have not found a solution to all of the mentioned problems
yet. There are arguments for both keeping and avoiding <tt>struct
page</tt>. Using <tt>page</tt> structures would allow the use of
<tt>O_DIRECT</tt> and 
other APIs, but would require much additional work and fixing all issues in
the process. On the other hand, avoiding <tt>struct page</tt> will lead to
a type of special memory zone that needs to be handled in a particular way.
The decision seems to have potentially important consequences.
<p>
The choice
has not yet been made, and it seems that there will be more discussion needed
before there is a solution the developers can agree on. In addition to
that, future submissions of this patch set will probably need to
include examples of the API usage so that the developers can understand
them better. It seems likely that the peer-to-peer memory will be available
from user space some time in the future, but there is still important
work to be done first.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Device_drivers-Support_APIs">Device drivers/Support APIs</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#PCI">PCI</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Rybczynska_Marta">Rybczynska, Marta</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/782489/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor783485"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Controlling device peer-to-peer access from user space</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 19, 2019 16:11 UTC (Tue)
                               by <b>ScottMinster</b> (subscriber, #67541)
                              [<a href="/Articles/783485/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; He mentioned a number of possible use cases, including one device controlling another device's command queue. An example of this situation is a network card accessing a block device command queue so that it can submit storage transactions without the CPU's involvement.</font><br>
<p>
This sounds like it could really enhance those Thunderclap vulnerabilities (<a href="https://lwn.net/Articles/782381/">https://lwn.net/Articles/782381/</a>).  A network adapter that could send read (or write) commands to the storage device without any mediation from the main system seems dangerous.  While things would be fine with a well behaving device, could a rogue device read and transmit an entire drive with nothing on the system aware of it?  Or some other nefarious behavior writing to the drive.<br>
<p>
What sort of security precautions are there to mitigate a rogue device, especially one plugged into an external port?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/783485/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor834885"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Controlling device peer-to-peer access from user space</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 21, 2020 17:50 UTC (Wed)
                               by <b>imMute</b> (guest, #96323)
                              [<a href="/Articles/834885/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It&#x27;s the same vulnerability.  To sum it up: PCIe devices can initiated read and write commands.  Typically, those commands target system RAM (this is how DMA works).  Devices can just as easily target Memory or I/O space in other devices.<br>
The solution is the same: IOMMUs as firewalls between devices you want to segregate.<br>
<p>
<font class="QuotedText">&gt;could a rogue device read and transmit an entire drive with nothing on the system aware of it? </font><br>
Yes.  It&#x27;s exactly the same hole as reading and transmitting system RAM without the CPU noticing (except that it&#x27;s typically more involved to access disk data than it is to access RAM).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/834885/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2019, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
