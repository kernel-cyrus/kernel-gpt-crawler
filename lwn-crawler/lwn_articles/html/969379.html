        <!DOCTYPE html>
        <html lang="en">
        <head><title>Weighted memory interleaving and new system calls [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/969379/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/970329/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/969379/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Weighted memory interleaving and new system calls</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>This article brought to you by LWN subscribers</b>
<p>
Subscribers to LWN.net made this article &mdash; and everything that
       surrounds it &mdash; possible.  If you appreciate our content, please
       <a href="/Promo/nst-nag3/subscribe">buy a subscription</a> and make the next
       set of articles possible.
</blockquote>
<div class="FeatureByline">
           By <b>Daroc Alden</b><br>April 19, 2024</br>
           </div>
<p>
Gregory Price recently posted version 4 of
<a href="https://lwn.net/ml/linux-kernel/20231223181101.1954-1-gregory.price@memverge.com/">
a patch set</a> that adds support for weighted memory interleaving — allowing a
process's memory to be distributed between
<a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">
non-uniform memory access</a> (NUMA)
nodes in a more controlled way.
According to the performance measurements he includes,
the patch set could provide a
significant improvement for computers with network-attached memory.
The patch set also
introduces new system calls and paves the way for future extensions
intended to give processes more control over their own memory.
</p>

<p>
Modern computers can have a variety of kinds of memory in use at the
same time. Not just traditional NUMA between separate banks of RAM within the
same computer, but also memory distributed across a data center, like
<a href="https://en.wikipedia.org/wiki/Compute_Express_Link">
Compute Express Link</a> (CXL) attached memory.
These technologies allow
computers to support much larger amounts of memory, at the cost of
significantly complicating memory management and slower memory access speeds.
</p>

<p>
Current Linux kernels group different kinds of memory into tiers based on their
latency. LWN <a href="/Articles/948037/">covered</a> how they interact with an
earlier version of Price's patch set in October.
The kernel also allows configuring processes to have different pages of
their memory resident on different NUMA nodes. This spreads out the load
between the separate parts of memory, but it's not perfect. For one thing,
banks of memory can have different available bandwidths. The current
default behavior is
to assign allocated pages to different nodes in a round-robin way, which could
over-allocate the bandwidth of the least-capable bank, even if other banks have
more available capacity.
</p>

<p>
Price's patch set lets users specify unique weights for each NUMA node,
and uses those weights when distributing freshly allocated pages. These weights
are configured globally, but can be applied to specific processes using the
kernel's NUMA
<a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/numa_memory_policy.html">
memory-policy</a> support. Only tasks that have the new
<tt>MPOL_WEIGHTED_INTERLEAVE</tt> memory policy will use the weights.
</p>

<p>
The cover letter of the patch set includes a performance comparison
(contributed by several different people)
demonstrating how much better weighted interleaving can perform than the default
round-robin scheme. In brief, it compares four settings for the same workloads:
plain DRAM, CXL attached memory with the default interleaving policy, CXL memory
with global weights according to bandwidth, and "targeted" weights that use
different settings for the executable code, stack, and heap of the process. The
default interleaving policy is on average 78% slower than DRAM. The global
weights bring that performance to between 6% slower and 4% faster than DRAM
depending on workload, and correctly chosen targeted weights push the
performance to 2.5% to 4% better than DRAM.
</p>

<p>
Targeted weights have such a dramatic effect because different areas of a
process's memory can have different access patterns that give an advantage to one
memory policy
or another.
Memory policies for a whole process or a specific area of memory are
configured with
<a href="https://www.man7.org/linux/man-pages/man2/set_mempolicy.2.html">
<tt>set_mempolicy()</tt></a> and
<a href="https://man7.org/linux/man-pages/man2/mbind.2.html">
<tt>mbind()</tt></a>
respectively:
</p>

<pre>
    long set_mempolicy(int mode, const unsigned long *nodemask,
                       unsigned long maxnode);

    long mbind(void addr[.len], unsigned long len, int mode,
               const unsigned long nodemask[(.maxnode + ULONG_WIDTH - 1)
                                            / ULONG_WIDTH],
               unsigned long maxnode, unsigned int flags);
</pre>

<p>
The signature of <tt>mbind()</tt> introduces problems
for weighted memory interleaving, however; the signature cannot be extended,
because it is running up against the limits
of how many arguments can be provided to a system call (at most six).
Price's patch set rectifies this by
<a href="/ml/linux-kernel/20231223181101.1954-11-gregory.price@memverge.com/">
introducing a new system call</a> — <tt>mbind2()</tt> —
that takes a structure as an
argument, but otherwise performs the same function.
</p>

<pre>
    struct mpol_args {
      __u16 mode;
      __u16 mode_flags;
      __s32 home_node;
      __u64 pol_maxnodes;
      __aligned_u64 *pol_nodes;
      /* Optional: interleave weights for MPOL_WEIGHTED_INTERLEAVE */
      unsigned char *il_weights;    /* of size MAX_NUMNODES */
    };

    mbind2(unsigned long addr, unsigned long len, struct mpol_args *args,
           size_t size, unsigned long flags);
</pre>

<p>
The <tt>mpol_args</tt> struct is intended to be extensible over time, which is
why the system call includes the structure's size. Any new extensions to the
memory-policy infrastructure can add options to the end of the structure, and
old callers won't need to be updated. Price's patch set also adds
<tt>set_mempolicy2()</tt> and <tt>get_mempolicy2()</tt> using the same scheme,
to support setting or retrieving task-wide weights, respectively.
</p>

<p>
Like existing memory-policy settings, Price's new weighted-interleaving policy
is not a hard-and-fast rule. Setting a new memory policy does not migrate
existing pages (unless the <tt>MPOL_MF_MOVE</tt> option is specified), and if
the preferred NUMA node has no more memory available, the kernel will fall back
to using the next available NUMA node. Price
<a href="/ml/linux-kernel/ZYqEjsaqseI68EyJ@memverge.com/">
spelled this out explicitly</a> in
response to concerns brought up during review:
</p>

<blockquote class="bq">
<p>
This interface does not limit memory usage of a particular node, it
distributes data according to the requested policy.
</p>

<p>
Nuanced distinction, but important.  If nodes become exhausted, tasks
are still free to allocate memory from any node in the nodemask, even if
it violates the requested mempolicy.
</p>

<p>
This is consistent with the existing behavior of mempolicy.
</p>
</blockquote class="bq">

<p>
The weighted-interleaving
patch set has gathered relatively little commentary, perhaps because the
idea itself has been in progress for a long while. Price
<a href="/ml/linux-kernel/20231122211200.31620-1-gregory.price@memverge.com/">
posted a related patch set</a> in November that would have changed
<tt>set_mempolicy()</tt> to take a process ID as an additional argument. The change
would allow
privileged processes to set memory policies for other processes. At the time,
Price described the November patch set as being designed
"<q>to make mempolicy more flexible and extensible,
such as adding interleave weights (which may need to change at runtime
due to hotplug events)</q>". Because <tt>mbind()</tt> already passes six
parameters, however, that change would have needed new system calls as well. The
November patch set did not end up being merged. Now that Price's newer
weighted-interleaving patch set
introduces the needed system calls, it is possible that another version of the
older patch set will follow once the weighted-interleaving one is accepted.
</p>

<p>
Price's weighted-interleaving
patch set does seem likely to be merged [Update: a reader <a
href="/Articles/970546/">points out</a> that some of these changes, but not the
new system calls, were merged under a different name as part of 6.9],
given the impressive
number of Suggested-by tags in the cover letter and the minimal objections from
Ying Huang and Geert Uytterhoeven, who reviewed it.
It seems as though many people
are eager to have more control over how their processes' memory is distributed.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Tiered-memory_systems">Memory management/Tiered-memory systems</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/969379/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor970517"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 14:48 UTC (Fri)
                               by <b>SLi</b> (subscriber, #53131)
                              [<a href="/Articles/970517/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Does DRAM mean DDR directly attached to the relevant CPU's memory controller? So this is remote RAM that is faster than local RAM? How is that even possible? Or is it faster as in higher aggregate throughput but also higher latency?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970517/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor970520"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 15:10 UTC (Fri)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/970520/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>In this use case, DRAM is DDR attached to the CPU memory controller, while CXL memory is DRAM attached to a CXL memory controller attached to the CPU's CXL interface. The CXL-attached memory is slower than the directly attached DRAM under all circumstances; it's lower throughput and higher latency.
<p>The trick is that the test where the combination can be better than CPU-attached memory alone is highly parallel; with the right weighting, you're saturating the direct-attached memory, and then adding more transactions to the CXL bus such that the total throughput is higher, while latency is still acceptable to the test.


      
          <div class="CommentReplyButton">
            <form action="/Articles/970520/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor970534"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 15:48 UTC (Fri)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/970534/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
His patch set gets no commentary because it doesn't show up on linux-mm. He needs to fix his email.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970534/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor970568"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 20, 2024 0:30 UTC (Sat)
                               by <b>neggles</b> (subscriber, #153254)
                              [<a href="/Articles/970568/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Is this not on linux-mm? it looks like it's on linux-mm... <a href="https://lore.kernel.org/linux-mm/20231223181101.1954-1-gregory.price@memverge.com/">https://lore.kernel.org/linux-mm/20231223181101.1954-1-gr...</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970568/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor970546"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 17:43 UTC (Fri)
                               by <b>intelfx</b> (subscriber, #130118)
                              [<a href="/Articles/970546/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Triggered by willy's comment, I went to check the mailing lists...<br>
<p>
The patch set linked from the article is titled "[PATCH **v5** 00/11] mempolicy2, mbind2, and weighted interleave" and is timestamped 23 Dec 2023, but at the same time "[PATCH v5 0/4] mm/mempolicy: weighted interleave mempolicy and sysfs extension" There is also a v6, titled "[PATCH v6 00/12] mempolicy2, mbind2, and weighted interleave", timestamped 3 Jan 2024.<br>
<p>
Then, apparently at the suggestion of a reviewer who noted the new syscalls aren't that necessary, the patch set was trimmed to 3 patches and resent under a new name, "[PATCH 0/3] mm/mempolicy: weighted interleave mempolicy with sysfs extension". This one got 5 revisions, the final one I'm seeing (v5) totaling at 4 patches (with the third one not available on lore.kernel.org?) and timestamped 2 Feb 2024.<br>
<p>
Moreover, this one was apparently applied by Andrew Morton and included in the mm pull request during the 6.9 merge window.<br>
<p>
So which patch set are we discussing here? :-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970546/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor970548"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 17:45 UTC (Fri)
                               by <b>intelfx</b> (subscriber, #130118)
                              [<a href="/Articles/970548/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
(There was a copy-paste error, disregard the "but at the same time ..." piece.)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970548/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor970551"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 18:43 UTC (Fri)
                               by <b>daroc</b> (editor, #160859)
                              [<a href="/Articles/970551/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Ah, shoot. I missed both the v6 patch set and the trimmed version under the new name. Thank you for pointing those out.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970551/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor973124"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2024 20:05 UTC (Thu)
                               by <b>gmprice</b> (subscriber, #167884)
                              [<a href="/Articles/973124/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
For context, after a discussion with Andi Kleen (original author of mempolicy and maintainer of numactl), I decided to drop the syscall avenue until a clear use-case for task-local weights appeared.  The issue is not just that the additional syscalls are needed (which is already an incredibly tall ask), but that the management of task-local weights increased the complexity of the underlying system pretty considerably.  Getting global weights in sooner <br>
<p>
Ultimately the syscalls are waiting for a user.  If folks find a need for fine-grained weighted-interleave support on a per-task basis, then it can be revived pretty trivially.  However, the road for those extensions was much longer - so I chose not to let the good be the enemy of the perfect here and get the global weights in sooner rather than later.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/973124/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor970549"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 17:53 UTC (Fri)
                               by <b>jdulaney</b> (subscriber, #83672)
                              [<a href="/Articles/970549/">Link</a>] (13 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I wish I had a computer big enough to have network attached ram<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970549/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor970558"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 19, 2024 20:40 UTC (Fri)
                               by <b>atnot</b> (subscriber, #124910)
                              [<a href="/Articles/970558/">Link</a>] (11 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I think it's plausible that it isn't actually too far away if we consider a few things: <br>
- CXL is ultimately just based on commodity technology and will already be built into most CPU cores<br>
- The cost and complexity of high speed signalling to support discrete RAM modules at ever growing speeds is increasing rapidly and makes up a significant amount of motherboard cost<br>
- technologies like hbm and 3D stacking have sent on-chip memory amounts soaring into the gigabyte range on server chips <br>
- IBM Power 10 supports serial-attached memory already and, if somewhat expensive and impractical, is available to buy as a desktop workstation<br>
- Apple has been using on-package DRAM for their apple silicon for years now, to great power and success. Other companies have been mostly unable to replicate this because non-upgradeable memory is undesired by large swaths of their customer base. If only there was some readily technology that could allow extending on-package dram...<br>
<p>
All of the pieces for a world in which your CPU comes with, say, 4-8GB of dram which you can extend via cxl memory slots are already there. It's also, at least to me, a fairly obvious direction to head in at least for desktop platforms. It's just a question of if and on what timeframe they'll do it and how much they'll get tempted and be annoying with market segmentation.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/970558/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972621"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 6, 2024 20:50 UTC (Mon)
                               by <b>immibis</b> (guest, #105511)
                              [<a href="/Articles/972621/">Link</a>] (10 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
why would it be CXL instead of DDR?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972621/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972654"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2024 10:56 UTC (Tue)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/972654/">Link</a>] (9 responses)
      </p>
      
      </div>
      </summary>
      <p>Because <a href="https://www.intel.com/content/www/us/en/docs/programmable/772538/24-1-6-1-0/ddr5-board-design-guidelines.html">DDR is a pain to route on a PCB</a>; you have a set of complicated rules to follow to get 32 (or more for ECC) data signals, 13 command/address signals, 4 data masks, plus various clocking signals all routed correctly, with impedance matching and time differences between the various signals kept under control. And all of these signals are differential signals, so they have to be routed as pairs, not as single lines.
<p>In contrast, CXL reuses the PCIe lower layers, with a different protocol on top. PCIe needs you to route 2 differential signals as an impedance matched, time matched set, which is much easier to get right than routing the 50-odd signals you need to route as a set for a single DDR5 chip. And note that if you want wider (e.g. x16), you still route each lane as a set of 4 wires, not routing 64 in one go). This is much simpler to get right.
<p>Against that, DDR is much lower latency than CXL (although the throughput is the same), and CXL products (for quite some time) will be built with a CXL to DDR bridge, instead of with native CXL DRAM.
<p>This is a good place for innovation - does caching solve this, by adding more layers (maybe eDRAM or DDR based)? Do we need NUMA policies as in this article to do well? Or will we solve the latency problem differently?


      
          <div class="CommentReplyButton">
            <form action="/Articles/972654/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972712"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2024 16:30 UTC (Tue)
                               by <b>atnot</b> (subscriber, #124910)
                              [<a href="/Articles/972712/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yes, DDR is just an absolutely royal PITA. This explanation is pretty good, but I'll add a few things:<br>
<p>
- This is just what you need for *one channel*. Modern CPUs need at bare minimum two channels to be performant, with up to 12 on enterprise CPUs and commonly two DIMMs per channel. This means that on a high end system with ECC (up to 80 pins per DIMM), you have to route on the order of 1000 signals to 2000 locations with sub-nanosecond precision within each channel. This is no small feat even on 30+ layer boards, which do not come cheap either. I tried doing even just DDR2 once and it's not an experience I want to repeat.<br>
<p>
- Just how basic the DDR protocol is. You're dealing with super low level stuff like pre-charging and doing memory refreshes. The CPU is pretty much just directly wired to the controls of some amplifiers with a big multiplexer behind it. This is why the latency is pretty unbeatable, but also why it is such a pain.<br>
<p>
- One of the issues in modern chip design is that the majority of energy is spent not on doing computations, but just moving data around. This is especially bad for things like memcpy. You're pushing 20cm of electrons in and out of the CPU just to copy data a few millimeters. What you really want is a little processor directly on the module or in the memory chip that can do these things for you much more efficiently. CPUs have already been moving in that direction with things like caches being able to perform simple arithmetic. But that sort of stuff isn't really possible with such a low level interface.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972712/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972720"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 7, 2024 16:44 UTC (Tue)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/972720/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <p>Note, too, that in DDR5, DIMMs have an even number of channels; you thus have to route more signals than I've described if you're using DIMMs, not just soldered-down chips. And an ECC DIMM is 80 data signals, plus the command/address signals, plus clocking signals (like strobes and clocks), plus chip selects, plus power. It's not trivial to route all 270 used pins of a 288 pin DIMM correctly - in large part because over 100 of them are signals whose routing is constrained by the rest of the pins in the "constrained routing" set.


      
          <div class="CommentReplyButton">
            <form action="/Articles/972720/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972773"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 9:24 UTC (Wed)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/972773/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From the above, it sounds like packets-over-serial interface for system RAM is inevitable. I guess the industry is stuck waiting for RAMBUS patents to expire?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972773/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972774"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 10:02 UTC (Wed)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/972774/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <p>No need to wait on RAMBUS patents - CXL is already packets over a PCIe compatible serial interface. Indeed, looking at CXL 3.1, it's built atop the PCIe data link layer, with some optional parts of PCIe made compulsory, and extra messages to support cache coherency protocols.
<p>Interestingly, it's also designed so that it's possible to create a card that falls back to PCIe when attached to a non-CXL host. This would allow an SSD or coprocessor that benefits from CXL's cache coherency to simply slow down if you attach to a PCIe-only host.


      
          <div class="CommentReplyButton">
            <form action="/Articles/972774/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972783"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 11:00 UTC (Wed)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/972783/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Ah, course. :) What's the latency like? RAMBUS was 2 to 3x the latency of DDR. I guess CXL is at least that if not a bit worse again?<br>
<p>
Could conceivably see another level in the memory/storage hierarchy: serial DRAM for main memory, and 2 slots of DDR for a slightly faster intermediate large cache between on-die L3 and serial DRAM.<br>
<p>
L1: on-core I$ and D$, 32 to 128 KiB<br>
L2: on-core, 512 to 2 MiB<br>
L3: core-complex level shared cache, 1 to 32 MiB<br>
L4: cross-socket cache, DDR on system board, 12 to 24 CAS latency, 8 to 128 GiB<br>
L5: System RAM, 40+ cycle access latency, GiBs to TBs of RAM<br>
L6: (In large cluster systems) Cross-node RAM, X00 ns access latency.<br>
<p>
And of course SSD and other storage after that, as is traditional.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972783/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972785"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 11:24 UTC (Wed)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/972785/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>There's about a 50ns interface penalty in each direction, assuming CXL.mem directly attached, and not via switches, as compared to DDR5's interface.
<p>Exactly how painful this is depends on what you're doing; in the best case, both interfaces need a single command over the interface to start a cacheline read. In the worst case for CXL.mem you'd send a read command, and have a complete cacheline come back about 100 ns later, where DDR5 would get you the "critical" byte in about 10 ns, and the full cacheline in about 50 ns total. In the best case for CXL.mem, latency is dominated by driving the DRAM state machine, and CXL.mem is about the same as DDR5 for latency. Exactly what you'll experience depends on the access patterns.



      
          <div class="CommentReplyButton">
            <form action="/Articles/972785/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor972781"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 11:08 UTC (Wed)
                               by <b>atnot</b> (subscriber, #124910)
                              [<a href="/Articles/972781/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I'd hypothesize what the industry is really waiting for, aside from the technology generally maturing, is the price of 3D stacking and on-package memory to drop. We've gone a lot into the benefits, but the latency penalty is a really tough pill to swallow. Consider how much AMD had to increase the cache amounts on their cpus just to mitigate the latency increase from chiplets. We went from a few megabytes to nearly a gigabyte in just a few years. I expect we'll have to see at least another magnitude of increase in on-package memory to really make this viable for system RAM. That is, gigabytes of on-package memory on the desktop and tens of gigabytes on servers. We're still pretty far away from being able to do that cost effectively.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972781/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972786"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 11:23 UTC (Wed)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/972786/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You mean MiBs of on-package memory, rather than GiBs, right? E.g., Zen4 EPYCs have 32 MiB of L3 RAM per CCX, so 384 MiB total for the highest CCX chips?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972786/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor972787"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 8, 2024 12:18 UTC (Wed)
                               by <b>atnot</b> (subscriber, #124910)
                              [<a href="/Articles/972787/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That's true for the base die, but they can stack another 64MiB of cache on top for 96MiB per CCX giving a total 768MB total in Milan-X. However they have now also released Genoa-X featuring up to 12 CCX for a total of 1152MiB.<br>
<p>
To be fair, these chips _are_ edge cases and rarely actually make sense economically. They pretty much only exist because of the perverse incentives of software with per-core licensing models. But they're a thing, you can buy them.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/972787/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor973129"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Weighted memory interleaving and new system calls</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2024 20:26 UTC (Thu)
                               by <b>gmprice</b> (subscriber, #167884)
                              [<a href="/Articles/973129/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You are likely to have a computer that can make use of this extension in the very near future, no network attached DRAM required!<br>
<p>
It's very likely we will see systems with at least 3-4 tiers of general purpose memory show up in a 1-or-2-socket server or workstation very soon.<br>
<p>
Local Socket HBM + Local Socket DDR + Local Socket CXL + Remote Socket HBM + Remote Socket DDR + Remote Socket CXL<br>
<p>
Of course, any remote socket accesses tend to happen over shared interconnects, and so they really need to all get collapsed down to a "Single tier" from the perspective of a local socket.  So the above 6-tier system is functionally a 4-tier system, depending on how hard you squint at it and how many lies you want to tell yourself about where a particular cacheline happens to live.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/973129/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2024, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
