        <!DOCTYPE html>
        <html lang="en">
        <head><title>Zoned storage and filesystems [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/932748/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/932928/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/932748/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Zoned storage and filesystems</h1>
</div>
<div class="ArticleText">
<div class="FeatureByline">
           By <b>Jake Edge</b><br>May 25, 2023</br>
           <hr>
<a href="/Articles/lsfmmbpf2023">LSFMM+BPF</a>
</div>
<p>
Issues around zoned storage for filesystems was the topic of a combined
storage and filesystem session at 
<a href="/Articles/lsfmmbpf2023">2023 Linux Storage, Filesystem,
Memory-Management and BPF Summit</a> led by
Bart Van Assche, 
Viacheslav A. Dubeyko, and Naohiro Aota.  Zoned storage began with the
advent of <a
href="https://en.wikipedia.org/wiki/Shingled_magnetic_recording">shingled
magnetic recording</a> (SMR) devices, but is now implemented by <a
href="https://zonedstorage.io/docs/introduction/zns">NVMe zoned
namespaces</a> (ZNS) as well.
SMR devices can have multiple zones with different
characteristics, with some zones that can only be written in sequential
order, while other, conventional zones can be written in any order.  The
talk was focused on filesystems using the sequential type of zones
since the conventional zones are already well-supported in Linux and its
filesystems. 
</p>

<a href="/Articles/932828/">
<img src="https://static.lwn.net/images/2023/lsfmb-vanassche-sm.png" border=0 hspace=5
align="right" alt="[Bart Van Assche]" title="Bart Van Assche" width=210
height=280>
</a>

<p>
Van Assche began by giving an overview of zoned storage and its advantages;
he quickly went through some bullet points from <a
href="https://docs.google.com/presentation/d/1u109nKNbDA0b7vzeeYUE_6hZ2PBy3XLAWutDl7XA464">the talk
slides</a>.  For NAND flash devices, having sequential zones means that
they can 
have a smaller logical-to-physical (L2P) mapping table, which improves
performance.  In addition, these zones
eliminate internal garbage collection and the consequent write
amplification, which 
allows the host to  have better control over the latency of writing to the
device.  Read 
performance can also be improved because filesystems can allocate
a contiguous logical-block-address (LBA) range for files.
</p>

<p>
He then turned to the zoned-storage interface.
Zones are contiguous LBA ranges that do not overlap with other zones;
multiple zones can be written simultaneously.
There are four states for a zone: empty, open, closed, or full.  Zones that
are either open or closed are considered active; devices may have limits on
the number of active zones.
</p>

<h4>Powers of two</h4>

<p>
He stated that the NVMe standard specifies that zone sizes are always a
power of two, but was corrected by several attendees.  Linux imposes that
restriction, not the standard.  Multiple NAND flash vendors want to be able
to have non-power-of-two (npo2) zone sizes.  In particular, vendors of <a
href="https://en.wikipedia.org/wiki/Universal_Flash_Storage">Universal
Flash Storage</a> (UFS) devices want more flexibility in the zone sizes.
</p>

<p>
Pankaj Raghav of Samsung has posted
<a
href="/ml/linux-kernel/20220923173618.6899-1-p.raghav@samsung.com/">patches</a>
for supporting zone sizes that are not a power of two.  Android also needs
this support, Van Assche said.  He wondered if the patches were ready to go
upstream at this point.  He was hoping that block maintainer Jens Axboe
would be present for 
the discussion, but that was not the case.
</p>

<p>
Josef Bacik wondered if the Linux filesystems community really cared one
way or the other.  He asked Johannes Thumshirn if Btrfs cared, for
example.  Thumshirn said that he thought it would be messy to support npo2,
but that the problems could perhaps be considered bugs and get fixed.
Bacik asked how many of these devices actually exist today.  Damien Le Moal
said that effectively everything on the market today has zones that are
sized as a power of two.
</p>

<a href="/Articles/932828/">
<img src="https://static.lwn.net/images/2023/lsfmb-dubeyko-sm.png" border=0 hspace=5
align="left" alt="[Viacheslav A. Dubeyko]" title="Viacheslav A. Dubeyko" 
width=215
height=280>
</a>

<p>
Le Moal said that his view is that flash-based zones should look like the
existing SMR sequential zones, all of which have sizes that are a power of
two.   As yet, there are few deployed flash-based zoned-storage systems, so
avoiding confusing things between SMR and flash devices was desirable.  The
UFS vendors 
are trying to push npo2 to avoid having to add more functionality in their
firmware, he said. "Do we want to take the burden of dealing with the
non-power-of-two, instead of the drive vendors doing it?"
</p>

<p>
Van Assche said it is more than just UFS vendors that would like to do
this.  Le Moal would still prefer that the drive vendors handle this and he
does not see why there would be performance problems in doing so, as has
sometimes come up.  Others disagreed, or at least thought that there was
enough push for npo2 from customers of various sorts that <i>something</i>
should be done.  One attendee suggested a middle layer that would mediate
between the filesystems and devices; extracting maximum performance is not
really needed for these devices.  "Let's just be done with it, please."
From the frustration expressed, it is clear that the topic has come up a
lot without getting resolved. 
</p>

<p>
Bacik said that he truly did not care, and thought that was generally true
for filesystem people, but he would also like to see this
problem resolved in some fashion.  He looked briefly at the patches, which
did not seem too invasive to him; "I'm not the block-layer guy, so I could
be wrong, and Jens isn't here to yell at me".  He does not understand "why
we are fighting about this, if it's not that big of a deal to support".
</p>

<p>
Someone pointed out that Christoph Hellwig was adamantly opposed to the
npo2 support; "now I understand it", Bacik said with a laugh.  Hannes
Reinecke suggested that even the middle-layer approach that was suggested
would get strong opposition from Hellwig (who was at the summit, but not at
this discussion).  Le Moal said that so far all of the reasons he has heard
for supporting npo2 in the kernel were wrong and demonstrate a
misunderstanding of zoned storage on the part of device makers.  If that
support goes into the kernel, it should only be done if there are sensible
reasons to do so, he said.
</p>

<p>
There was a fair amount of disagreement in the room, with people talking
over each other and several simultaneous side conversations taking place.
It was not particularly heated, but was somewhat chaotic and hard to follow.
Van
Assche said that there were not good arguments either for or against the npo2
support in his mind, but Android, at least, is being pushed hard by the
storage vendors.  The ultimate decision is Axboe's, Bacik said; more
discussion of it in the room is not really going to change anything, so he
suggested moving on.
</p>

<h4>Zoned Btrfs</h4>

<a href="/Articles/932828/">
<img src="https://static.lwn.net/images/2023/lsfmb-aota-sm.png" border=0 hspace=5
align="right" alt="[Naohiro Aota]" title="Naohiro Aota" width=191
height=280>
</a>

<p>
At that point, Aota switched over to the status of <a
href="/Articles/853308/">zoned-storage support in 
Btrfs</a>, which he has been <a href="/Articles/788851/">working on</a> for a
number of years now.  Btrfs supports both SMR and ZNS devices, with the
latter added for the&nbsp;5.16 
kernel.  SMR works well, but there are some problems with the ZNS support,
he said.
</p>

<p>
Currently, Btrfs on ZNS can report <tt>ENOSPC</tt> even when there is still
space on the device due to zones being activated at reservation time,
rather than only while data is being written.  That means there may be no
zones available to be activated when data needs to be written.  There can
also be slow performance because  metadata overcommit is disabled in Btrfs
on zoned storage.  He is reworking some of the code to address these
problems, he said, which will allow the metadata overcommit to be re-enabled.
</p>

<h4>Zone sizing</h4>

<p>
Dubeyko then shifted gears to another topic: what is the best zone size
based on the differing needs of filesystems and SSD devices?   Smaller
zones (hypothetically 128KB) are more complicated for
the device because they require a huge 
mapping table and a complex mapping scheme.  But, for a filesystem, a small
zone can
have smaller extents, with faster reclaim, lower garbage-collection overhead,
and faster read I/O, he said.  Larger zones (2GB, for example) have a lot
of negatives for filesystems, but are much easier for the devices.  He
wondered if it might make sense to allow filesystems to choose among a few
different zone sizes for a device.
</p>

<p>
Le Moal said that the zone size and overall capacity of the device have to
work together.  A 16TB drive with 128KB zones is "going to suffer"; the
number of zones in the device makes a difference.  He said that it is also not
something that 
can be changed at the software level; it is up to the drive vendors to
choose a zone size that makes the most sense for the most use cases of
their hardware.
</p>

<p>
One attendee said that they think the next generation of ZNS drives will
generally have zones of around&nbsp;50 or&nbsp;100MB, and wondered if that was a
reasonable size for filesystems.  He believes that the&nbsp;1-2GB zones
used in current 
devices will likely be around&nbsp;100MB in devices for high-volume
deployments. 
Ted Ts'o said that he was confused why the zone size was even being
discussed in the room, "because, ultimately, I don't think it's up to us".
The market will dictate its needs to the vendors, so if a high-volume handset
maker, such as Samsung, were to say that it wants UFS devices with zones of
a certain size, that's what will be built, he said.  Others generally
agreed with that as time
ran out on the session.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer-Zoned_devices">Block layer/Zoned devices</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems">Filesystems</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_Memory-Management_and_BPF_Summit-2023">Storage, Filesystem, Memory-Management and BPF Summit/2023</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/932748/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor933079"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zoned storage (SMR) drive end-user opacity</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 26, 2023 15:13 UTC (Fri)
                               by <b>Hobart</b> (subscriber, #59974)
                              [<a href="/Articles/933079/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Has anyone found a consumer SMR drive that even tells the device owner what the status of its SMR behavior is?<br>
<p>
There's been protocol support for years - IIRC Ted T'so had a patch set for extfs to interact with them in a more optimal way - but IDK if any HD manufacturer has actually made that available to the end user.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933079/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor933840"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zoned storage (SMR) drive end-user opacity</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 5, 2023 5:34 UTC (Mon)
                               by <b>faramir</b> (subscriber, #2327)
                              [<a href="/Articles/933840/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I tried to figure this out at one point and it seems the issue is:<br>
<p>
Drive Managed SMR (consumer drives) vs. Host Managed SMR (enterprise drives).<br>
<p>
Consumer SMR (DM) drives don't implement the protocol and who knows what their<br>
firmware is doing underneath.   Some people say that they do something like modern SSD<br>
drives (TLC or QLC) which use part of the flash as 'pseudo-SLC' caches in order to provide<br>
a fast write cache which might eventually be moved to TLC/QLC when the SSD gets less busy.<br>
So part of a DM SMR drive might actually not be SMR.  Also, the algorithms involved are almost<br>
certainly considered vendor proprietary.  It's not even clear it would be reasonable for a DM SMR<br>
drive to attempt to say anything to the Host as it might change it's layout on the fly.<br>
<p>
Or at least the above was my take away before I gave up on the subject.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933840/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor933195"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zone size = hardware erase block size?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 29, 2023 23:39 UTC (Mon)
                               by <b>DemiMarie</b> (subscriber, #164188)
                              [<a href="/Articles/933195/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Why not have the zone size be equal to the hardware erase block size?  That is the smallest value that makes sense, and anything bigger is going to make the lives of filesystem writers unnecessarily harder.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933195/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor933247"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zone size = hardware erase block size?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 30, 2023 11:53 UTC (Tue)
                               by <b>adobriyan</b> (subscriber, #30858)
                              [<a href="/Articles/933247/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Internally zones are spread over multiple erase blocks to get better performance on sequential workloads (which constitute 100% of write workloads).<br>
You can guess mapping topology by dividing ZSZE/ZCAP by erase block size.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933247/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor933499"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zone size = hardware erase block size?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 1, 2023 22:38 UTC (Thu)
                               by <b>DemiMarie</b> (subscriber, #164188)
                              [<a href="/Articles/933499/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How much does this improve storage performance compared to writing to multiple zones at the same time?  Is it worth the decrease in filesystem performance?  Or are zoned storage devices primarily intended for workloads that bypass the filesystem and handle everything in userspace?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933499/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor933542"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zone size = hardware erase block size?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 2, 2023 11:25 UTC (Fri)
                               by <b>adobriyan</b> (subscriber, #30858)
                              [<a href="/Articles/933542/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<span class="QuotedText">&gt; Or are zoned storage devices primarily intended for workloads that bypass the filesystem and handle everything in userspace?</span><br>
<p>
Any application which expects to overwrite data in-place or write data randomly in LBA space is immediately disqualified from using zoned devices and must become small database-like engine (hello, O_DIRECT!) which writes is very precise order (less headache with Zone Append).<br>
<p>
So, all classic block/extent filesystems are out. In theory, journalling filesystem may accept zoned device for external journal.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/933542/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor935186"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Zone size = hardware erase block size?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 19, 2023 14:10 UTC (Mon)
                               by <b>DemiMarie</b> (subscriber, #164188)
                              [<a href="/Articles/935186/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From my perspective, ZNS essentially moves the flash translation layer from the device to the host.  Linux filesystems (other than zonefs) running on zoned storage expose a full POSIX API, and that includes random write support.  The question is what zone size will yield optimal performance for workloads that are random-write from a userspace perspective.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/935186/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2023, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
