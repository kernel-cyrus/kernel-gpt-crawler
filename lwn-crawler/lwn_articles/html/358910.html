        <!DOCTYPE html>
        <html lang="en">
        <head><title>JLS2009: Generic receive offload [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/358910/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/358221/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/358910/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>JLS2009: Generic receive offload</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>October 27, 2009</br>
           </div>
Your editor still remembers installing his first Ethernet adapter.  Through
the expenditure of massive engineering resources, DEC was able to squeeze
this device onto a linked pair of UNIBUS boards - the better part of a
square meter of board space in total - so that a VAX system could be put
onto a modern network.  Supporting 10Mb/sec was a bit of a challenge in
those days.  In the intervening years, leading-edge network adaptors have
sped up to 10Gb/sec - a full three orders of magnitude.  Supporting them is
<i>still</i> a challenge, though for different reasons.  At the 2009 Japan
Linux Symposium, Herbert Xu discussed those challenges and how Linux has
evolved to meet them.
<p>
Part of the problem is that 10G Ethernet is still Ethernet underneath.
There is value in that; it minimizes the changes required in other parts of
the system.  But it's an old technology which brings some heavy baggage
with it, with the heaviest bag of all being the 1500-byte maximum transfer
unit (MTU) limit.  With packet size capped at 1500 bytes, a 10G network
link running at full speed will be transferring over 800,000 packets per
second.  Again, that's an increase of three orders of magnitude from the
10Mb days, but CPUs have not kept pace.  So the amount of CPU time
available to process a single Ethernet packet is less than it was in the
early days.  Needless to say, that is putting some pressure on the
networking subsystem; the amount of CPU time required to process each
packet must be squeezed wherever possible.
<p>
(Some may quibble that, while individual CPU speeds have not kept pace, the
number of cores has grown to make up the difference.  That is true, but the
focus of Herbert's talk was single-CPU performance for a couple of reasons:
any performance work must benefit uniprocessor systems, and distributing a
single adapter's work across multiple CPUs has its own challenges.)
<p>
Given the importance of per-packet overhead, one might well ask whether it
makes sense to raise the MTU.  That can be done;  the "jumbo frames"
mechanism can handle packets up to 9KB in size.  The problem, according to
Herbert, is that "the Internet happened."  Most connections of interest go
across the Internet, and those are all bound by the lowest MTU in the

<a href="/Articles/358922/"><img
src="https://static.lwn.net/images/conf/ks-jls-09/herbert-xu-sm.jpg" width=125 height=156
alt="[Herbert Xu]" border=0 align="right" hspace=2></a>

entire path.  Sometimes that MTU is even less than 1500 bytes.
Protocol-based mechanisms for finding out what that MTU is exist, but they
don't work well on the Internet; in particular, a lot of firewall setups
break it.  So, while jumbo frames might work well for local networks, the
sad fact is that we're stuck with 1500 bytes on the wider Internet.
<p>
If we can't use a larger MTU, we can go for the next-best thing: pretend
that we're using a larger MTU.  For a few years now Linux has supported
network adapters which perform "TCP segmentation offload," or TSO.  With a
TSO-capable adapter, the kernel can prepare much larger packets (64KB, say)
for outgoing data; the adapter will then re-segment the data into smaller
packets as the data hits the wire.  That cuts the kernel's per-packet
overhead by a factor of 40.  TSO is well supported in Linux; for systems
which are engaged mainly in the sending of data, it's sufficient to make
10GB work at full speed.
<p>
The kernel actually has a generic segmentation offload mechanism (called
GSO) which is not limited to TCP.  It turns out that performance improves
even if the feature is emulated in the driver.  But GSO only works for data
transmission, not reception.  That limitation is entirely fine for broad
classes of users; sites providing content to the net, for example, send far
more data than they receive.  But other sites have different workloads,
and, for them, packet reception overhead is just as important as
transmission overhead.
<p>
Solutions on the receive side have been a little slower in coming, and not
just because the first users were more interested in transmission
performance.  Optimizing the receive side is harder because packet
reception is, in general, harder.  When it is transmitting data, the kernel
is in complete control and able to throttle sending processes if
necessary.  But incoming packets are entirely asynchronous events, under
somebody else's control, and the kernel just has to cope with what it gets.
<p>
Still, a solution has emerged in the form of "large receive offload" (LRO),
which takes a very similar approach: incoming packets are merged at
reception time so that the operating system sees far fewer of them.  This
merging can be done either in the driver or in the hardware; even LRO
emulation in the driver has performance benefits.  LRO is widely supported
by 10G drivers under Linux.
<p>

But LRO is a bit of a flawed solution, according to Herbert; the real
problem is that it "merges everything in sight."  This transformation is
lossy; if there are important differences between the headers in incoming
packets, those differences will be lost.  And that breaks things.  If a
system is serving as a router, it really should not be changing the headers
on packets as they pass through.  LRO can totally break satellite-based
connections, where some very strange header tricks are done by providers to
make the whole thing work.  And bridging breaks, which is a serious
problem: most virtualization setups use a virtual network bridge between
the host and its clients.  One might simply avoid using LRO in such
situations, but these also tend to be the workloads that one really wants
to optimize.  Virtualized networking, in particular, is already slower; any
possible optimization in this area is much needed.
<p>
The solution is generic receive offload (GRO).
In GRO, the criteria for which packets can be merged is greatly
restricted; the MAC headers must be identical and only a few TCP or IP
headers can differ.  In fact, the set of headers which can differ is
severely restricted: checksums are necessarily different, and the IP ID
field is allowed to increment.  Even the TCP timestamps must be identical,
which is less of a restriction than it may seem; the timestamp is a
relatively low-resolution field, so it's not uncommon for lots of packets
to have the same timestamp.  As a result of these restrictions, merged
packets can be resegmented losslessly; as an added benefit, the GSO code
can be used to perform resegmentation.
<p>
One other nice thing about GRO is that, unlike LRO, it is not limited to
TCP/IPv4. 
<p>
The GRO code was merged for 2.6.29, and it is supported by a number of 10G
drivers.  The conversion of drivers to GRO is quite simple.  The biggest
problem, perhaps, is with new drivers which are written to use the LRO API
instead.  To head this off, the LRO API may eventually be removed, once the
networking developers are convinced that GRO is fully functional with no
remaining performance regressions. 
<p>
In response to questions, Herbert said that there has not been a lot of
effort toward using LRO in 1G drivers.  In general, current CPUs can keep
up with a 1G data stream without too much trouble.  There might be a
benefit, though, in embedded systems which typically have slower
processors.  How does the kernel decide how long to wait for incoming
packets before merging them?  It turns out that there is no real need for
any special waiting code: the NAPI API already has the driver polling for
new packets occasionally and processing them in batches.  GRO can simply be
performed at NAPI poll time.
<p>
The next step may be toward "generic flow-based merging"; it may also be
possible to start merging unrelated packets headed to the same destination
to make larger routing units.  UDP merging is on the list of things to do.
There may even be a benefit in merging TCP ACK packets.  Those packets are
small, but there are a lot of them - typically one for every two data
packets going the other direction.  This technology may go in surprising
directions, but one thing is clear: the networking developers are not short
of ideas for enabling Linux to keep up with ever-faster hardware.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Networking">Networking</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/358910/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor359287"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 3:58 UTC (Thu)
                               by <b>felixfix</b> (subscriber, #242)
                              [<a href="/Articles/359287/">Link</a>] (10 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Back when Token Ring and Ethernet were battling it out, I was amazed at the sloppiness of the Ethernet protocol with collision detection at its heart. It seemed so clunky, and some reports said Ethernet collisions with even a few nodes slowed it down to Token RIng's 4Mbps.  I never quite understood why Token Ring lost out ... but I didn't envision the big networks that came down the pike where Token Ring would have been lost.  Switches made a huge difference compared to hubs, too.<br>
<p>
Have there been any attempts to replace Ethernet that have actually gone anywhere?  I suppose if there had been, I wouldn't be asking.  But the idea of looking for collisions on so many dinky packets seems even clunkier to me now.  What kind of real thruput do 10Gbps networks actually achieve today?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359287/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359289"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 4:11 UTC (Thu)
                               by <b>agrover</b> (guest, #55381)
                              [<a href="/Articles/359289/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
All 1Gb and above Ethernet is full duplex, point to point, no hubs (only switches) and therefore collisions are not an issue. 10Gbps networks can hit wire speed, taking header overhead and interframe gap into account. Like the article says, the big problem really is that the Ethernet packet has not scaled with the enormous throughput increase.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359289/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359295"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 4:57 UTC (Thu)
                               by <b>felixfix</b> (subscriber, #242)
                              [<a href="/Articles/359295/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Makes sense ... hadn't thought of it that way.  If there are no more collisions, is there any reason to keep collision detection in the spec?  I seem to recall the packet preamble had to be long enough to cover the collision detection.  Would it add anything to get rid of that?<br>
<p>
It's funny how the old cable strung from one node to the next has stuck in my mind even tho I haven't seen that kind of install in ages.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359295/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor359350"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 11:08 UTC (Thu)
                               by <b>epa</b> (subscriber, #39769)
                              [<a href="/Articles/359350/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>All 1Gb and above Ethernet is full duplex, point to point, no hubs (only switches) and therefore collisions are not an issue.</blockquote>So it doesn't really make sense to call it Ethernet any more; mostly just a marketing name.
      
          <div class="CommentReplyButton">
            <form action="/Articles/359350/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359382"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 14:01 UTC (Thu)
                               by <b>SEJeff</b> (guest, #51588)
                              [<a href="/Articles/359382/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Doesn't make sense to call it CSMACD if there won't ever be collisions at the <br>
very least.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359382/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359592"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 30, 2009 15:47 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/359592/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <p>
There appears to be interoperability that justifies calling the new protocol Ethernet.  You can replace one link of an existing Ethernet network with a 10G one and it all still works -- and not by simply negotiating down to the old protocol.
<p>
My understanding is that is what makes the 1500 byte MTU a constraint on 10GE.
<p>
Also:  Can you replace a 1G ethernet card in a computer with a 10G and now talk at full speed (to a 10G peer) without replacing the device driver or anything else in the computer?

      
          <div class="CommentReplyButton">
            <form action="/Articles/359592/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359675"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 30, 2009 20:59 UTC (Fri)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/359675/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There are lots of 10G cards that also do 1G, but I don't think you'll find <br>
any device drivers for 1G-only cards that work with the quite different <br>
hardware that does 10G :))<br>
<p>
Fundamentally though most of the hardware (as in the physical components <br>
on the 10G board) that does 10G is also used to do 1G; and most of the <br>
hardware that does 1G is also used to do 100M. The protocols could be <br>
considered distinct, but they're so similar at heart that giving them the <br>
same name seems sensible to me.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359675/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor359421"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 16:50 UTC (Thu)
                               by <b>Shewmaker</b> (guest, #1126)
                              [<a href="/Articles/359421/">Link</a>] 
      </p>
      
      </div>
      </summary>
      Collisions are not an issue ... data is not lost immediately, but contention at a transmit port of a switch is a problem.  When that happens, one packet gets transmitted while the other is placed on a queue.  That queue may be in memory shared by all ports, a subset of ports (a blade in the switch, for instance), and some switches allow you to set aside dedicated memory for a port's queue.
<p>
I've got a simple illustration in this <a href="http://www.issdm.ucsc.edu/sites/default/files/Shewmaker.pdf">presentation</a>.
<p>
Packet loss occurs when the queue overflows, and TCP uses that as a signal to slow down.  It has worked remarkably well over the years, but switches have been making it work partly be increasing the amount of memory for the queues.  Large queues cause large variations in delay and mean that TCP will have increasingly more packets in flight before it knows that it needs to slow down.
<p>
Infiniband has a reliable (connected) mode that allows packets with an MTU of 64k, which is quite nice for high performance storage networks and parallel file systems that are IP based.
<p>
There are new Ethernet standards coming out that have equivalents to Infiniband's features, but I don't know how much longer until products are out.  <a href="http://en.wikipedia.org/wiki/Data_center_bridging">Data Center Ethernet and Converged Enhanced Ethernet</a>
<p>
Incidentally, my research is focused on providing performance guarantees even on current inexpensive Ethernet hardware (LAN, not WAN).  I expect it will also be beneficial on the reliable (i.e. lossless) network technologies as it allows arbitrary guarantees (unfairness) that are more flexible than priority based QoS schemes.  

      
          <div class="CommentReplyButton">
            <form action="/Articles/359421/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor359495"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 19:41 UTC (Thu)
                               by <b>mebrown</b> (subscriber, #7960)
                              [<a href="/Articles/359495/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
"All 1Gb and above Ethernet is full duplex, point to point"<br>
<p>
Actually not true. When 1Gb Ethernet first came out, you could buy actual hubs which had actual collisions and the wonderful-ness (or lack thereof) that entailed (meaning half-duplex-only). They were very quickly obsolete when almost all vendors switched to using switches exclusively and I dont think you could actually buy a hub anymore.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359495/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359544"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 30, 2009 2:46 UTC (Fri)
                               by <b>smoogen</b> (subscriber, #97)
                              [<a href="/Articles/359544/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I was told that if you sent a flood of ICMP traffic to certain 1GB switches they would 'fail' into a hub-like mode. <br>
<p>
The collision detection is still needed because they are still possible on a pure switch.. usually they happen with certain traffic patterns or if the network gets saturated. However they occur a LOT less than they did on hubs.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359544/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor359496"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">How creaky is Ethernet the spec?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 20:03 UTC (Thu)
                               by <b>bronson</b> (subscriber, #4806)
                              [<a href="/Articles/359496/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From my perspective, it was the star topology that brought Ethernet's big win.<br>
<p>
I remember trying to wrestle thick cable around corners without causing network issues and fighting endlessly with vampire taps (and SCSI terminators, but that's unrelated).  Token ring was definitely better than this mess.<br>
<p>
Then thin cable brought Ethernet competitive with token ring.  Now the biggest problem with both systems was employees rolling their chairs over the cable under their desks and bringing the entire network down.  And the occasional NIC that loses its mind and goes into a retransmit flood.  And sometimes terminators.  But, overall, it was OK.<br>
<p>
Then 10baseT and cheap switches came along and allowed Ethernet to use an inexpensive star topology.  This is what absoulutely murdered token ring.  Twisted-pair star setups were so much faster to set up and more reliable to keep working that nobody looked back.<br>
<p>
Funny how admins at the time would look at the neck-thick bundle of wires slamming into 80-port switches in the server closet and think, "Good lord, I'd never want to try to keep all that mess working."  It didn't take them much time adminning a twisted pair ethernet network for them to grow to love it though.<br>
<p>
Fun times.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359496/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor359462"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 17:25 UTC (Thu)
                               by <b>sgros</b> (guest, #36440)
                              [<a href="/Articles/359462/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Herbert says that jumbograms are problem on the Internet, but this is not a rock solid argument. How many access points allow for 10Gb/s, and more importantly, how many of them communicate at full speed? Namely, if you have 10Gb/s link between two points that goes over the Internet, then I think that you also have control over what is between those two points.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359462/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359497"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 20:20 UTC (Thu)
                               by <b>iabervon</b> (subscriber, #722)
                              [<a href="/Articles/359497/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The issue is that you'd actually like to receive 1Mb/s each from 10000 different points across the public internet all past the same router. You want to speak at 10Gb/s, and you're limited to the MTUs on each of the remote networks.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359497/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359508"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 21:22 UTC (Thu)
                               by <b>bangert</b> (subscriber, #28342)
                              [<a href="/Articles/359508/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
At OpenSourceDays last weekend here in Copenhagen Jacob Brouer presented his work on <br>
getting 10 Gbit routing going on Linux. His point of departure was a full-duplex 2 port solution.<br>
<p>
The result was that routing 40Gbit/s with a packetsize of 1500 is doable. Reducing the average <br>
packet size to the internet average the performance would drop off obviously, but things are <br>
getting quite close.<br>
<p>
As far as i understood, his company is looking at this ~6000 dollar setup as a alternative to 60000 <br>
dollar equipment.<br>
<p>
<a href="http://opensourcedays.org/CommunityDay2009/print/133">http://opensourcedays.org/CommunityDay2009/print/133</a><br>
Slides and video forthcoming.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359508/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359520"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 29, 2009 22:53 UTC (Thu)
                               by <b>Shewmaker</b> (guest, #1126)
                              [<a href="/Articles/359520/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <a href="http://vger.kernel.org/netconf2009_slides/LinuxCon2009_JesperDangaardBrouer_final.pdf">Jesper's slides</a> from LinuxCon2009.
      
          <div class="CommentReplyButton">
            <form action="/Articles/359520/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor359613"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 30, 2009 16:55 UTC (Fri)
                               by <b>sgros</b> (guest, #36440)
                              [<a href="/Articles/359613/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
In that case we are speaking of some content provider that has &gt;=10Gb/s link(s) (heavy loaded) that uses linux as:<br>
<p>
a) router, or<br>
b) application server (or something similar)<br>
<p>
in case of a), no hardware offloading is allowed. In case of b) I somehow doubt that you'll put extreme load on your server(s), i.e. you'll do load balancing.<br>
<p>
Of course, there could be some other use scenario I'm not aware of, but the point is that I'm still not convinced. BTW, it doesn't mean I'm against achieving something like this that Herbert is trying to do. If nothing else, it would be definitely cool to say linux can do it, others can't. :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359613/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor359780"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">UDP merging vs VoIP</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 31, 2009 18:31 UTC (Sat)
                               by <b>NAR</b> (subscriber, #1313)
                              [<a href="/Articles/359780/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Is this a good idea to wait for UDP packets to be merged if these packets contain voice data? It could create noticable jitter...<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359780/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359793"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">UDP merging vs VoIP</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 1, 2009 1:02 UTC (Sun)
                               by <b>intgr</b> (subscriber, #39733)
                              [<a href="/Articles/359793/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; to wait for UDP packets to be merged</font><br>
<p>
The article partly answers your question:<br>
<font class="QuotedText">&gt; How does the kernel decide how long to wait for incoming packets before</font><br>
<font class="QuotedText">&gt; merging them? It turns out that there is no real need for any special</font><br>
<font class="QuotedText">&gt; waiting code: the NAPI API already has the driver polling for new packets</font><br>
<font class="QuotedText">&gt; occasionally and processing them in batches. GRO can simply be performed</font><br>
<font class="QuotedText">&gt; at NAPI poll time.</font><br>
<p>
In a low-throughput setting, the kernel uses the normal interrupt-driven networking mode. Individual packets are processed as quickly as they come in over the wire.<br>
<p>
Only when the CPU is too pegged to keep up with the interrupt load, does NAPI revert to the polling mode. At this point, without NAPI, the CPU would already be thrashing -- spending time to receive packets that it has no time to process. GRO merely increases the throughput that can be handled in polling mode.<br>
<p>
How many packets are grabbed in each polling cycle can be changed with /proc/sys/net/core/netdev_budget (the default 300 is quite modest IMHO)<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359793/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359808"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">UDP merging vs VoIP</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 1, 2009 11:00 UTC (Sun)
                               by <b>NAR</b> (subscriber, #1313)
                              [<a href="/Articles/359808/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I think VoIP doesn't mind if a couple of packets is lost (that's why it uses UDP in the first place), the jitter is more problematic. Buffering doesn't really help, because in a phone conversation one might want to interrupt what the other says and seconds of buffering kind of breaks this.<br>
<p>
I'm not quite sure where this merging could be done. Only at the endpoints or in routers between?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359808/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor359830"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">UDP merging vs VoIP</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 1, 2009 16:58 UTC (Sun)
                               by <b>intgr</b> (subscriber, #39733)
                              [<a href="/Articles/359830/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From the article:<br>
<font class="QuotedText">&gt; If a system is serving as a router, it really should not be changing the</font><br>
<font class="QuotedText">&gt; headers on packets as they pass through.</font><br>
<p>
Also, VOIP uses UDP simply because TCP would add additional delay when it attempts to retransmit packets that already missed their deadline.<br>
<p>
But VOIP is still affected by packet loss. A UDP packet that missed its deadline is never way any worse than a UDP packet that got dropped.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359830/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor359792"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 1, 2009 0:22 UTC (Sun)
                               by <b>intgr</b> (subscriber, #39733)
                              [<a href="/Articles/359792/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From the article:<br>
<font class="QuotedText">&gt; sending of data, it's sufficient to make 10GB work at full speed</font><br>
<p>
Correction, I think that should be "10Gb". Even current memory controllers have a hard time coping with 10 Gbytes/s. :)<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/359792/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor360515"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 5, 2009 8:13 UTC (Thu)
                               by <b>RamiRosen</b> (guest, #37330)
                              [<a href="/Articles/360515/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;With packet size capped at 1500 bytes, a 10G network link running at full &gt;speed will be transferring over 800,000 packets per second. </font><br>
<p>
Is it accurate ? <br>
<p>
a simple calculation :<br>
1000000000/1500=<br>
666,666<br>
<p>
<p>
Rami Rosen<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/360515/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor360848"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2009 19:33 UTC (Fri)
                               by <b>ollibolli</b> (guest, #61845)
                              [<a href="/Articles/360848/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
One 1500 byte packet: 64 bits preamble + 14*8 bits Ethernet header + 1500*8 bits payload + 32 bits FCS + 96 bits interframe gap = 12304 bits<br>
10Gbit/s = 10000000000bits/s<br>
10000000000bits/s / 12304bits/pkt = 812743 pkt/s<br>
Data bits might not translate one-to-one into bits on the media so the actual packet rate might be slightly smaller.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/360848/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor853681"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">JLS2009: Generic receive offload</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 21, 2021 12:58 UTC (Wed)
                               by <b>samplelin</b> (guest, #151768)
                              [<a href="/Articles/853681/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Q: If GRO will create the packet latency in RX path? How to sure that the latency is not impacted? Depended on user?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/853681/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2009, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
