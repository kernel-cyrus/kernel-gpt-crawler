        <!DOCTYPE html>
        <html lang="en">
        <head><title>Journal-guided RAID resync [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/363490/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/362781/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/363490/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Journal-guided RAID resync</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Did you know...?</b>
<p>
LWN.net is a subscriber-supported publication; we rely on subscribers
       to keep the entire operation going.  Please help out by <a
       href="/Promo/nst-nag4/subscribe">buying a subscription</a> and keeping LWN on the
       net.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>November 24, 2009</br>
           </div>
The RAID4, 5, and 6 storage technologies are designed to protect against
the failure of a single 
drive.  Blocks of data are spread out across the array and, for each
stripe, there is a parity block stored on one of the drives.  Should one
drive fail, the lost data can be recovered through the use of the remaining
drives and the parity information.  This mechanism copes less well with
system crashes and power failures, though, forcing software RAID administrators to
choose between speed and reliability.  A new mechanism called
journal-guided resyncronization may make 
life easier, but only if it actually gets into the kernel.
<p>
The problem is that data and parity blocks must be updated in an atomic
manner; if the two go out of sync, then the RAID array is no longer in a
position to recover lost data.  Indeed, it could return corrupted data.
Expensive hardware RAID solutions use battery backup to ensure that updates
are not interrupted partway through, but software RAID solutions often do
not have that option.  So if the system crashes - or the power fails -
in the middle of an update to a RAID volume, that volume could end up being
corrupted.  Computer users, being a short-sighted kind of people in
general, tend to regard this as a Bad Thing.
<p>
There are a couple of possible ways of mitigating this risk.  One is to
perform a full rescan of the RAID volume after a crash, fixing up any
partially-updated stripes.  The problem here is that (1)&nbsp;the correct
fix for an inconsistent stripe may not always be clear, and (2)&nbsp;this
process can take a <i>long</i> time.  Long enough to cause users to think
nostalgically about the days of fast, reliable floppy-disk storage.
<p>
An alternative approach is to introduce a type of journaling to the RAID layer.  The
RAID implementation can set aside some storage where it writes stripes
(perhaps not the data, but, perhaps, just the numbers of the affected stripes) 
prior
to changing the real array.  This approach works, and it can recover a
crashed RAID array without a full rescan, but there is a cost here too:
that journaling can slow down the operation of the array significantly.
Writes to the journal must be synchronous or it cannot be counted on to do
its job, so write operations become far slower than they were before.
Given that, it's not surprising that a lot of RAID administrators turn off
RAID-level journaling and spend a lot of time hoping that nothing goes
wrong.
<p>
A few years ago, Timothy E. Denehy, Andrea C. Arpaci-Dusseau, and Remzi
H. Arpaci-Dusseau published <a
href="http://www.usenix.org/events/fast05/tech/full_papers/denehy/denehy_html/main.html">a
paper</a> describing a better way, which they called "journal-guided
resynchronization."  Contemporary filesystems tend to do 
journaling of their own; why not use the filesystem journal to track
changes to the RAID array as well?  Running one journal can only be cheaper
than running two - especially when one considers that the RAID journal must
track, among other things, changes to the filesystem journal.  The only
problem is that the RAID and filesystem layers communicate through the
relatively narrow block-layer API; using filesystem journaling to track
RAID-level information has the potential to mix the layers considerably.
<p>
Jody McIntyre's <a
href="http://lwn.net/Articles/363114/">journal-guided resync
implementation</a> adds a new "declared" 
mode to the ext3 filesystem.  As the journal is being written, a new
"declare block" is added describing exactly which blocks are to be written
to the storage device.  Those blocks are then written with a new BIO flag
stating that the filesystem has taken responsibility for resynchronizing
the stripe should something go wrong; that lets the storage layer forget
about that particular problem.  Should the system crash, the filesystem
will find those declare blocks in the journal; it can then issue a (new)
<tt>BIO_SYNCRAID</tt> operation asking the storage subsystem to
resynchronize the specific stripes containing the listed blocks.
<p>
The result should be the best of both worlds.  The cost of adding one more
block to the filesystem journal is far less than doing that journaling at
the RAID layer; Jody claims a 3-5% performance hit, as compared to 30% with
the MD write-intent bitmap mechanism.  But resynchronization after a crash
should be quite fast, since it need only look at the parts of the array
which were under active modification at the time.  The only problem is that
it requires the addition of specific support at the filesystem layer, so
each filesystem must be modified separately.  How this technique could be
used in a filesystem which works without journaling (Btrfs comes to mind)
would also have to be worked out.
<p>
There's one other little problem as well.  This work was done at Sun as a
way of improving performance with the Lustre filesystem.  But Jody notes:
<p>
<div class="BigQuote">
	Unfortunately, we have determined that these patches are NOT useful
	to Lustre.  Therefore I will not be doing any more work on them.  I
	am sending them now in case they are useful as a starting point for
	someone else's work.
</div>
<p>
So this patch series has been abandoned for now.  It seems like this
functionality should be useful to software RAID users, so, hopefully,
somebody will pick them up and carry them forward.  In the absence of a new
developer, software RAID administrators will continue to face an unhappy
choice well into the future.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer-RAID">Block layer/RAID</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#RAID">RAID</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/363490/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor363775"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 25, 2009 21:56 UTC (Wed)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/363775/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
<i>(1) the correct fix for an inconsistent stripe may not always be clear</i>
</blockquote>

For an inconsistency caused by a crash interrupting a write, there is no correctness problem.  If there are two credible options (such as a RAID1 with two devices where a block differs) then either option is equally correct.  One version might be "newer" than the other, but that doesn't mean it is more "correct".

"correctness" can be an issue when inconsistency is caused by dodgy hardware - e.g. a stuck-bit in a device buffer.  But that is a completely different problem.
      
          <div class="CommentReplyButton">
            <form action="/Articles/363775/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor363925"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 12:17 UTC (Thu)
                               by <b>nye</b> (subscriber, #51576)
                              [<a href="/Articles/363925/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Surely this is not the case in the (fairly common) case of out-of-order writes? This sounds like exactly the sort of situation that caused the ext4 kerfuffle earlier this year.<br>
<p>
On a different note, can anyone explain to me why running a check or (god forbid) rebuild on a RAID array takes like a dozen times longer than reading/writing the contents of all the disks?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/363925/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364015"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 21:00 UTC (Thu)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/364015/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <blockquote><i>
Surely this is not the case in the (fairly common) case of out-of-order writes? This sounds like exactly the sort of situation that caused the ext4 kerfuffle earlier this year.
</i></blockquote>

I don't see how out-of-order writes complicate the question ... maybe I'm missing something.  A key question that must be understood is "what data is valid", and in the case of multiple parallel writes pending when a crash happens, there are lots of combinations that are all equally valid.

<blockquote><i>
On a different note, can anyone explain to me why running a check or (god forbid) rebuild on a RAID array takes like a dozen times longer than reading/writing the contents of all the disks?
</i></blockquote>

This is not my experience.  If you post specifics to <tt>linux-raid@vger.kernel.org</tt> you will probably get a useful reply.
<p>
The only explanation for what you describe that immediately occurs to me is that fact that the check/repair code deliberately slows down when any other IO is active so as not to inconvenience that IO, but you probably know that already.

      
          <div class="CommentReplyButton">
            <form action="/Articles/364015/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364703"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 2, 2009 14:03 UTC (Wed)
                               by <b>nye</b> (subscriber, #51576)
                              [<a href="/Articles/364703/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;I don't see how out-of-order writes complicate the question ... maybe I'm missing something. A key question that must be understood is "what data is valid", and in the case of multiple parallel writes pending when a crash happens, there are lots of combinations that are all equally valid.</font><br>
<p>
I think I may have misunderstood the nature of the problem in question, but IIRC I was thinking about when one version is 'correct', in the sense that it is an old but consistent version of the data, but the other version has been partially updated, and ended up in an invalid state. This might happen, for example, if a metadata change is written before the corresponding data change.<br>
I suppose from the point of the RAID though, that data is valid, in that it's an accurate representation of what the filesystem asked to be on the disk at the given moment, so I see your point.<br>
<p>
<font class="QuotedText">&gt;This is not my experience. If you post specifics to linux-raid@vger.kernel.org you will probably get a useful reply.</font><br>
<p>
Well, the specific incident that prompted the question involved a hardware RAID controller (my experiences with Linux software RAID have, thus far, been unproblematic). Knowing next to nothing about how RAID is implemented I wondered if the procedure is expected to involve, say, an O(n^2) number of read or writes, or something else that would lead to this slowness being generally expected. Obviously not :).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364703/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor363972"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 16:54 UTC (Thu)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/363972/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hang on. If you crash after a RAID-5 stripe has been written to one of the <br>
disks but not the other, you can tell the stripe is inconsistent, but not <br>
what the valid contents are (at least, not programmatically.)<br>
<p>
You can tell with RAID-6, but ISTR you mentioning a few months ago that md <br>
can't actually do this yet.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/363972/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364013"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 20:54 UTC (Thu)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/364013/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
<i>Hang on. If you crash after a RAID-5 stripe has been written to one of the
disks but not the other, you can tell the stripe is inconsistent, but not
what the valid contents are (at least, not programmatically.)</i>
</blockquote>

Between the moment when a write is requested, and the moment when the success of that write is reported - and possibly further until a barrier request has been acknowledged - both the 'old' data and the 'new' data are valid.  The correct thing to do in this case is to treat all of the data blocks as "valid" (because they are) and update the parity block to ensure it is consistent.
<p>
What do you think "Valid" means in this context?

      
          <div class="CommentReplyButton">
            <form action="/Articles/364013/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364021"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 21:34 UTC (Thu)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/364021/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
My worry is that between the time when the new data is written, and the <br>
time when the parity block is updated (or vice versa if the parity write <br>
gets to the disk surface first), if you have a crash, bingo, you have <br>
instant corruption of that stripe. There isn't any way to make those two <br>
writes happen in sync, after all.<br>
<p>
(I *know* you know this, so am quite mystified that you're apparently <br>
claiming that it isn't a problem. I don't see how a workaround is even <br>
*possible*: it's why battery-backing of RAID-5 arrays is done at all...)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364021/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364051"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 27, 2009 1:14 UTC (Fri)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/364051/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You only have "instant corruption" if the array becomes degraded before the parity gets corrected.  For this reason mdadm will not assemble an array which is both dirty and degraded. I thought I had mentioned this in my original comment, but apparently not.  Maybe this is the case implied in the original article, though I don't think the text really matches reality:  Either there is a correct fix that is trivial, or no fix is possible.<br>
<p>
(The only two ways to avoid corruption when a crash happens on a degraded array are 1/ to journal updates at the raid level, or 2/ use a copy-on-write filesystem that knows about the stripe size and only ever writes into a stripe that does not contain any important information.).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364051/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364082"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 27, 2009 6:56 UTC (Fri)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/364082/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Aaah, I see (actually whenever you mention this I get it for a few minutes <br>
and then it blurs out of memory again). Yes, that makes sense: if you <br>
don't lose a disk, you have two intact stripes and one stripe containing <br>
not-yet-written garbage: whether that's the parity stripe or not is <br>
immaterial.<br>
<p>
So the thing to be worried about here is that RAID-5 only really protects <br>
you from a single disk failure if your array is not being written to (or <br>
is battery-backed).<br>
<p>
And I suspect this is the case the article was discussing.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364082/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor363866"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 26, 2009 8:12 UTC (Thu)
                               by <b>stefanor</b> (subscriber, #32895)
                              [<a href="/Articles/363866/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; The RAID4, 5, and 6 storage technologies are designed to protect against the failure of a single drive.</font><br>
<p>
Misleading, RAID-6 protects against double drive failures.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/363866/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor697384"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Journal-guided RAID resync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 16, 2016 22:33 UTC (Tue)
                               by <b>immibis</b> (guest, #105511)
                              [<a href="/Articles/697384/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It _also_ protects against single-drive failures.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/697384/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor364221"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 29, 2009 7:03 UTC (Sun)
                               by <b>qu1j0t3</b> (guest, #25786)
                              [<a href="/Articles/364221/">Link</a>] (10 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
ZFS did an elegant end run around this issue around five years ago.<br>
<p>
At least this discussion gives some publicity to the nasty shortcomings of RAID-*.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364221/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364423"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 1, 2009 3:08 UTC (Tue)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/364423/">Link</a>] (9 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I thought that ZFS included software raid, how does it deal with the problem of figuring out what is supposed to be in place when some drives in a stripe are updated and others are not (after a power failure)?<br>
<p>
note that checksums don't help here as all drives have valid checksums.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/364423/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor364599"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 1, 2009 22:48 UTC (Tue)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/364599/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <p>My basic, potentially a bit wrong, understanding of ZFS is that:
</p><p>
a) It uses COW, and blocks are arranged in a <a href="http://en.wikipedia.org/wiki/Hash_tree">Merkle tree</a>. An updated block isn't properly part of the ZFS until its referencing meta-data block has been updated with its hash. This means that ultimately there is only block you absolutely must update atomically - the root "uber-block" (and even there, you can arrange to have redundant uber-blocks and you can arrange that old uber-block(s) remain valid until you're confident the uber-block is properly written out).
</p><p>
Even if hardware lies massively and re-arranges the writes and manages to lose/corrupt the child data block write, while still writing the parent, you'll at least be able to detect the inconsistency, generally.
</p><p>
b) Because ZFS integrates RAID and FS, it can do variable-sized stripes. So if you want to write data, there's no requirement to read-in other data in order to calculate parity. You only have to write the data.
</p><p>
Combine the 2 and you have avoided the RAID-5 "write hole" AND gained performance (presuming your data hash function is quite cheap relative to I/O - which tends to be true on any half-recent hardware). See <a href="http://blogs.sun.com/bonwick/entry/raid_z">Jeff Bonwick's RAID-Z blog entry</a>.
</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/364599/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365114"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 4, 2009 1:26 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/365114/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
normal raid 5 implementations do not require that you read all the blocks to update one, they only require thatyou know the old and new contents of the block that you are modifying and the parity block. so that isn't a ZFS advantage<br>
<p>
I wasn't aware that ZFS could mix raid types/configurations within a single filesystem. that seems very strange to me, are you sure that it can do this? I thought that when you created the ZFS filesystem you told it the raid type and configuration to use.<br>
<p>
the basic problem is that wuthout doing a sync, the OS has no way of knowing when the data written to different drives actually hits the media and is safe (and this can happen in different orders for different writes, even to the same spot), thus the need to do 'raid journaling' or similar. <br>
<p>
I don't believe that ZFS does COW, doing that would fragment the data very rapidly, and that is death to performance for drives that need to seek.<br>
<p>
I don't believe that your two points are valid in themselves, and even if they were I don't see how they combine to solve this problem.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365114/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365223"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 4, 2009 19:10 UTC (Fri)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/365223/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I've only had a quick glance, so I could be completely wrong and just looking for anything consistent with my pre-existing notions, but Linux RAID-5 seems to schedule old blocks to be read-in when a block is modified, in order to update the parity. E.g. look at handle_stripe, handle_strip_dirtying and places where the R5_Wantread bit is set. ICBW though.<br>
<p>
There are quite a few reasonably authorative sources that say ZFS is COW and transactional. So there's no reason to think it isn't, though I havn't read the code.<br>
<p>
I didn't say ZFS mixed RAID types in a single OS. I said it used variable sized stripes. ZFS always writes full-stripes, so it never has to read-back data to finish the write. Combined with the Merkle tree arrangement, it means ordinary writes do not leave the RAID or FS inconsistent, even for short windows of time.<br>
<p>
Block layer RAID-5 and separate FS have never been able to achieve that, hence the talk now of introducing more interfaces to make them more closely coupled (and doesnt btrfs have its own device management layer, ala ZFS?).<br>
<p>
There's lots of stuff on the net about this.. ;)<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365223/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365340"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 4, 2009 22:56 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/365340/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
so you mean that sometimes it uses 16k stripes across the disks, and in the same filesystem will use 64K stripes across the disks? (picking a couple sizes out of thin air here)<br>
<p>
how can it decide when to use what, and what data needs to be stored on what stripe size?<br>
<p>
given all the people who are claiming that ZFS is the best performing filesystem ever, I don't believe that it is COW and transactional across multiple drives. on rotating media where seeks are expensive these features and performance just don't coexist.<br>
<p>
the problem is that when ZFS wants to update a raid 5 strip it has two writes to do, to two different drives. unless it does a full sync and flush of the drive write buffer for each write (which will kill performance), it has no way of knowing which block on disk gets updated first.<br>
<p>
if the system crashes after one block is updated and before the second block is updated, how can it know which one is correct?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365340/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365343"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 4, 2009 23:20 UTC (Fri)
                               by <b>foom</b> (subscriber, #14868)
                              [<a href="/Articles/365343/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; I don't believe that it is COW and transactional across multiple drives. </font><br>
<p>
You could, perhaps, go read about it instead of saying what it can't possibly be.<br>
<p>
Here's one place to start:<br>
<p>
<a href="http://en.wikipedia.org/wiki/ZFS#Copy-on-write_transactional_model">http://en.wikipedia.org/wiki/ZFS#Copy-on-write_transactio...</a><br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365343/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365357"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 5, 2009 1:29 UTC (Sat)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/365357/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
note that this says it is tranactional when syncronous writes are desired. that is not the normal operation<br>
<p>
second, it days that it uses an intent bitmap to implement this protection. this is one of the methods described in the article to address this problem<br>
<p>
so ZFS didn't sidestep this issue, it implemented the same method that MD offers (which is usually disabled for performance reasons), write intent bitmaps.<br>
<p>
I stand corrected on the COW issue, but by doing COW you have two major problems<br>
<p>
1. every write to a file ends up updating many more places on the filesystem (the file block gets moved and re-written, then the metadata that points to that file gets moved and re-written, then the metadata that points to that metadata..... until the root directory gets re-written) this is one of the failues of btree filesystems that made them unusable on rotating media.<br>
<p>
wikipedia states that ZFS works around this by buffering the writes to consolodate them, but that is the same strategy that ext3 uses, and the need to then write all that buffered data in the face of a sync call is why fsync is so horrible on ext3<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365357/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365383"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 5, 2009 12:22 UTC (Sat)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/365383/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <p>Note that the <a href="http://src.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/fs/zfs/zil.c">ZFS Intent Log</a> doesn't affect the consistency of the filesystem. The fs is consistent without the intent log, thanks to the COW, Merkle tree arrangement. So its not solving any consistency problems, i.e.:
</p><p>
<ul>
<li>ZFS is transactional, with or without the ZIL
<li>ZFS is always consistent on disk, with or without the ZIL
</ul>
</p><p>
The ZIL is there to solve the problem of supporting performant synchronous writes. ZFS would still support consistent, synchronous writes without the ZIL, just they'd be very slow.
</p><p>
I think it'd be useful if you looked at this <a href="http://hub.opensolaris.org/bin/download/Community+Group+zfs/docs/zfslast.pdf">ZFS presentation</a>, which is where I'm getting most of my info from. The <a href="http://hub.opensolaris.org/bin/view/Community+Group+zfs/">OSOL ZFS community</a> also has a <a href="http://hub.opensolaris.org/bin/view/Community+Group+zfs/source">source overview</a>.
</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/365383/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor365384"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 5, 2009 12:31 UTC (Sat)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/365384/">Link</a>] 
      </p>
      
      </div>
      </summary>
      A good <a href="http://blogs.sun.com/realneel/entry/the_zfs_intent_log">blog entry on the ZIL</a>.
      
          <div class="CommentReplyButton">
            <form action="/Articles/365384/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor365381"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Delayed reaction</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 5, 2009 10:20 UTC (Sat)
                               by <b>paulj</b> (subscriber, #341)
                              [<a href="/Articles/365381/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
My understanding is that COW is fundamental to how ZFS works (refer back to my previous comment on Merkle trees and how data and meta-data reference each other). I havn't read the code to verify this, but it's what all the papers and presentations on ZFS say.<br>
<p>
I think that also already answers your question as to how ZFS can know which one is correct. I'm not sure there's any value in, again, explaining how ZFS consistency (in theory) comes down to the update of a single "uber-block".<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/365381/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2009, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
