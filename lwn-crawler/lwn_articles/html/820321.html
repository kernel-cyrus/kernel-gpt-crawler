        <!DOCTYPE html>
        <html lang="en">
        <head><title>Completing and merging core scheduling [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/820321/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/819797/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/820321/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Completing and merging core scheduling</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ready to give LWN a try?</b>
<p>
With a subscription to LWN, you can stay current with what is happening in the Linux and free-software community and take advantage of subscriber-only site features.  We are pleased to offer you <b><a href="https://lwn.net/Promo/nst-trial/claim">a free trial subscription</a></b>, no credit card required, so that you can see for yourself.  Please, join us!
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 13, 2020</br>
           <hr>
<a href="/Articles/820337/">OSPM</a>
</div>
<a href="/Articles/780703/">Core scheduling</a> is a proposed modification
to the kernel's CPU scheduler that allows system administrators to control
which processes can be running simultaneously on the same processor core.
It was originally proposed as a security mechanism, but <a
href="/Articles/799454/">other use cases</a> have shown up over time as
well.  At the 2020 <a href="http://retis.sssup.it/ospm-summit/">Power
Management and Scheduling in the Linux Kernel summit</a> (OSPM), a group of
some 50 developers gathered online to discuss the current 
state of the core-scheduling patches and what is needed to get them into
the mainline kernel.
<p>
<h4>Status update</h4>
<p>
Vineeth Pillai started off by noting that, while mitigations have been
developed for the known Spectre vulnerabilities, they do not provide
complete protection on systems where simultaneous multi-threading (SMT or
"hyperthreading") is in use.  SMT creates the illusion of multiple CPUs
running on shared hardware, increasing performance.  The sharing of the
underlying processor, however, provides numerous opportunities for
speculative-execution vulnerabilities and the creation of covert channels.
The only way to truly protect a system against these vulnerabilities is to
disable SMT, which comes at a high performance cost for some workloads.
<p>
Many Spectre vulnerabilities can be mitigated through selective cache
flushing, Pillai said, but that is of limited utility when SMT is in use.
Performing a cache flush will remove any information placed there before
the flush, making it inaccessible afterward.  To be effective, though,
tasks that do not trust each other must not share the core between flushes.
Without SMT, the 
kernel can perform a flush whenever it switches tasks, ensuring that this
never happens.  On an SMT core, though, each CPU schedules independently,
so this kind of flushing discipline cannot be maintained.

<p>
The way to avoid this problem is to keep tasks that don't trust each
other from running on the same core â€” which means disabling SMT on current
kernels.  The alternative is core scheduling, where tasks in a given trust
domain can be explicitly grouped together.  The scheduler maintains

<a href="/Articles/820334/"><img
src="https://static.lwn.net/images/conf/2020/ospm/coresched-sm.png" alt="[Vineeth Pillai, Dario
Faggioli, and Tim Chen]"
title="Vineeth Pillai, Dario Faggioli, Tim Chen" width=200 height=178
hspace=3 vspace=3 border=0 align="right"></a>

core-wide knowledge of these trust domains and ensures that only tasks in
the same domain run simultaneously on any given core.  If there is no
trusted companion for the highest-priority task, sibling CPUs can be forced
idle while that task runs rather than run an untrusted task there.
<p>
One open area, Pillai said, was in the area of load balancing, which
doesn't currently work well with core scheduling.  This could
perhaps be improved by selecting a single run queue to hold the shared
information needed for core scheduling.  When a scheduling event happens,
the highest-priority task would be chosen as usual.  Then any sibling
processors can be populated with matching tasks from across the system,
should any exist.
<p>
Core scheduling currently uses CPU control groups for grouping; there is a
<tt>cpu.tag</tt> field that can be set to assign a "cookie" identifying the
scheduling group a task belongs to.  This was done for a quick and easy
implementation, he said, and need not be how things will work in the end.
There is a <a href="/Articles/184495/">red-black tree</a> in each run
queue, ordered by cookie value, that is used to select tasks for sibling
processors.
<p>
The patch series is up to <a href="/Articles/813808/">version 5</a>, which
includes some load-balancing improvements.  Earlier versions did not
understand load balancing at all, so if a task was migrated to a CPU running
(incompatible) tagged tasks, it could end up being starved for CPU time. 
A sixth revision is coming soon, he said.
<p>
One challenge that has to be dealt with is comparing the priority of
tasks across siblings.  Within a run queue, a task's <tt>vruntime</tt> value
is used to determine whether it should run next.  This value is a sort of
virtual run time, indicating how much CPU time the task has received
relative to others
(though it is scaled by the process priority and adjusted in various other
ways), but this value is specific 
to each run queue.  A <tt>vruntime</tt> in one run queue cannot be directly
compared to a <tt>vruntime</tt> in another queue.
<p>
One possible solution is to normalize these values.  Each queue maintains a
<tt>min_vruntime</tt> value, which is the lowest <tt>vruntime</tt> of any
task in that queue.  If a specific task's <tt>vruntime</tt> is normalized
by subtracting the local <tt>min_vruntime</tt>, it can then be compared to
a value in another run queue by adding that queue's <tt>min_vruntime</tt>.
A solution based on this turned out to have starvation issues, though,
leading to the creation of a core-wide <tt>vruntime</tt> instead;
unfortunately, there are still starvation problems with this
implementation, and discussions are ongoing.
<p>
Work for the sixth revision includes some attention to the process
selection code, which currently only picks the highest-priority task to
run.  That can cause starvation issues as well.  There are also problems
with tasks that go into the kernel (to handle an interrupt, for example)
then end up being co-scheduled with an 
untrusted task; this will be expensive to fix.  Some thought is going into
the API, and perhaps switching to <tt>prctl()</tt> to set a task's
grouping.
<p>
Pillai concluded by asking whether this work should be merged into the
mainline kernel.  There are a number of arguments for that.  Core
scheduling shows better performance than just disabling SMT for a number of
production use cases.  It is controlled by a configuration option and is
off by default even when configured in; there is no impact on performance
when core scheduling is disabled.  On the other hand, it's still only a
partial mitigation for the problem, it has some fairness issues, there is
code cleanup needed, and it still lacks a widely accepted API.
<p>
<h4>IRQ leak mitigation and accounting</h4>
<p>
Joel Fernandes then took over to talk about one of the remaining Spectre
mitigation 
issues: interrupt leaks.  Google (his employer) wants to use core scheduling with
Chrome&nbsp;OS, since there are some tangible benefits.  In tests with the
camera running on a Chromebook, core scheduling reduced key-press latency
considerably while improving the camera's frame rate by 3%.
But developers there are concerned that interrupts (of both
the hard and soft variety) can cause untrusted tasks to run simultaneously
with the kernel's interrupt handlers, exposing the kernel to data leaks.  Core
scheduling currently only works at the task level, with no control over
interrupt handling, so it cannot address this problem.
<p>
The proposed solution is the just-posted <a
href="/ml/linux-kernel/20200510234652.249917-1-joel@joelfernandes.org/">IRQ
leak mitigation patch</a>.  It works by tracking whenever a CPU within a
core calls <tt>irq_enter()</tt> â€” when it starts to handle an interrupt, in
other words.  The first such call within a core will, if another CPU is
running an untrusted task, cause an inter-processor interrupt forcing the
other CPU to go idle.  The scheduler itself also must be modified so
that, when it switches from one task to another, it checks to see if
another CPU is handling interrupts at the moment.  If so it will wait until
the coast is clear and caches have been flushed.
<p>
There were some questions about how Chrome OS uses core scheduling.  It
seems that all "system tasks" are allowed to run together, outside of any
core-scheduling group.  Browser-based tasks (which are nearly all user
tasks in Chrome OS) are each put into their own group, and thus run
isolated.  In other words, the <i>untrusted</i> tasks are specially marked
by the system.  Peter Zijlstra remarked that this means tasks default to
the trusted state, which seems insecure; he suggested that the default be
untrusted instead.
<p>
Juri Lelli asked about other scheduling classes; what happens if there is a
realtime FIFO task in the system?  Zijlstra answered that the usual
ordering will be followed; the FIFO task, since it has the highest
priority, will be picked first.  Non-realtime tasks in the same group can
then run on sibling processors, if they exist, though that would be a bit
unusual since such tasks could interfere with the realtime task.
<p>
Dario Faggioli talked for a bit about SUSE's use case for core scheduling:
making sure that accounting for virtualized guests is accurate.  A typical
host system is running a lot of tasks, many of which represent the virtual
CPUs (vCPUs) of different virtual machines.  The scheduler can mix up those
vCPUs in any way it likes, regardless of how they correspond to the virtual
machines they emulate.
<p>
Since tasks running on sibling CPUs are contending for the underlying
hardware, they can affect each other's performance.  Two vCPUs may appear
to spend the same amount of time running on sibling CPUs, but one of those
two may have actually consumed much more run time than the other.  The
result is unfair accounting of CPU time.
Core scheduling can help by ensuring that vCPUs from the same virtual
machine run on the same core, so that they only contend against each
other; that makes the accounting more fair.
<p>
Things can be improved
further by defining the virtual machines so that some of their vCPUs are
set up as SMT siblings, allowing the guest operating system to optimize its
scheduling accordingly.  That only works, though, if the virtual machine's
description bears some relationship to the physical reality.  Once again,
core scheduling can make that happen.
<p>
The security use case also applies to virtualization, Faggioli said.  Core
scheduling helps there, but does not yet appear to be a complete solution;
the interrupt situation discussed by Fernandes is one place where work
still needs to be done.  A full solution is likely to need technologies
like <a href="/Articles/803823/">address-space isolation</a> as well.
<p>
<h4>Performance</h4>
<p>
Tim Chen presented a number of performance benchmarks that emulate several
different use cases.  A set of virtualization tests showed the system
running at 96% of the performance of an unmodified kernel with core
scheduling enabled; the 4% performance hit hurts, but it's far better than
the 87% performance result measured for this workload with SMT turned off
entirely.  Some tests with the sysbench benchmark gave similar results for
core scheduling, but turning off SMT cut performance nearly in half.  The
all-important kernel-build benchmark showed almost no penalty with core
scheduling, while turning off SMT cost 8%.
<p>
Julien Desfossez presented results from a MySQL benchmark showing
performance dropping by 60% when core scheduling is used.  Painful, but
turning off SMT is far worse.  A CPU-intensive benchmark based on Linpack
showed core-scheduling performance that was slightly better than mainline,
while turning off SMT incurs a 90% performance hit.
<p>
Faggioli ran a set of tests on a 256-CPU AMD system, which does not need
core scheduling for Spectre mitigation at all.  He was interested in the
performance cost of having core scheduling built into the kernel but turned
off.  Kernbench ran at 98.6% of mainline with 128 jobs, up to 99.9% with
256&nbsp;jobs.  Various other tests yielded similar numbers.
<p>
There was some vague discussion of fairness testing â€” ensuring that all
tasks get equal amounts of CPU time.  The results were described as "messy"
and hard to interpret, but the overall impression is that core scheduling
yields less fair results overall.
<p>
<h4>Discussion</h4>
<p>
The final part of this three-hour session was given over to unstructured
discussion.  Zijlstra, who will probably make the decision over whether
core scheduling will be merged or not, started by saying that he would like
to see some better documentation.  In particular, he wants clear
information on where core scheduling helps and where it doesn't; in which
situations will it be helpful to turn core scheduling on?  There are some
things to work out, he said, including fairness and some problems with CPU
hotplug.  That can be sorted out, but the documentation is necessary to be
able to move forward with this work.
<p>
Dhaval Giani said that there are some cases that just don't work; not all
problems are amenable to solution in the scheduler.  Address-space
isolation may also be needed to have a complete solution to the (known)
Spectre problems.  Zijlstra repeated that documentation covering what does
work is needed.  Then users can decide whether they care enough to turn it
on for their specific situations.
<p>
Aaron Lu said that there are still problems around <tt>vruntime</tt>.  If
two tasks have the same tag but differing weights (priorities), the core
<tt>vruntime</tt> will become that of the lower-weight task since that task
will not run as much.  The difference between the two can become large.
Zijlstra answered that unbounded divergence of <tt>vruntime</tt> between
tasks is not a good thing, but renormalization is expensive.  It is also
unneeded.  Once a sibling CPU has been idled, there is only one run queue
that matters; that would be a good time to synchronize <tt>vruntime</tt>
values.  Lu expressed a desire to see a patch implementing that; Zijlstra
expressed a weary willingness to try to find time to create one.
<p>
Vincent Guittot raised the load-balancing issue; the results can be unfair,
he said.  If there are five tasks on a four-CPU system, one of those tasks
will end up running slower than the others.  He will be talking more about
this issue later in the conference.  In any case, Zijlstra said, the
<tt>vruntime</tt> issues need to be worked out before load balancing can be
resolved.
<p>
As the session wound down,
Giani tried to put together a list of the things that need to be worked
out; these included <tt>vruntime</tt>, CPU hotplug, and the fairness
issues.  Pillai added starvation of untagged tasks to the list.  Zijlstra
asked if that problem was for tasks with a <tt>cpu.tag</tt> value of zero,
which means no tag at all; Pillai said yes, but the special zero tag means
that the task does not go into the core-scheduling red-black tree.
Zijlstra suggested adding those tasks to the tree, which would remove the
exceptional case and make things work again.
<p>
Fernandes raised a related issue: that red-black tree contains task
<tt>vruntime</tt> values that are used when selecting compatible tasks,
but those values are not updated as the tasks 
run.  Pillai said that this is a problem, old <tt>vruntime</tt> values can
cause the scheduler to select the wrong task to run.  Zijlstra said
selecting a task for execution should remove it from this tree, as is done
for the run-queue red-black tree.  Doing this would slow things down, but
it may be worth it; the penalty should be relatively small for
virtualization workloads, since vCPUs are not rescheduled that often.
<p>
The session ended with Zijlstra saying that this work looks ready to
proceed, and that the remaining issues can be worked out on the mailing
list.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Scheduler-Core_scheduling">Scheduler/Core scheduling</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#OS-Directed_Power-Management_Summit-2020">OS-Directed Power-Management Summit/2020</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/820321/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor820429"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Completing and merging core scheduling</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2020 18:03 UTC (Wed)
                               by <b>jdesfossez</b> (subscriber, #123831)
                              [<a href="/Articles/820429/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; MySQL benchmark showing performance dropping by 60% when core scheduling is used</font><br>
Just a quick precision, this test case was performed with 96 1-vcpu "noise VMs" running at the same time on the same NUMA (a 3:1 overcommit ratio).<br>
So yes it's a significant drop, but it is on a very busy host running some kind of a worst case scenario with lots of 1-vcpu VMs. Core scheduling (or nosmt for that matter) doesn't show a significant performance degradation if there is enough headroom left on the system.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/820429/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor820447"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Completing and merging core scheduling</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2020 21:20 UTC (Wed)
                               by <b>dvdeug</b> (guest, #10998)
                              [<a href="/Articles/820447/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
"There were some questions about how Chrome OS uses core scheduling. It seems that all "system tasks" are allowed to run together, outside of any core-scheduling group. Browser-based tasks (which are nearly all user tasks in Chrome OS) are each put into their own group, and thus run isolated. In other words, the untrusted tasks are specially marked by the system. Peter Zijlstra remarked that this means tasks default to the trusted state, which seems insecure; he suggested that the default be untrusted instead."<br>
<p>
On a system like Chrome OS? On my single-user Linux system, pretty much any program could copy arbitrary data to random servers, including tax and bank data I've had stored on system. At that level, there's no point in panicking about core scheduling. Browser-based tasks are the only times I'm running programs that aren't implicitly trust.  That's the only time where worrying about side-channel attacks is important to me. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/820447/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor820532"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Completing and merging core scheduling</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2020 15:57 UTC (Thu)
                               by <b>jtaylor</b> (subscriber, #91739)
                              [<a href="/Articles/820532/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; A CPU-intensive benchmark based on Linpack showed core-scheduling performance that was slightly better than mainline, while turning off SMT incurs a 90% performance hit.</font><br>
<p>
This is very surprising benchmark result. I would not have assumed SMT on or off makes any significant difference for a linpack benchmark.<br>
<p>
Typically well written linear algebra programs utilize all relevant components of the cpu constantly with very little idle time. As I understand it SMT does not increase the number of floating point units of the physical cpu it only allows them to be used by another thread if they are idle which should not be the case for most numerical code.<br>
<p>
Poorly written code or operations lower than BLAS level 3 may of course be blocked on memory loads but if that is the case you SMT or not should also not make much difference either as it does not double your memory bandwidth (and linpack should not be poorly written).<br>
<p>
In my experience SMT for numerical programs give almost zero performance improvements. In most cases it decreases performance when the program assumes a logical core is a physical core and starts too many workers leading to cache trashing.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/820532/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor820578"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Completing and merging core scheduling</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2020 20:23 UTC (Thu)
                               by <b>jdesfossez</b> (subscriber, #123831)
                              [<a href="/Articles/820578/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You are correct that linpack alone doesn't have this issue when the system is not overcommitted (and actually benefit from smt off). However, in this case, it is running concurrently with 96 1-vcpu noise VMs on the same NUMA node (3:1 overcommit that becomes 6:1 with smt off). I clarified this above for the MySQL benchmark, but the linpack benchmark was running in the same configuration.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/820578/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2020, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
