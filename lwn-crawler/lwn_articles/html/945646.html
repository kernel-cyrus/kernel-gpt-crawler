        <!DOCTYPE html>
        <html lang="en">
        <head><title>Moving the kernel to large block sizes [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/945646/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/945212/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/945646/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Moving the kernel to large block sizes</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>September 27, 2023</br>
           <hr>
<a href="/Archives/ConferenceByYear/#2023-Open_Source_Summit_Europe">OSSEU</a>
</div>
<p>
Using larger block sizes in the kernel for I/O is a recurring topic in
storage and 
block-layer circles.  The topic came up in <a
href="/Articles/933437/">discussions</a> 
at the Linux Storage, Filesystem, Memory-Management and BPF Summit (LSFMM)
back in 
May.  One of the participants in those discussions, Hannes Reinecke,  gave
a talk at Open Source Summit Europe 2023 with an overview of the reasons
behind using larger blocks for I/O, the current status of that work, and
where it all might lead from here.
</p>

<p>
Reinecke has worked at SUSE for "like an eternity, nearly 20 years now" and
was involved with Linux before that; his first kernel was 1.1.5 or 1.0.5.
More recently he has been involved with storage and with NVMe in
particular.  That led to a pet project of his "that has finally come to
life", which is to be able to use larger blocks in Linux.
</p>

<h4>Blocks and pages</h4>

<p>
Currently, Linux is restricted to using block sizes that are no larger than
<tt>PAGE_SIZE</tt>, which is typically 4KB.  But there are some systems and
applications that would benefit from using larger pages; for example, some
databases would really like to be able to work with chunks of 16KB because
that is how they are organized internally.  In addition, some hardware
would benefit from handling data in larger block sizes because it reduces
the amount of overhead needed to internally track the blocks, thus making
the drives more efficient and cheaper.
</p>

<a href="/Articles/945729/">
<img src="https://static.lwn.net/images/2023/osseu-reinecke-sm.png" border=0 hspace=5 align="left"
width=260 height=240 alt="[Hannes Reinecke]" title="Hannes Reinecke">
</a>

<p>
But, does there have to be a block size, he asked, couldn't the kernel just use
whatever amount of data it wants to at the time?  The problem is that there
is no "do I/O" instruction that atomically reads or writes some arbitrary
amount of data.  Each I/O operation requires multiple instructions to set
it up, transfer the data, and gather up the results.  That increases
the latency for each operation, so the
goal is to minimize the number of I/O operations that are done, but there
is a balance to be struck.
</p>

<p>
There is a question of what the right size for these I/O operations should
be.  That was the subject of a lot of experimentation in the early days, he
said.  Eventually, researchers at University of California Berkeley ("of
course, as usual") figured out that 512 bytes gave a reasonable compromise
value 
between the overhead and I/O granularity.  That was twenty years ago, at
this point, but we still use 512 bytes—at least for now.
</p>

<p>
CPUs have hardware-assisted memory management that operates in pages,
however.  There is support for determining which pages are dirty
(i.e. need to be written to the backing store) that
operate in page-size chunks, for example.  That means the size of the page
is CPU-dependent, Linux cannot just arbitrarily choose a size.  For x86_64,
the choices are 4KB, 2MB, or 1GB; for PowerPC and some Arm
systems, 16KB is used as the page size.  The kernel has a compile-time
<tt>PAGE_SIZE</tt> setting that governs the size of the page.
</p>

<p>
There is a need to read the pages in memory from disk, or to flush their
contents to disk, at times.  For buffered I/O, the page cache is what
manages all of that; it 
uses the hardware-supplied dirty-page information to determine which pages
need to be written. Since all of that is done at page granularity,
it is natural to do I/O in page-size units.
</p>

<p>
But if you had a number of consecutive pages that were all dirty, you could
do I/O on the whole set of pages at once.  Having a data structure that
handles more than one page as a single unit would facilitate that, which is
what <a href="/Articles/849538/">folios</a> are all about.  
Beyond buffered I/O, there is direct I/O, which user
space has complete control over; the page cache is not involved and user
space can do I/O in multiple blocks if it wants.  Buffered I/O is
provided by the 
filesystems via the page cache and there are a few different interfaces
that can be used for that I/O.  The oldest is buffer heads, its successor
(of sorts) uses 
<tt>struct bio</tt>, and more recently there is iomap, which he said he would
be getting back to.  In order to do buffered I/O in larger sizes, though,
the page 
cache needs to be converted to use folios.
</p>

<h4>Folios</h4>

<p>
Folios are an effort to treat different kinds of pages in a common way.
There are normal pages, compound pages (like an array of pages), and
transparent hugepages (THPs), each of which has its own quirks.  All of
them can be addressed using a <tt>struct page</tt>, though, so kernel
developers have to know whether a given page structure is actually a
page—or something more complicated.  A folio is explicitly designed to
handle the different types and, importantly for his talk, it can represent
more than a single page, thus allowing it to be used for larger block I/O.
</p>

<p>
That requires converting the page cache—and probably the memory-management
subsystem eventually—to use folios.  That effort was proposed by Matthew
Wilcox in 
2020 and has been discussed at every LSFMM since.  It has also been
the subject of sometimes contentious mailing-list discussions over that
span.  But the work is 
ongoing and will be for several more years ("we will get there
eventually").  He showed counts of "<tt>struct page</tt>" (8385) versus
"<tt>struct folio</tt>" (1859) in the 6.4-rc2 kernel as a rough guide to
where things stand.
</p>

<p>
He then turned to buffer heads, which were present in the 0.01 kernel, so
they are the original structure for I/O in Linux.  Each buffer head is for
a single 512-byte disk sector, it is linked to a particular page structure,
and is internally cached in the buffer cache (to save on I/O when accessing
it).  Buffer heads are still in use by most filesystems and they are also
used in a pseudo-filesystem for block devices.  The page cache only came
later in the kernel's history because the buffer cache was sufficient for
the early days. 
</p>

<p>
A <tt>struct buffer_head</tt> is complicated, so in the 2.5 kernel, <a
href="/Articles/26404/"><tt>struct bio</tt> was introduced</a> as a "basic
I/O structure" for device drivers.  It allows for vectorized I/O to or from
an array of pages, routing and rerouting the bio structures to various
devices, and is abstracted away from the page cache.  These days, buffer
heads are implemented on top of the bio infrastructure. There are a number
of filesystems, such as AFS, CIFS, NFS, and FUSE, that use <tt>struct
bio</tt> directly, thus do not rely on buffer heads.
</p>

<p>
Finally, there is iomap "or Christoph Hellwig going crazy"; Hellwig got fed
up with the existing I/O interfaces and created iomap as a replacement,
Reinecke said.  Iomap is a modern interface that already uses folios; it
provides a way for a filesystem to specify how the I/O should
be mapped and leaves the rest to the block layer.  Several filesystems have
been converted to use iomap, including XFS, Btrfs, and Zonefs, so nothing
more needs to be done for those with regard to the folio conversion.  One
problem area for iomap, though, is documentation, which is somewhat hard to
find and often out of date because iomap is under active development.
</p>

<h4>Replacing buffer heads?</h4>

<p>
The storage community has long had a consensus that "buffer heads must
die", he said.  He led a <a href="/Articles/931809/">discussion on that
topic</a> at this year's LSFMM.  The thinking is that buffer heads are a
legacy interface, using 
an ancient structure, so users should be converting to <tt>struct bio</tt>
or iomap. But, a recent conversation on the ksummit-discuss mailing list
contained a <a
href="/ml/ksummit-discuss/CAHk-=wg=xY6id92yS3=B59UfKmTmOgq+NNv+cqCMZ1Yr=FwR9A@mail.gmail.com/">disagreement
from Linus Torvalds</a>.

<p>
The vehemence of that response perhaps indicates that a different path
should be chosen to get to the 
goal of larger block sizes, Reinecke
said.  Conversion to folios is useful, but only affects the page cache and
the memory-management subsystem; buffer heads assume that I/O will be done
in sub-page granularity (i.e. 512 bytes), so that needs to be addressed.
One path might be to convert everything to iomap and then remove buffer
heads, another would be to update buffer heads to work with larger I/O sizes.
</p>

<p>
In an ideal world, all filesystems would be converted to use iomap, he said; it
is a "modern interface and it is actually quite a nice interface".  But, as
the ksummit-discuss <a
href="/ml/ksummit-discuss/ZO9NK0FchtYjOuIH@infradead.org/">thread</a> has
shown, there are legacy filesystems that lack an active maintainer—or any
maintainer at all.  There is often little or no documentation for the
legacy filesystems and no real way to test changes to them.  Beyond that,
converting any filesystems (legacy or not) is going to require better iomap
documentation for the developers working on the conversions.
</p>

<p>
Another possibility is to simply remove buffer heads; there is a patch set
from Hellwig that <a href="/Articles/930173/">allows compiling buffer heads
out of the kernel</a>, which was merged for the 6.5 kernel.  Turning that
on would mean disabling all of the filesystems that use buffer heads, which
is not entirely realistic at this point, Reinecke said.  In particular, the
FAT filesystem, which is needed for booting UEFI systems, would
not be present in such a kernel.
</p>

<p>
At LSFMM, Josef Bacik raised the idea of converting buffer heads to use
folios, so that it could handle both sub-page and super-page I/O.  While
that is not the direction Reinecke would have chosen, he started to
consider it.  A conversion of that sort could either be fairly trivial, if
the code was written without wholesale assumptions about sub-page I/O, or
"it could be a complete nightmare" because that assumption is pervasive.
</p>

<p>
Later that day, he was sitting at the bar after looking at the buffer heads code
and "complaining bitterly" to his neighbor about them.  He wondered how
anyone could be expected to convert them, since they are so closely tied to
pages.  He then
realized that his neighbor was Andrew Morton, who said: "back in the day
when I wrote it, it was quite good—and it still works, doesn't it?"
</p>

<p>
So, Reinecke started to reconsider the idea of converting buffer heads to
folios, but there are a number of problems that need to be solved.  For one
thing, buffer heads and iomap are fundamentally incompatible. For example,
there is a void pointer in the page structure that either points to a
buffer head or an iomap structure, depending on which is being used; when
looking at a page in the page cache, it is important to know which you
have.  The "mix and match approach" needs to be considered carefully.
Reviewing the changes will be difficult, he said, because dependencies on
<tt>PAGE_SIZE</tt> are hard to spot.
</p>

<p>
All of that starts to make him wonder whether the overarching goal of I/O
using larger block sizes is really worth all of this effort. "I think it is
... but that's just me."  But he does know that databases really want to be
able to do larger I/Os and the hope is that supporting larger I/Os will be
more efficient for filesystems as well.  For the most part, filesystems
already do I/O in larger chunks.  Beyond those benefits, the drive vendors
would like to use larger blocks for efficiency, capacity, and, ultimately,
cheaper devices.
</p>

<h4>Progress</h4>

<p>
Reinecke had been working away on his patches and finished his <a
href="/ml/linux-fsdevel/20230918110510.66470-1-hare@suse.de/">patch set</a>
the previous week.  As sometimes happens in the open-source world, though,
another implementation surfaced around the same time.
Luis Chamberlain and his colleagues at Samsung posted a <a href="/ml/linux-fsdevel/20230915213254.2724586-1-mcgrof@kernel.org/">different
patch set</a> that covers much of the same ground.  In the talk, Reinecke
said that he was presenting his own patches to solve these problems, but that he
would be working with the folks from Samsung on combining the two
approaches in the near future.
</p>

<p>
The overall idea is to switch buffer heads to be attached to a folio
rather than to a page.  That way, all of the I/O would still be smaller
than the attached unit, so the assumptions in the buffer heads code would
still be met. The folio would have a pointer to a single buffer head or to
a list of buffer heads.  There are some things that need to be kept in mind
with this conversion; foremost is that the memory-management subsystem
still works in units of <tt>PAGE_SIZE</tt>, while the page cache and buffer
cache have moved to folios.
</p>

<p>
But, in order to do I/O, buffer heads use the bio mechanism, which operates
in 512-byte blocks.  That is effectively hardwired throughout the block
layer and its drivers—it is not something that can be changed without
enormous effort, he said.
But the actual I/O is handled by the lower-layer drivers, which already
merge adjacent blocks into larger units.  So the folios in the page cache
can be handed to the block layer, which will enumerate them in 512-byte
blocks, hand the results to the driver that will reassemble them into
larger units. It all "should just work", even though it is not really the
obvious way to attack the problem.
</p>

<p>
So that is the core of what his patch set does.  There was still other work
to do, of course, 
including auditing the page cache to ensure that it is allocating folios of
the size used by the underlying drive and to ensure that it is incrementing
in folio-size steps, not by pages.  He also needed to add an interface for
the block drivers to report their block size to the page cache.  It all
worked well, perhaps even too well, since NFS wanted 128MB blocks—and got
them—at least until the virtual machine hit an out-of-memory condition.
That particular test "neatly proved that all large blocks leads to a higher
memory fragmentation" if such a proof was actually needed. 
</p>

<h4>Done yet?</h4>

<p>
While it is great that these patches enable the kernel to talk to drives
with large block sizes, there is a still a problem: there are no drives
with large block sizes "because no one can talk to them".  He has patches
to update the block ramdisk driver (brd) to support larger blocks for
testing purposes.  That driver could then be used as the backing device for
an NVMe target so that it could be tested with large block sizes. "That was
quite cool but, of course, there is still 
some testing needed."
</p>

<p>
There are still some pieces needed as well.  QEMU needs to be updated to
support large block sizes, the drivers need to be exercised using them, and
other subsystems, such as SCSI, need to be tested.  Beyond that,
unification with the Samsung work will be required.  Once that is all in
hand, there will be reviews and the fallout from those to deal with as well
before this work can go upstream.
</p>

<p>
The memory-fragmentation issue is one that is still unresolved.  Systems
may well have devices with different block sizes in the future; 16KB should
not be a major problem in this regard, but even larger block sizes are
possible down the road.  The memory-management layer continues to work in
page-size chunks, which will lead to additional fragmentation.  If systems
could switch to using memory at the same granularity as the larger blocks,
all would be well—but that assumes there is only one large block size,
which may well not be true.
</p>

<p>
One possible solution, which may be worth doing in its own right, is to
switch the SLUB allocator to use higher-order folios, rather than page
granularity.   Then if <tt>alloc_page()</tt> users were converted to use
SLUB, it would remove the fragmentation problem for allocations.  Once
again, though, that relies on there being a single large block size.  He
would be interested in hearing other ideas for improving the fragmentation
situation in the presence of larger block sizes.
</p>

<p>
He closed his talk with a suggestion "in case you are really bored": there
is still the block 
layer and its 512-byte orientation that could be improved.  Switching the
block layer to use 
folios is not something for the faint of heart, but it should be doable, he
thinks.
The bio structure does not store the data directly, but uses a <tt>struct
bio_vec</tt> for the data in a vectorized form.  Those could perhaps be
converted to use folios instead of pages, though there are some 4,000 uses
of <tt>bio_vec</tt> in the block layer.
</p>

<p>
[I would like to thank LWN's travel sponsor, the Linux Foundation, for
travel assistance to Bilbao for OSSEU.]
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer">Block layer</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Open_Source_Summit_Europe-2023">Open Source Summit Europe/2023</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/945646/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor945734"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 27, 2023 21:27 UTC (Wed)
                               by <b>james</b> (subscriber, #1325)
                              [<a href="/Articles/945734/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      So why not leave these barely-maintained filesystems alone? I'd have thought their users would have preferred stability to increased performance, especially since the filesystems are rarely going to be on NVMe (so software performance is unlikely to be a limiting factor).



      
          <div class="CommentReplyButton">
            <form action="/Articles/945734/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor945740"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 28, 2023 3:41 UTC (Thu)
                               by <b>stop50</b> (subscriber, #154894)
                              [<a href="/Articles/945740/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
yes, if they have no maintainer then they are on their way out of the kernel anyway.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945740/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor945741"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 28, 2023 4:17 UTC (Thu)
                               by <b>kazer</b> (subscriber, #134462)
                              [<a href="/Articles/945741/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Because you would potentially need to maintain two concepts at the same time, doubling the workload and headache. Main purpose of this work is to reduce workload and interfaces, not add yet another path. And like it was said, you still need some of those filesystems to talk to UEFI and stuff like that.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945741/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor945744"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 28, 2023 7:34 UTC (Thu)
                               by <b>iabervon</b> (subscriber, #722)
                              [<a href="/Articles/945744/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
What tends to happen when only unmaintained drivers continue to use a core API after all of the maintained drivers move to a newer, nicer core API is that the old core API gets broken eventually and none of the tests people know how to write catch the problem, because it's only depended on by code that's gone unmaintained, and the users who preferred stability find that programs running on a new kernel accessing files on the old filesystem only get 4k of I/O out of every ~16k of memory because memory management has started using larger folios but the old core API is still assuming single pages.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945744/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor945986"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 30, 2023 0:49 UTC (Sat)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/945986/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Twenty years ago when 512 bytes was chosen?  Maybe forty! (1983, not 2003)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945986/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor945994"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 30, 2023 6:50 UTC (Sat)
                               by <b>Wol</b> (subscriber, #4433)
                              [<a href="/Articles/945994/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That makes a lot of sense actually. 128 * 512B blocks was your typical MAXIMUM memory back then ...<br>
<p>
"My" first computer was a mini that served 20 users on 256kB of ram, this would be 1982.<br>
<p>
Cheers<br>
Wol<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945994/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor946915"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 6, 2023 21:32 UTC (Fri)
                               by <b>pjdesno</b> (guest, #167375)
                              [<a href="/Articles/946915/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The VAX had a 512-byte page size, which I've heard was because marketing wanted to be able to sell a version with 128KB RAM and you needed more than 128 pages to avoid deadlock if each of several critical processes executed crazy multi-memory-access VAX instructions (+ crossing page boundaries) at the same time. (128K wasn't realistic, but I can remember using a 780 with 1MB memory and 20 or more simultaneous users. I don't remember it fondly.)<br>
<p>
The dominance of the 512-byte disk sector size probably traces to IDE in the mid-80s.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/946915/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor945987"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Moving the kernel to large block sizes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 30, 2023 1:44 UTC (Sat)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/945987/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Wow, there's a lot here to comment on ...<br>
<p>
I kicked off the conversion of buffer heads from b_page to b_folio in December 2022 with a 12 patch series (LSFMM was 6 months later in May). Pankaj from Samsung did some work in April. I did some more in June, then another round in July, and another this month.<br>
<p>
Buffer heads do not represent 512 byte blocks. They represent filesystem block size blocks (this may be &gt;= device block size). Where 512 bytes is still used is in describing disc locations to the block layer. But that's just a shift; we might as well use bytes.<br>
<p>
The important reason to need large folios to support large drive block sizes is that the block size is the minimum I/O size. That means that if we're going to write from the page cache, we need the entire block to be present. We can't evict one page and then try to write back the other pages -- we'd have to read the page we evicted back in. So we want to track dirtiness and presence on a per-folio basis; and we must restrict folio size to be no smaller than block size.<br>
<p>
<span class="QuotedText">&gt; the folios in the page cache can be handed to the block layer, which will enumerate them in 512-byte blocks, hand the results to the driver that will reassemble them into larger units.</span><br>
<p>
That's not how it works. The writeback code will enumerate each dirty folio. The filesystem ends up calling bio_add_folio() (most currently call bio_add_page()) and the bio will contain the entire contents of the folio.<br>
<p>
SLUB already uses higher order folios. There's a boot option to set the minimum order.<br>
<p>
There's no immediate need to replace bio_vec. It handles multiple pages just fine. There are very good reasons to replace it though -- see my struct phyr proposal, that I don't have time to work on.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/945987/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2023, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
