        <!DOCTYPE html>
        <html lang="en">
        <head><title>The 2010 Linux Storage and Filesystem Summit, day 2 [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/399313/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/398846/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/399313/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>The 2010 Linux Storage and Filesystem Summit, day 2</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>August 10, 2010</br>
           </div>
The second day of the 2010 Linux Storage and Filesystem Summit was held on
August&nbsp;9 in Boston.  Those who have not yet read <a
href="http://lwn.net/Articles/399148/">the coverage from day&nbsp;1</a> may
want to start there.  This day's topics were, in general, more detailed and
technical and less amenable to summarization here.  Nonetheless, your
editor will try his best.
<p>
<h4>Writeback</h4>
<p>
The first session of the day was dedicated to the writeback issue.
Writeback, of course, is the process of writing modified pages of files
back to persistent store.  There have been numerous complaints over recent
years that writeback performance in Linux has regressed; the curious reader
can
refer to <a href="http://lwn.net/Articles/396561/">this article</a> for
some details, or <a
href="https://bugzilla.kernel.org/show_bug.cgi?id=12309">this 
bugzilla entry</a> for many, many details.  The discussion was less focused
on this specific problem, though; instead, the developers considered the
problems with writeback as a whole.
<p>
Sorin Faibish started with a discussion of some research that he has done
in this area.  The challenges for writeback are familiar to those who have
been watching the industry; the size of our systems - in terms of both
memory and storage - has increased, but speed of those systems
has not increased proportionally.  As a result, writing back a given
percentage of a system's pages takes longer than it once did.  It is always
easier for the writeback system to fail to keep up with processes which are
dirtying pages, leading to poor performance.
<p>
His assertion is that the use of watermarks to control writeback is no
longer appropriate for contemporary systems.  Writeback should not wait
until a certain percentage of memory is dirty; it should start sooner, and,
crucially, be tied to the rate with which processes are dirtying pages.
The system, he says, should work much more aggressively to ensure that the
writeback rate matches the dirty rate.
<p>
From there, the discussion wandered through a number of specific issues.
Linux writeback now works by flushing out pages belonging to a specific file
(inode) at a time, with the hope that those pages will be located nearby on
the disk.  The memory management code will normally ask the filesystem to
flush out up to 4MB of data for each inode.  One poorly-kept secret of
Linux memory management is that filesystems routinely ignore that request -
they typically flush far more data than requested if there are that many
dirty pages.  It's only by generating much larger I/O requests that they
can get the best performance.
<p>
Ted Ts'o wondered if blindly increasing writeback size is the best thing to
do.  4MB is clearly too small 
for most drives, but it may well be too large for a filesystem located on a
slow USB drive.  Flushing large amounts of data to such a filesystem can
stall any other I/O to that device for quite some time.  From this
discussion came the idea that writeback should not be based on specific
amounts of data, but, instead, should be time-based.  Essentially, the
backing device should be time-shared between competing interests in a way
similar to how the CPU is shared.
<p>
James Bottomley asked if this idea made sense - is it ever right to cut
off I/O to an inode which still has contiguous, dirty pages to write?  The answer
seems to be 
"yes."  Consider a process which is copying a large file - a DVD image or
something even larger.  Writeback might not catch up with such a process
until the copy is done, which may not be for a long time into the future;
meanwhile, all other users of that device will be starved.  That is bad for
interactivity, and it can cause long delays before other files are flushed
to disk.  Also, the incremental performance benefit of extending large I/O
operations tend to drop off over time.  So, in the end, it's necessary to
switch to another inode at some point, and making the change based on
wall-clock time seems to be the most promising approach.
<p>
Boaz Harrosh raised the idea of moving the I/O scheduler's
intelligence up to the virtual memory management level.  Then, perhaps,
application priorities could be used to give interactive processes
privileged access to I/O bandwidth.  Ted, instead, suggested that there may
be value in allowing
the assignment of priorities to individual file descriptors.  It's fairly
common for an application to have files it really cares about, and others
(log files, say) which matter less.  The problem with all of these ideas,
according to Christoph Hellwig, is that the kernel has far too many I/O
submission paths.  The block layer is the only place where all of those I/O
operations come together into a single place, so it's the only place where
any sort of reasonable I/O control can be applied.  A lot of fancy schemes
are hard to implement at that level, so, even if descriptor-based
priorities are a good idea (not everybody was convinced), it's not
something that can readily be done now.  Unifying the I/O submission paths
was seen as a good idea, but it's not something for the near future.

<p>
Jan Kara asked about how results can be measured, and against what
requirements will they be judged?  Without that information, it is hard to
know if any changes have had good effects or not.  There are trivial cases,
of course - changes which slow down kernel compiles tend to be caught early
on.  But, in general, we have no way to measure how well we are doing with
writeback.
So, in the end, the first action item is likely to be an attempt to set
down the requirements and to develop some good test cases.  Once it's
possible to decide whether patches make sense, there will probably an
implementation of some sort of time-based writeback mechanism.
<p>
<h4>Solid-state storage devices</h4>
<p>
There were two sessions on solid-state storage devices (SSDs) at the
summit; your editor was able to attend only the first.  The situation which
was described there is one we have been hearing about for a couple of years
at least.  These devices are getting faster: they are heading toward a
point where
they can perform one million I/O operations per second.  That said, they
still exhibit significant latency on operations (though much less than
rotating drives do), so the only way to get that kind of operation count is
to run a lot of operations in parallel.  "A lot" in this case means having
something like 100 operations in flight at any given time.
<p>
Current SSDs work reasonably well with Linux, but there are certainly some
problems.  There is far too much overhead in the ATA and SCSI layers; at
that kind of operation rate, microseconds hurt. The block layer's request
queues are becoming a bottleneck; it's currently only possible to have
about 32 concurrent operations outstanding on a device.  The system needs to be able to
distribute I/O completion work across multiple CPUs, preferably using smart
controllers which can direct each completion interrupt to the CPU which
initiated a specific operation in the first place.
<p>
For "storage-attached" SSDs (those which look like traditional disks),
there are not a lot of problems at the filesystem level; things work pretty
well.  Once one gets into bus-attached devices which do not look like
disks, though, the situation changes.  One participant asserted that, on
such devices, the ext4 filesystem could not be expected to get reasonable
performance without a significant redesign.  There is just too much to do
in parallel.
<p>
Ric Wheeler questioned the claim that SSDs are bringing a new challenge
for the storage subsystem.  Very high-end enterprise storage arrays have
achieved this kind of I/O rate for some years now.  One thing those arrays
do is present multiple devices to the system, naturally helping with
parallelism; perhaps SSDs could be logically partitioned in the same way.

<p>
<h4>Resizing guest memory</h4>
<p>
A change of pace was had in the memory management track, where Rik van Riel
talked about the challenges involved in resizing the memory available to
virtualized guests.  There are four different techniques in use currently:
<p>
<ul>
<li> Memory hotplug by way of simulated hardware hotplug events.  This
     mechanism works well for adding memory to guests, but it cannot really
     be used to take memory back.  Hot remove simply does not work well,
     because there's always some sort of non-movable allocation which ends
     up in the space which would be removed.
<p>
<li> Ballooning, wherein a special driver in the guest allocates pages and
     retires them from use, essentially handing them back to the host.
     Memory can be fed back into the guest by having the balloon driver
     free the pages it has allocated.  This mechanism is simple, if
     somewhat slow, but simple management policies are scarce.
<p>
<li> Transcendent memory techniques like <a
     href="http://lwn.net/Articles/386090/">cleancache and frontswap</a>,
     which can be used to adjust memory availability between virtual
     guests.
<p>
<li> Page hinting, whereby guests mark pages which can be discarded by the
     host.  These pages may be on the guest's free list, or they may simply
     be clean pages.  Should the guest try to access such a page after the
     host has thrown it away, that guest will receive a special page fault
     telling it that it needs to allocate the page anew.  Hinting
     techniques tend to bring a lot of complexity with them.
</ul>
<p>
The real question of interest in this session seemed to be the
"rightsizing" of guests - giving each guest just enough memory to optimize
the performance of the system as a whole.  Google is also interested in
this problem, though it is using cgroup-based containers instead of full
virtualization.  It comes down to figuring out what a process's minimal
working set size is - a problem which has resisted attempts at solution for
decades.
<p>
Mel Gorman proposed one approach to determine a guest's working set size.
Place that guest under memory pressure, slowly shrinking its available
memory over time.  There will come a point where the kernel starts scanning
for reclaimable pages, and, as the pressure grows, a point where the
process starts paging in pages which it had previously used.  That latter
point could be deemed to be the place where the available memory had fallen
below the working set size.  It was also suggested that page reactivations
- when pages are recovered from the inactive list and placed back into
active use - could also be the metric by which the optimal size is
determined.
<p>
Nick Piggin was skeptical of such schemes, though.  He gave the example of
two processes, one of which is repeatedly working through a 1GB file, while
the other is working through a 1TB file.  If both processes currently have
512MB of memory available, they will both be doing significant amounts of
paging.  Adjusting the memory size will not change that behavior, leading
to the conclusion that there's not much to be done - until the process with
the smaller file gets 1GB of memory to work with.  At that point, its
paging will stop.  The process working with the larger file will never
reach that point, though, at least on contemporary systems.  So, even
though both processes are paging at the same rate, the initial 512MB memory
size is too small for one process, but is just fine for the other.
<p>
The fact that the problem is hard has not stopped developers from trying to
improve the situation, though, so we are likely to see attempts made at
dynamically resizing guests in an attempt to work out their optimal sizes.
<p>
<h4>I/O bandwidth controllers</h4>
<p>
Vivek Goyal led a brief session on the I/O bandwidth controller problem.
Part of that problem has been solved - there is now a proportional-weight
bandwidth controller in the mainline kernel.  This controller works well
for single-spindle drives, perhaps a bit less so with large arrays.  With
larger systems, the single dispatch queue in the CFQ scheduler becomes a
bottleneck.  Vivek has been working on <a
href="http://lwn.net/Articles/396530/">a set of patches</a> to improve that
situation for a little while now.
<p>
The real challenge, though, is the desired maximum bandwidth controller.
The proportional controller which is there now will happily let a process
consume massive amounts of bandwidth in the absence of contention.  In most
cases, that's the right result, but there are hosting providers out there
who want to be able to keep their customers within the bandwidth limits
they have paid for.  The problem here is figuring out where to implement
this feature.  Doing it at the I/O scheduler level doesn't work well when
there are devices stacked higher in the chain.
<p>
One suggestion is to create a special device mapper target which would do
maximum bandwidth throttling.  There was some resistance to that idea,
partly because some people would rather avoid the device mapper altogether,
but also due to practical problems like the inability of current Linux
kernels to insert a DM-based controller into the stack for an
already-mounted disk.  So 
we may see an attempt to add this feature at the request queue level, or we
may see a new hook allowing a block I/O stream to be rerouted through a new
module on the fly.
<p>
The other feature which is high on the list is support for controlling
buffered I/O bandwidth.  Buffered I/O is hard; by the time an I/O request
has made it to the block subsystem, it has been effectively detached from
the originating process.  Getting around that requires adding some new
page-level accounting, which is not a lightweight solution.
<p>
<h4>Reclaim topics</h4>
<p>
Back in the memory management track, a number of reclaim-oriented topics
were covered briefly.  The first of these is per-cgroup reclaim.  Control
groups can be used now to limit total memory use, so reclaim of anonymous
and page-cache pages works just fine.  What is missing, though, is the sort
of lower-level reclaim used by the kernel to recover memory: shrinking of
slab caches, trimming the inode cache, etc.  A cgroup can consume
considerable resources with this kind of structure, and there is currently
no mechanism for putting a lid on such usage.
<p>
Zone-based reclaim would also
be nice; that is evidently covered in the VFS scalability patch set, and
may be pushed toward the mainline as a standalone patch.
<p>
Reclaim of smaller structures is a problem which came up a few times this
afternoon.  These structures are reclaimed individually, but the virtual
memory subsystem is really only concerned with the reclaim of full pages.
So reclaiming individual inodes (or dentries, or whatever) may just serve
to lose useful cached information and increase fragmentation without
actually freeing any memory for the rest of the system.  So it might be
nice to change the reclaim of structures like dentries to be more
page-focused, so that useful chunks of memory can be returned to the
system.
<p>
The ability to move these structures around in memory,
freeing pages through defragmentation, would also be useful.  That is a
hard problem, though, 
which will not be amenable to a quick solution.
<p>
There is an interesting problem with inode reclaim: cleaning up an inode
also clears all related page cache pages out of the system.  There can be
times when that's not what's really called for.  It can free vast amounts of
memory when only small amounts are needed, and it can deprive the system of
cached data which will just need to be read in again in the near future.
So there may be an attempt to change how inode reclaim works sometime soon.
<p>
There are some difficulties with how the page allocator works on larger
systems; free memory can go well below the low watermark before the system
notices.  That is the result of how the per-CPU queues work; as the number
of processors grows, the accounting of the size of those queues gets
fuzzier.  So there was talk of sending inter-processor interrupts on
occasion to get a better count, but that is a very expensive solution.
Better, perhaps, is just to iterate over the per-CPU data structures and
take the locking overhead.
<p>
<h4>Slab allocators</h4>
<p>
Christoph Lameter ran a discussion on slab allocators, talking about the
three allocators which are currently in the kernel and the attempts which
are being made to unify them.  This is a contentious topic, but there was a
relative lack of contentious people in the room, so the discussion was
subdued.  What happens will really depend on what patches Christoph posts
in the future.
<p>
<h4>O_DIRECT</h4>
<p>
A brief session touched on a few problems associated with direct I/O.  The
first of these is an obscure race between <tt>get_user_pages()</tt> (which
pins user-space pages in memory so they can be used for I/O) and the
<tt>fork()</tt> system call.  In some cases, a <tt>fork()</tt> while the
pages are mapped can corrupt the system.  A number of fixes have been
posted, but they have not gotten past Linus.  The real fix will involve
fixing all <tt>get_user_pages()</tt> callers and (the real point of
contention) slowing down <tt>fork()</tt>.  The race is a real problem, so
some sort of solution will need to find its way into the mainline.
<p>
Why, it was asked, do applications use direct I/O instead of just mapping
file pages into their address space?  The answer is that these applications
know what they want to do with the hardware and do not want the virtual
memory system getting in the way.  This is generally seen as a valid
requirement.
<p>
There is some desire for the ability to do direct I/O from the virtual
memory subsystem itself.  This feature could be used to support, for
example, swapping over NFS in a safe way.  Expect patches in the near
future.
<p>
Finally, there is a problem with direct I/O to transparent hugepages.  The
kernel will go through and call <tt>get_user_pages_fast()</tt> for each 4K
subpage, but that is unnecessary.  So 512 mapping calls are being made when
one would do.  Some kind of fix will eventually need to be made so that
this kind of I/O can be done more efficiently.
<p>
<h4>Lightning talks</h4>
<p>
Once again, the day ended with lightning talk topics.  Matthew Wilcox
started by asking developers to work at changing more uninterruptible waits
into "killable" waits.  The difference is that uninterruptible waits can,
if they wait for a long time, create unkillable processes.  System
administrators don't like such processes; "<tt>kill&nbsp;-9</tt>" should
really work at all times.
<p>
The problem is that making this change is often not straightforward; it
turns a function call which cannot fail into one which can be interrupted.
That means that, for each change, a new error path must be added which
properly unwinds any work which had been done so far.  That is typically
not a simple change, especially for somebody who does not intimately
understand the code in question, so it's not the kind of job that one
person can just take care of.
<p>
It was suggested that iSCSI drives - which can cause long delays if they
fall off the net - are a good way of testing this kind of code.  From
there, the discussion wandered into the right way of dealing with the
problems which result when network-attached drives disappear.  They can
often hang the system for long periods of time, which is unfortunate.  Even
worse, they can sometimes reappear as the same drive after buffers have
been dropped, leading to data corruption.
The solution to all of this is faster and better recovery when devices
disappear, especially once it becomes clear that they will not be coming
back anytime soon.  Additionally, 
should one of those devices reappear after the system has given 
up on it, the storage layer should take care that it shows up as a totally
new device.  Work will be done to this 
end in the near future.
<p>
Mike Rubin talked a bit about how things are done at Google.  There are
currently about 25 kernel engineers working there, but few of them are
senior-level developers.  That, it was suggested, explains some of the
things that Google has tried to do in the kernel.
<p>
There are two fundamental types of workload at Google.  "Shared" workloads
work like classic mainframe batch jobs, contending for resources while the
system tries to isolate them from each other.  "Dedicated workloads" are
the ones which actually make money for Google - indexing, searching, and
such - and are very sensitive to performance degradation.  In general, any
new kernel which shows a 1% or higher performance regression is deemed to
not be good enough.
<p>
The workloads exhibit a lot of big, sequential writes and smaller, random
reads.  Disk I/O latencies matter a lot for dedicated workloads; 15ms
latencies can cause phone calls to the development group.  The systems are
typically doing direct I/O on not-too-huge files, with logging happening on
the side.  The disk is shared between jobs, with the I/O bandwidth
controller used to arbitrate between them.
<p>
Why is direct I/O used?  It's a decision which dates back to the 2.2 days,
when buffered I/O worked less well than it does now.  Things have gotten
better, but, meanwhile, Google has moved much of its buffer cache
management into user space.  It works much like enterprise database systems
do, and, chances are, that will not change in the near future.
<p>
Google uses the "fake NUMA" feature to partition system memory into 128MB
chunks.  These chunks are assigned to jobs, which are managed by control
groups.  The intent is to firmly isolate all of these jobs, but writeback
still can cause interference between them.
<p>
Why, it was asked, does Google not use xfs?  Currently, Mike said, they are
using ext2 everywhere, and "it sucks."  On the other hand, ext4 has turned
out to be everything they had hoped for.  It's simple to use, and the
migration from ext2 is straightforward.  Given that, they feel no need to
go to a more exotic filesystem.
<p>
Mark Fasheh talked briefly about "cluster convergence," which really means
sharing of code between the two cluster filesystems (GFS2 and OCFS2) in the
mainline kernel.  It turns out that there is a surprising amount of sharing
happening at this point, with the lock manager, management tools, and more
being common to both.  The biggest difference between the two, at this
point, is the on-disk format.
<p>
The cluster filesystems are in a bit of a tough place.  Neither has a huge
group dedicated to its development, and, as Ric Wheeler pointed out, there
just isn't much of a hobbyist community equipped with enterprise-level
storage arrays out there.  So these two projects have struggled to keep up
with the proprietary alternatives.  Combining them into a single cluster
filesystem looks like a good alternative to everybody involved.  Practical
and political difficulties could keep that from happening for some years,
though.
<p>
There was a brief discussion about the DMAPI specification, which describes
an API to be used to control hierarchical storage managers.  What little
support exists in the kernel for this API is going away, leaving companies
with HSM offerings out in the cold.  There are a number of problems with
DMAPI, starting with the fact that it fails badly in the presence of
namespaces.  The API can't be fixed without breaking a range of proprietary
applications.  So it's not clear what the way forward will be.
<p>
<h4>Closing</h4>
<p>
<a href="/Articles/399357/"><img
src="https://static.lwn.net/images/conf/2010/linuxcon/lsf-group-sm.jpg" width=200 height=101
alt="[Group photo]" border=0 align="right"></a>

The summit was widely seen as a successful event, and the participation of
the memory management community was welcomed.  So there will be a joint
summit again for storage, filesystem, and memory management developers next
year.  It could happen as soon as early 2011; the participants would like
to move the event back to the (northern) spring, and waiting for 18 months
for the next gathering seemed like too long.
<br clear="all"><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer">Block layer</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems-Workshops">Filesystems/Workshops</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2010">Storage, Filesystem, and Memory-Management Summit/2010</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/399313/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor399382"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 16:22 UTC (Tue)
                               by <b>MTecknology</b> (subscriber, #57596)
                              [<a href="/Articles/399382/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; The fact that the problem is hard has not stopped developers from trying to improve the situation, [...]</font><br>
And that's why it's called Linux. Giving up and calling something impossible should never happen.<br>
<p>
<font class="QuotedText">&gt; "kill -9" should really work at all times.</font><br>
It's being addressed, yay! I was fighting this yesterday.<br>
<p>
<font class="QuotedText">&gt; In general, any new kernel which shows a 1% or higher performance regression is deemed to not be good enough.</font><br>
That is definitely very sensitive. It's great that kernel devs are able to hold up to it though.<br>
<p>
This was an extremely interesting article. Thanks for taking the time. :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399382/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399418"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Was James Bottomley wearing a bow tie?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 19:03 UTC (Tue)
                               by <b>dougg</b> (guest, #1894)
                              [<a href="/Articles/399418/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Due to Matthew Wilcox's head, I'm unable to determine from the group photograph whether James was wearing one of his trademark bow ties?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399418/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399444"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Was James Bottomley wearing a bow tie?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 21:11 UTC (Tue)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/399444/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
He wore different coloured ones on Sunday and Monday.<br>
I was unable to ascertain whether he was wearing his "Sunday Best" :-)<br>
<p>
Sorry my head got in the way ... I did suggest the front row should crouch.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399444/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399750"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Was James Bottomley wearing a bow tie?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 16:56 UTC (Thu)
                               by <b>bronson</b> (subscriber, #4806)
                              [<a href="/Articles/399750/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Who's the guy looking at the jogger?  :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399750/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399756"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Was James Bottomley wearing a bow tie?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 17:58 UTC (Thu)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/399756/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
My wife pointed that out too ... it's Hannes :-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399756/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor399437"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 20:30 UTC (Tue)
                               by <b>mjthayer</b> (guest, #39183)
                              [<a href="/Articles/399437/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Process-based scheduling for the block I/O layer sounds wonderful.  Can we look forward to systems that no longer become completely unresponsive when the disk I/O gets too heavy?  (That was meant to be happiness at the thought of what is coming, not complaining about what is now...)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399437/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399470"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 0:49 UTC (Wed)
                               by <b>dgc</b> (subscriber, #6611)
                              [<a href="/Articles/399470/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The problem with converting to killable waits is that if you interrupt a filesystem transaction that is waiting on another resource (e.g. a buffer lock) before the transaction can complete, the filesystem has to be able to undo the modifications already made in the transaction to be able to successfully back out and return an error. This is far from simple.<br>
<p>
I'd estimate implementing such functionality in XFS will touch around 30% of the code base and introduce several hundred new error paths that have to be tested (somehow). It's a fundamental design change - the assumption of being allowed to wait forever when in transaction context makes error handling and test matrices so much simpler.<br>
<p>
The only reason I would consider making such a drastic change is if there is some new functionality that requires it. e.g. as the first step for triggering on-line repair when corruption is detected during a transaction....<br>
<p>
Anyway, if you have a hung filesystem, continuing operations after the hang is not going to improve the situation - it'll just get stuck again as the problematic resource is encountered by the next transaction. Being able to run "kill -9" doesn't avoid the issue of needing to correct the problem - that is still likely to require a reboot because you won't be able to unmount the filesystem for repair.<br>
<p>
So while the idea of a fully interruptible fileystem is nice, it's far from being a reality....<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399470/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399487"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 6:23 UTC (Wed)
                               by <b>tialaramex</b> (subscriber, #21167)
                              [<a href="/Articles/399487/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The killable waits are a matter of pragmatism, so you should approach the situation with that in mind. Perhaps there are, as you suggest, hundreds of places in XFS where a device failure could theoretically hang a process indefinitely, but how many actually trigger?<br>
<p>
I'd suggest building an XFS filesystem on an iSCSI disk, and trying two basic scenarios:<br>
<p>
1. Run a heavy file layer benchmark to simulate active use of the disk, pull the Ethernet from the iSCSI device<br>
<p>
2. Idle the filesystem, pull the Ethernet, then immediately do 'ls' or 'cat /dev/urandom &gt; hugeTestFile'<br>
<p>
I suspect that in fact these scenarios will repeatedly get processes stuck in just a handful of waits within XFS. Making just these killable will, we may reasonably guess, help out a lot of administrators for much less work than your proposed "fundamental design change".<br>
<p>
How about it?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399487/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399729"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 15:03 UTC (Thu)
                               by <b>ebirdie</b> (guest, #512)
                              [<a href="/Articles/399729/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Excuse me, if I miss something here, but aren't USB sticks practically used today the way that people tend to pull them off and forget that there was a process copying onto the device/file system or there was some indexer or hidden helper in background having operations on the file system. Not to mention people forget ejecting the device before physical disconnect. Today there exists all the more pluggable devices used as storage so ejecting becomes more and more unpractical all the time. So a process still writing onto an unpluged device should just be killed away automatically (the system administrator example is bad in this usage, I think) or die itself from eating resources like power and from complicating situations like suspending.<br>
<p>
<p>
Secondly, can't a writing process conclude, there will no time in future to complete the request after a decent time has passed, when IO requests become time shared as was told on the Writeback section? I'm just an sysadmin, so I may get many many things wrong here. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399729/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399631"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 21:07 UTC (Wed)
                               by <b>buchanmilne</b> (guest, #42315)
                              [<a href="/Articles/399631/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>The cluster filesystems are in a bit of a tough place. Neither has a huge group dedicated to its development, and, as Ric Wheeler pointed out, there just isn't much of a hobbyist community equipped with enterprise-level storage arrays out there.
</blockquote>
<p>
Enterprise-level storage arrays aren't required to run clustered filesystems. In fact, a single machine (laptop in my case) is sufficient, assuming you can run either Xen or KVM.
<p>
I'm not a kernel developer, but I test 'cluster' and 'gfs2' quite often on a test cluster that comprises two VMs that were originally installed under Xen, but now run under KVM, sharing one block device which is used as the GFS2 filesystem.
<p>
Unfortunately, virtualbox doesn't allow concurrent use of virtual hard disks by multiple VMs, but that's only a problem if you need OSs KVM doesn't boot.
      
          <div class="CommentReplyButton">
            <form action="/Articles/399631/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399636"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 21:24 UTC (Wed)
                               by <b>sniper</b> (guest, #13219)
                              [<a href="/Articles/399636/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
This has been addressed in the recently released VirtualBox 3.2.8.<br>
<p>
<a href="http://www.virtualbox.org/wiki/Changelog">http://www.virtualbox.org/wiki/Changelog</a><br>
<p>
VirtualBox 3.2.8 (released 2010-08-06)<br>
<p>
Sharing disks: support for attaching one disk to several VMs without external tools and tricks (see here for a short explanation)<br>
<p>
<a href="http://www.virtualbox.org/manual/ch05.html#hdimagewrites">http://www.virtualbox.org/manual/ch05.html#hdimagewrites</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399636/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor400142"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2010 23:29 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/400142/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <blockquote>
... I test 'cluster' and 'gfs2' quite often on a test cluster that comprises two VMs ...
</blockquote>
<p>
As a hobby?  The point was that hobbyists don't care about cluster filesystems because they don't have enterprise-level storage arrays and cluster filesystems aren't good for anything else.

      
          <div class="CommentReplyButton">
            <form action="/Articles/400142/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor400360"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 15, 2010 23:33 UTC (Sun)
                               by <b>tytso</b> (subscriber, #9993)
                              [<a href="/Articles/400360/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
Enterprise-level storage arrays aren't required to run clustered filesystems. In fact, a single machine (laptop in my case) is sufficient, assuming you can run either Xen or KVM.</blockquote>

<P>Shared-block cluster file systems (which is what Red Hat's GFS and OCFS2 are) don't provide any redundancy in case of failure; they depend on the hardware being utterly reliable.  Which generally means enterprise storage arrays which are FC connected, which can be equally accessed by all all of the nodes in the cluster file system.  You don't _have_ to run them on an enterprise storage array, but then if any of the hard drives fail, you're really badly out of luck.</p>

<p>There are other possibilities, such as a network block device connected a server which uses multiple disks via an MD software RAID device, but your nbd server becomes a single point of failure.   In theory at least an enterprise storage array has many more redundancies and can survive a controller card or hard drive failure, even though the enterprise storage array itself is still a single point of failure.</P>

<P>There are other types of cluster file systems, such as Google's GFS, Hadoopfs (which is basically a copy of GFS as described in the GFS paper), Lustre, Ceph, etc.  These generally use an object-based storage paradigm with the objects replicated across multiple object stores, so you can survive a single disk or a single server biting the dust.</P>

<p>But what was being discussed in the original post was really focused on the two cluster file systems currently in the kernel which are supported by enterprise distributions: GFS2 and OCFS2, which are really very much identical in terms of feature set, scalability, and design at the 10,000 foot level.   The fact that we have two of them is largely an accident of history, but it's splitting the amount of resources available hack on shared-block cluster file systems, which since they require rather specialized and expensive equipment in order to use them practically, tends to constrain the size of their developer communities.</p>

-- Ted

      
          <div class="CommentReplyButton">
            <form action="/Articles/400360/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor401781"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 24, 2010 21:06 UTC (Tue)
                               by <b>jpnp</b> (guest, #63341)
                              [<a href="/Articles/401781/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>There are other possibilities, such as a network block device connected a server which uses multiple disks via an MD software RAID device, but your nbd server becomes a single point of failure. In theory at least an enterprise storage array has many more redundancies and can survive a controller card or hard drive failure, even though the enterprise storage array itself is still a single point of failure.</blockquote>

<p>Probably the most relevant option outside of enterprise SAN devices is DRBD.  It's not mainline yet, but is quite widely used by smaller setups, offers no single point of failure, and provides a good substrate to run clustered file systems such as GFS/OCFS2, at least in 2 node systems.</p>

John
      
          <div class="CommentReplyButton">
            <form action="/Articles/401781/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor401786"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 2</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 24, 2010 21:35 UTC (Tue)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/401786/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
drbd went into the mainline in 2.6.33<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/401786/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor400145"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">storage-attached SSDs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2010 23:32 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/400145/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
For "storage-attached" SSDs (those which look like traditional disks)
</blockquote>
<p>
There isn't any other kind of SSD.  SSD specifically refers to devices that stand in for disk drives but use solid state memory.  (The name comes from "Solid State Disk" or "Solid State Drive").
<p>
If it doesn't have an ATA or SCSI interface, it's just memory.

      
          <div class="CommentReplyButton">
            <form action="/Articles/400145/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor400153"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">storage-attached SSDs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 14, 2010 0:09 UTC (Sat)
                               by <b>corbet</b> (editor, #1)
                              [<a href="/Articles/400153/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      A PCI-attached SSD is very much not memory; you can't map it.  You have to do block transfers to it, almost as if it were a block device; it just doesn't try to look like a traditional ATA device.  If you have an acronym you prefer, please use it, but people generally call such devices SSDs.
      
          <div class="CommentReplyButton">
            <form action="/Articles/400153/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor400166"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">storage-attached SSDs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 14, 2010 2:08 UTC (Sat)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/400166/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <blockquote>
A PCI-attached SSD is very much not memory;
</blockquote>
Right, I didn't mean memory like a computer's main memory.  I meant like electronics that remembers.
<p>
<blockquote>
If you have an
acronym you prefer, please use it, but people generally call such
devices SSDs.
</blockquote>
<p>
I don't believe people do generally call that setup SSD.  I encounter solid state storage a lot in my job and though I do sometimes see people use the term that way, they usually get corrected by someone who points out they'll cause unnecessary confusion, since the issues surrounding solid state emulated disk drives are so much different from those surrounding other deployments of persistent electronic memory.
<p>
In fact, now that I think of it, I'm pretty sure SSD implies not only ATA etc electronic interface, but a form factor that lets you plug one into a frame intended for a disk drive.  E.g. the CompactFlash "memory card" you put in a camera is not called an SSD, but has a SATA interface at the electronic level.

      
          <div class="CommentReplyButton">
            <form action="/Articles/400166/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor400146"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMAPI</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2010 23:48 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/400146/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
There was a brief discussion about the DMAPI specification, which describes an API to be used to control hierarchical storage managers.
</blockquote>
<p>
DMAPI isn't an API; it's a class of APIs -- ones that contain certain broad data management functions you need for a hierarchical storage manager.   For example, "we're designing a simple DMAPI that has only 3 functions."  The API almost everyone calls "DMAPI" is really called XDSF.
<p>
It's sort of like IDE is usually used as a misnomer for ATA.

      
          <div class="CommentReplyButton">
            <form action="/Articles/400146/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor400508"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMAPI</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 17, 2010 4:24 UTC (Tue)
                               by <b>jone</b> (guest, #62596)
                              [<a href="/Articles/400508/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
erm .. i think you mean XDSM which is a specification for Data Management (DM) applications which also specified an API (XDSF is the security framework) .. but, you're right - DMAPI was really the interface to the specification that defines multiple APIs to deal with DM .. IBM and SGI really latched onto this since it meant that they didn't have to rewrite their global filesystems (GPFS, [C]XFS) to deal with EAs, offline/partials/etc, region mgmt and auth, etc ..<br>
<p>
the big problem with DMAPI is that it simply doesn't scale .. take a look at the NERSC/LBNL paper for some hints at some of the architectural issues .. metadata scan performance is ultimately key, and the ability to abstract this away when necessary can be quite valuable<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/400508/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor401698"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">&quot;Highwater mark&quot; versus &quot;high watermark&quot;</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 24, 2010 15:44 UTC (Tue)
                               by <b>daniel</b> (guest, #3181)
                              [<a href="/Articles/401698/">Link</a>] 
      </p>
      
      </div>
      </summary>
      Can we clear up this malapropism please?  There is a difference between <a href=http://en.wiktionary.org/wiki/watermark>watermark</a> as in a small dot on a dollar and "<a href=http://en.wiktionary.org/wiki/highwater>highwater</a> mark" in the sense of the highest point reached by the tide.  The kernel does not track a high watermark, it tracks a highwater mark.  Kernel commentary conflates these words in its typical lovable way, but there is no good reason for our esteemed editor to join in the perpetration of this outrageous assault on linguistic sensibilities.
      
          <div class="CommentReplyButton">
            <form action="/Articles/401698/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor401910"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">One reason to use O_DIRECT</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 25, 2010 16:33 UTC (Wed)
                               by <b>bhaskar</b> (guest, #69531)
                              [<a href="/Articles/401910/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The journal (log) file of a database is a write-only file under normal operation.  It needs to be read from only during recovery.  Using O_DIRECT with a decent IO subsystem means that more the file buffer cache is available for the database file itself, where it helps since databases are read and written.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/401910/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2010, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
