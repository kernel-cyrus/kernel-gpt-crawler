        <!DOCTYPE html>
        <html lang="en">
        <head><title>HMM and CDM [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/717601/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/717387/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/717601/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>HMM and CDM</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>This article brought to you by LWN subscribers</b>
<p>
Subscribers to LWN.net made this article &mdash; and everything that
       surrounds it &mdash; possible.  If you appreciate our content, please
       <a href="/Promo/nst-nag3/subscribe">buy a subscription</a> and make the next
       set of articles possible.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>March 22, 2017</br>
           <hr>
<a href="/Articles/lsfmm2017/">LSFMM 2017</a>
</div>
The first topic in the memory-management track of the 2017 Linux Storage,
Filesystem, and Memory-Management Summit continued <a
href="/Articles/717614/">a discussion</a>
begun during the preceding plenary session, which had introduced
the heterogeneous memory management (HMM) issue and updated the group on the
status of those patches.  In this session, Jérôme Glisse focused the
discussion on what needs to be done to move this work forward.  Balbir
Singh then followed up with a different approach to the HMM problem where
more hardware support is available.
<p>
<h4>Pushing HMM forward</h4>
<p>
The HMM discussion started with a question from the group: what features
does a device like a GPU need to be able to support HMM?  The answer is
that it needs some sort of a page-table structure that can be used to set
the access permissions on each page of memory.  That enables, for example,
execute permissions to be set on either the CPU or the GPU (but not on
both), depending on 
which kind of code is found in the relevant pages.  HMM
also needs to be able to prevent simultaneous writes by both processors, so
the GPU needs to be able to handle faults.
<p>
Dave Hansen asked whether more was needed than an I/O memory-management
unit (IOMMU) could provide.  Glisse responded that the IOMMU is there
primarily to protect the system from I/O devices, which is a different use
case.  Mel Gorman added that HMM needs to be able to trap write faults on a
specific page and provide different protections on each side — things an
IOMMU cannot do.
<p>
There is work underway to use the <a href="/Articles/330589/">KSM</a>
mechanism to do write protection; those patches will be posted soon.  KSM
allows the same page to be mapped into multiple address spaces, a feature

<a href="/Articles/717624/"><img
src="https://static.lwn.net/images/conf/2017/lsfmm/JeromeGlisse-sm.jpg" alt="[Jérôme Glisse]"
title="Jérôme Glisse" class="lthumb"></a>

which will be useful, especially on systems with multiple GPUs, all of
which need access to the same data.
<p>
Andrea Arcangeli started a discussion on the handling of write faults.
Normally such faults on shared pages lead to copy-on-write (COW)
operations, but the situation here is different; the response in the HMM
setting is to ensure that the writing side has exclusive access to the
memory in question.  Gorman raised some worries about the semantics of
writes after <tt>fork()</tt> calls by processes using HMM.  <tt>fork()</tt> works
by marking writable pages for COW, but it's not clear what should happen if
the pages are COW-mapped into both the parent and child; a write fault
could end up giving ownership of the page to one process or the other in a
timing-dependent (i.e. racy) manner.
<p>
To avoid this eventuality, Gorman suggested that all memory used with HMM
should be marked as <tt>MADV_DONTFORK</tt> using the <tt>madvise()</tt>
system call; that would cause that memory to not be made available to the
child of a <tt>fork()</tt> at all.  Indeed, he said that it should be
mandatory.  He relented a bit, though, after it was explained that all HMM
memory is pulled into the parent process at <tt>fork()</tt> time, with none
left in the GPU.  He was willing to accept the situation as long as it is
clear that the HMM memory is associated with the parent and is not visible
to the child.
<p>
With that resolved, Gorman asked if there were any remaining obstacles to
merging.  Hansen mentioned that HMM will not work with systems that already
have the maximum amount of memory installed; there simply is no physical
address space for the GPU memory.  Gorman replied that this problem will
indeed come up in practice and users will be burned by it, but that it is a
limitation of the hardware and is not a reason to block the merging of the
HMM patches.
<p>
Dan Williams expressed a concern that the HMM patches place GPU memory into
the <tt>ZONE_DEVICE</tt> zone, which is also used for persistent memory.
The two uses are distinct and can get along, but the code around
<tt>ZONE_DEVICE</tt> becomes that much easier to break if a developer
making a change doesn't
understand all of the users.  Gorman suggested that Williams should do a
detailed review of the HMM code from a <tt>ZONE_DEVICE</tt> perspective;
the long-term maintainability of this code is a fundamental issue, he said,
and needs to be considered carefully.  Johannes Weiner suggested, in jest,
that <tt>ZONE_HIGHMEM</tt> could be used instead, to which Gorman told him
to "go home".
<p>
The final concern is the lack of any drivers for the HMM code; if it is
merged in its current form, it will be dead code with no users.  There is,
it seems, some hope that a Nouveau-based driver for NVIDIA GPUs will be
available by the time the 4.12 merge window opens.  Gorman suggested to
Andrew Morton that the HMM code could be kept in the -mm tree until at
least one driver becomes available, but Morton asked whether it would
really be a problem for the code to go upstream as-is.  What he would most
like, instead, is a solid explanation of what the code is for so he can
justify it to Linus Torvalds when the time comes.
<p>
The end result is that HMM has a few hurdles to get over still, but its
path into the mainline is beginning to look a little more clear.
<p>
<h4>Coherent device memory nodes</h4>
<p>
Singh then stepped forward to describe the IBM view of HMM; for IBM, the
problem has been mostly solved in hardware.  On suitably equipped systems,
the device 
memory shows up as if it were on its own NUMA node that happens to lack a
CPU.  That memory is entirely cache-coherent with the rest of the system,

<a href="/Articles/717624/"><img
src="https://static.lwn.net/images/conf/2017/lsfmm/BalbirSingh-sm.jpg" alt="[Balbir Singh]"
title="Balbir Singh" class="rthumb"></a>

though.  There is <a href="/Articles/713035/">a patch series</a> under
development to support these "coherent device memory nodes" (or CDMs) on Linux.
<p>
There are still a number of questions about how such hardware should work
with Linux.  The desire is to provide selective memory allocation:
user-space applications could choose whether memory should be allocated in
normal or CDM memory.  Reclaim needs to be handled carefully, though, since
the kernel may not have a full view of how the memory on the CDM is being
used.  For obvious reasons, normal NUMA balancing needs to be disabled, or
pages will be move into and out of the CDM incorrectly.  When migrations
are desired, they should be accelerated using DMA engines.
<p>
The plan for CDM memory is to allocate it on the CPU, but then to run
software using that memory on the CDM's processor.  The device is able to
access its own memory and normal system memory transparently via pointers.
The hope is to migrate memory to the most appropriate node based on the
observed usage patterns.  Hansen noted that the NUMA balancing code in the
kernel works fairly well, but most people still turn it off; will there
really be a call for it in this setting?  Singh responded that it can make
a big difference; hints from the application can also help.
<p>
Thus far, the patches include support for isolating the CDMs using the
cpuset mechanism.  But the system doesn't have enough information to do
memory balancing properly yet.  The zone lists have been split to separate out
CDM memory; that serves to hide it from most of the system and avoid
confusion with regular memory.  At the end of the session, transparent
hugepage migration was raised as another missing piece, but that topic was
deferred for later discussion.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Heterogeneous_memory_management">Memory management/Heterogeneous memory management</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2017">Storage, Filesystem, and Memory-Management Summit/2017</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/717601/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2017, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
