        <!DOCTYPE html>
        <html lang="en">
        <head><title>Hierarchical RCU [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/305782/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/305172/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/305782/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Hierarchical RCU</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</blockquote>
<div class="GAByline">
           <p>November 4, 2008</p>
           <p>This article was contributed by Paul McKenney</p>
           </div>
<h3>Introduction</h3>

<p>Read-copy update (RCU) is a synchronization mechanism that was added to
the Linux kernel in October of 2002.
RCU improves scalability
by allowing readers to execute concurrently with writers.
In contrast, conventional locking primitives require that readers
wait for ongoing writers and vice versa.
RCU ensures coherence for read accesses by
maintaining multiple versions of data structures and ensuring that they are not
freed until all pre-existing read-side critical sections complete.
RCU relies on efficient and scalable mechanisms for publishing
and reading new versions of an object, and also for deferring the collection
of old versions.
These mechanisms distribute the work among read and
update paths in such a way as to make read paths extremely fast. In some
cases (non-preemptable kernels), RCU's read-side primitives have zero
overhead.

<P>Although Classic RCU's read-side primitives enjoy excellent
performance and scalability, the update-side primitives which
determine when pre-existing read-side critical sections have
finished, were designed with only a few tens of CPUs in mind.
Their scalability is limited by a global lock that must be
acquired by each CPU at least once during each grace period.
Although Classic RCU actually scales to a couple of hundred CPUs, and
can be tweaked to scale to roughly a thousand CPUs (but at the expense of
extending grace periods), emerging multicore systems will require
it to scale better.

<P>In addition, Classic RCU has a sub-optimal dynticks interface,
with the result that Classic RCU will wake up every CPU at least
once per grace period.
To see the problem with this, consider a 16-CPU system that
is sufficiently lightly loaded that it is keeping only four
CPUs busy.
In a perfect world, the remaining twelve CPUs could be put into
deep sleep mode in order to conserve energy.
Unfortunately, if the four busy CPUs are frequently performing
RCU updates, those twelve idle CPUs will be awakened frequently,
wasting significant energy.
Thus, any major change to Classic RCU should also leave sleeping CPUs lie.

<P>Both the existing and the
<a href="http://lkml.org/lkml/2008/10/10/291">proposed implementation</a>
have have Classic RCU semantics and identical APIs, however,
the old implementation will be called &ldquo;classic RCU&rdquo;
and the new implementation will be called &ldquo;tree RCU&rdquo;.

</p><ol>
<li>	<a href="#Review of RCU Fundamentals">
	Review of RCU Fundamentals</a></li>
<li>	<a href="#Brief Overview of Classic RCU Implementation">
	Brief Overview of Classic RCU Implementation</a></li>
<li>	<a href="#RCU Desiderata">RCU Desiderata</a></li>
<li>	<a href="#Towards a More Scalable RCU Implementation">
	Towards a More Scalable RCU Implementation</a></li>
<li>	<a href="#Towards a Greener RCU Implementation">
	Towards a Greener RCU Implementation</a></li>
<li>	<a href="#State Machine">State Machine</a></li>
<li>	<a href="#Use Cases">Use Cases</a></li>
<li>	<a href="#Testing">Testing</a></li>
</ol>

<p>These sections are followed by
<a href="#Conclusion">concluding remarks</a> and the
<a href="#Answers%20to%20Quick%20Quizzes">answers to the Quick Quizzes</a>.

</p><h3><a name="Review of RCU Fundamentals">
Review of RCU Fundamentals</a></h3>

<P>In its most basic form, RCU is a way of waiting for things to finish.
Of course, there are a great many other ways of waiting for things to
finish, including reference counts, reader-writer locks, events, and so on.
The great advantage of RCU is that it can wait for each of
(say) 20,000 different things without having to explicitly
track each and every one of them, and without having to worry about
the performance degradation, scalability limitations, complex deadlock
scenarios, and memory-leak hazards that are inherent in schemes
using explicit tracking.

<P>In RCU's case, the things waited on are called
"RCU read-side critical sections".
An RCU read-side critical section starts with an
<code>rcu_read_lock()</code> primitive, and ends with a corresponding
<code>rcu_read_unlock()</code> primitive.
RCU read-side critical sections can be nested, and may contain pretty
much any code, as long as that code does not explicitly block or sleep
(although a special form of RCU called
"<A HREF="http://lwn.net/Articles/202847/">SRCU</A>"
does permit general sleeping in SRCU read-side critical sections).
If you abide by these conventions, you can use RCU to wait for <I>any</I>
desired piece of code to complete.

<P>RCU accomplishes this feat by indirectly determining when these
other things have finished, as has been described elsewhere for
<A HREF="http://www.rdrop.com/users/paulmck/RCU/whatisRCU.html">
Classic RCU</A> and
<A HREF="http://lwn.net/Articles/253651/">realtime RCU</A>.

<P>In particular, as shown in the following figure, RCU is a way of
waiting for pre-existing RCU read-side critical sections to completely
finish, including memory operations executed by those critical sections.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/GracePeriodGood.png" WIDTH="525"
height=261 ALT="Grace periods extend to contain pre-existing RCU read-side critical sections.">
</blockquote>

<P>However, note that RCU read-side critical sections
that begin after the beginning
of a given grace period can and will extend beyond the end of that grace
period.

<P>The following section gives a very high-level view of how
the Classic RCU implementation operates.

</p><h3><a name="Brief Overview of Classic RCU Implementation">
Brief Overview of Classic RCU Implementation</a></h3>

The key concept behind the Classic RCU implementation is that
Classic RCU read-side critical sections are confined to kernel
code and are not permitted to block.
This means that any time a given CPU is seen
either blocking, in the idle loop, or exiting the kernel, we know that all
RCU read-side critical sections that were previously running on
that CPU must have completed.
Such states are called &ldquo;quiescent states&rdquo;, and
after each CPU has passed through at least one quiescent state,
the RCU grace period ends.

<IMG src="https://static.lwn.net/images/ns/kernel/hrcu/FlatClassicRCU.png" 
HEIGHT="163" WIDTH="243" ALT="Schematic of Classic RCU" align="right"
hspace=3>

<P>Classic RCU's most important data structure is the <code>rcu_ctrlblk</code>
structure, which contains the <code>-&gt;cpumask</code> field, which contains
one bit per CPU.
Each CPU's bit is set to one at the beginning of each grace period,
and each CPU must clear its bit after it passes through a quiescent
state.
Because multiple CPUs might want to clear their bits concurrently,
which would corrupt the <code>-&gt;cpumask</code> field, a
<code>-&gt;lock</code> 
spinlock is used to protect <code>-&gt;cpumask</code>, preventing any
such corruption.
Unfortunately, this spinlock can also suffer extreme contention if there
are more than a few hundred CPUs, which might soon become quite common
if multicore trends continue.
Worse yet, the fact that <i>all</i> CPUs must clear their own bit means
that CPUs are not permitted to sleep through a grace period, which limits
Linux's ability to conserve power.

<P>


<P>The next section lays out what we need from a new non-real-time
RCU implementation.

</p><h3><a name="RCU Desiderata">RCU Desiderata</a></h3>

The list of RCU desiderata called out at LCA2005 for
<a href="http://www.rdrop.com/users/paulmck/RCU/realtimeRCU.2005.04.23a.pdf">
real-time RCU</a> is a very good start:

<OL>
<LI>	Deferred destruction, so that an RCU grace period cannot end
	until all pre-existing RCU read-side critical sections have
	completed.
<LI>	Reliable, so that RCU supports 24x7 operation for years at
	a time.
<LI>	Callable from irq handlers.
<LI>	Contained memory footprint, so that mechanisms exist to expedite
	grace periods if there are too many callbacks.  (This is weakened
	from the LCA2005 list.)
<LI>	Independent of memory blocks, so that RCU can work with any
	conceivable memory allocator.
<LI>	Synchronization-free read side, so that only normal non-atomic
	instructions operating on CPU- or task-local memory are permitted.
	(This is strengthened from the LCA2005 list.)
<LI>	Unconditional read-to-write upgrade, which is used in several
	places in the Linux kernel where the update-side lock is
	acquired within the RCU read-side critical section.
<LI>	Compatible API.
</OL>

<P>Because this is not to be a real-time RCU, the requirement for
preemptable RCU read-side critical sections can be dropped.
However, we need to add a few more requirements to account for changes
over the past few years:

<OL>
<LI>	Scalability with extremely low internal-to-RCU lock contention.
	RCU must support at least 1,024 CPUs gracefully, and preferably
	at least 4,096.
<LI>	Energy conservation: RCU must be able to avoid awakening
	low-power-state dynticks-idle CPUs, but still determine
	when the current grace period ends.
	This has been implemented in real-time RCU, but needs serious
	simplification.
<LI>	RCU read-side critical sections must be permitted in NMI
	handlers as well as irq handlers.  Note that preemptable RCU
	was able to avoid this requirement due to a separately
	implemented <code>synchronize_sched()</code>.
<LI>	RCU must operate gracefully in face of repeated CPU-hotplug
	operations.
	This is simply carrying forward a requirement met by both
	classic and real-time.
<LI>	It must be possible to wait for all previously registered
	RCU callbacks to complete, though this is already provided
	in the form of <code>rcu_barrier()</code>.
<LI>	Detecting CPUs that are failing to respond is desirable,
	to assist diagnosis both of RCU and of various infinite
	loop bugs and hardware failures that can prevent RCU grace
	periods from ending.
<LI>	Extreme expediting of RCU grace periods is desirable,
	so that an RCU grace period can be forced to complete within
	a few hundred microseconds of the last relevant RCU read-side
	critical second completing.
	However, such an operation would be expected to incur
	severe CPU overhead, and would be primarily useful when
	carrying out a long sequence of operations that each needed
	to wait for an RCU grace period.
</OL>

<P>The most pressing of the new requirements is the first one, scalability.
The next section therefore describes how to make order-of-magnitude reductions
in contention on RCU's internal locks.

</p><h3><a name="Towards a More Scalable RCU Implementation">
Towards a More Scalable RCU Implementation</a></h3>

<P>One effective way to reduce lock contention is to create a hierarchy,
as shown in the following figure.
Here, each of the four <code>rcu_node</code> structures has its own lock,
so that only CPUs&nbsp;0 and 1 will acquire the lower left
<code>rcu_node</code>'s lock, only CPUs&nbsp;2 and 3 will acquire the
lower middle <code>rcu_node</code>'s lock, and only CPUs&nbsp;4 and 5
will acquire the lower right <code>rcu_node</code>'s lock.
During any given grace period,
only one of the CPUs accessing each of the lower <code>rcu_node</code>
structures will access the upper <code>rcu_node</code>, namely, the
last of each pair of CPUs to record a quiescent state for the corresponding
grace period.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/TreeClassicRCU.png" HEIGHT="213"
WIDTH="243" ALT="Schematic of Tree RCU"> 
</blockquote>

<P>This results in a significant reduction in lock contention:
instead of six CPUs contending for a single lock each grace period,
we have only three for the upper <code>rcu_node</code>'s lock 
(a reduction of 50%) and only
two for each of the lower <code>rcu_node</code>s' locks (a reduction
of 67%).

<P>The tree of <code>rcu_node</code> structures is embedded into
a linear array in the <code>rcu_state</code> structure,
with the root of the tree in element zero, as shown below for an eight-CPU
system with a three-level hierarchy.
The arrows link a given <code>rcu_node</code> structure to its parent.
Each <code>rcu_node</code> indicates the range of CPUs covered,
so that the root node covers all of the CPUs, each node in the second
level covers half of the CPUs, and each node in the leaf level covering
a pair of CPUs.
This array is allocated statically at compile time based on the value
of <code>NR_CPUS</code>.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/TreeMapping.png" HEIGHT="143" WIDTH="493" ALT="Embedding of Tree RCU">
</blockquote>

<P>The following sequence of six figures shows how grace periods are detected.
In the first figure, no CPU has yet passed through a quiescent state,
as indicated by the red rectangles.
Suppose that all six CPUs simultaneously try to tell RCU that they have
passed through a quiescent state.
Only one of each pair will be able to acquire the lock on the
corresponding lower <code>rcu_node</code>, and so the second figure
shows the result if the lucky CPUs are numbers 0, 3, and 5, as indicated
by the green rectangles.
Once these lucky CPUs have finished, then the other CPUs will acquire
the lock, as shown in the third figure.
Each of these CPUs will see that they are the last in their group,
and therefore all three will attempt to move to the upper
<code>rcu_node</code>.
Only one at a time can acquire the upper <code>rcu_node</code> structure's
lock, and the fourth, fifth, and sixth figures show the sequence of
states assuming that CPU&nbsp;1, CPU&nbsp;2, and CPU&nbsp;4 acquire
the lock in that order.
The sixth and final figure in the group shows that all CPUs have passed
through a quiescent state, so that the grace period has ended.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/TreeClassicRCUGP.png" HEIGHT="285" WIDTH="359" ALT="Schematic of Tree RCU">
</blockquote>

<P>In the above sequence, there were never more than three CPUs
contending for any one lock, in happy contrast to Classic RCU,
where all six CPUs might contend.
However, even more dramatic reductions in lock contention are
possible with larger numbers of CPUs.
Consider a hierarchy of <code>rcu_node</code> structures, with
64 lower structures and 64*64=4,096 CPUs, as shown in the following figure.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/BigTreeClassicRCU.png" HEIGHT="316" WIDTH="353" ALT="Schematic of Tree RCU">
</blockquote>

<P>Here each of the lower <code>rcu_node</code> structures' locks
are acquired by 64 CPUs, a 64-times reduction from the 4,096 CPUs
that would acquire Classic RCU's single global lock.
Similarly, during a given grace period, only one CPU from each of
the lower <code>rcu_node</code> structures will acquire the
upper <code>rcu_node</code> structure's lock, which is again
a 64x reduction from the contention level that would be experienced
by Classic RCU running on a 4,096-CPU system.

</p><p><a name="Quick Quiz 1"><b>Quick Quiz 1</b>:</a>
Wait a minute!  With all those new locks, how do you avoid deadlock?

</p><p><a name="Quick Quiz 2"><b>Quick Quiz 2</b>:</a>
Why stop at a 64-times reduction?
Why not go for a few orders of magnitude instead?

</p><p><a name="Quick Quiz 3"><b>Quick Quiz 3</b>:</a>
But I don't care about McKenney's lame excuses in the answer to
Quick Quiz 2!!!
I want to get the number of CPUs contending on a single lock down
to something reasonable, like sixteen or so!!!

<P>The implementation maintains some per-CPU data, such as lists of
RCU callbacks, organized into <code>rcu_data</code> structures.
In addition, rcu (as in <code>call_rcu()</code>) and
rcu_bh (as in <code>call_rcu_bh()</code>) each maintain their own
hierarchy, as shown in the following figure.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/BigTreeClassicRCUBH.png" HEIGHT="263" WIDTH="228" ALT="Schematic of Tree RCU">
</blockquote>

<p>

</p><p><a name="Quick Quiz 4"><b>Quick Quiz 4</b>:</a>
OK, so what is the story with the colors?

<P>The next section discusses energy conservation.

</p><h3><a name="Towards a Greener RCU Implementation">
Towards a Greener RCU Implementation</a></h3>

<p>As noted earlier, an important goal of this effort is to leave sleeping
CPUs lie in order to promote energy conservation.
In contrast, classic RCU will happily awaken each and every sleeping CPU
at least once per grace period in some cases,
which is suboptimal in the case where
a small number of CPUs are busy doing RCU updates and the majority of
the CPUs are mostly idle.
This situation occurs frequently in systems sized for peak loads, and
we need to be able to accommodate it gracefully.
Furthermore, we need to fix a long-standing bug in Classic RCU where
a dynticks-idle CPU servicing an interrupt containing a long-running
RCU read-side critical section will fail to prevent an RCU grace period
from ending.

</p><p><a name="Quick Quiz 5"><b>Quick Quiz 5</b>:</a>
Given such an egregious bug, why does Linux run at all?

<P>This is accomplished by requiring that all CPUs manipulate counters
located in a per-CPU <code>rcu_dynticks</code> structure.
Loosely speaking, these counters have even-numbered values when the
corresponding CPU is in dynticks idle mode, and have odd-numbered values
otherwise.
RCU thus needs to wait for quiescent states only for those CPUs whose
<code>rcu_dynticks</code> counters are odd, and need not wake up sleeping
CPUs, whose counters will be even.
As shown in the following diagram, each per-CPU <code>rcu_dynticks</code>
is shared by the &ldquo;rcu&rdquo; and &ldquo;rcu_bh&rdquo; implementations.

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/BigTreeClassicRCUBHdyntick.png" HEIGHT="426" WIDTH="289" ALT="Schematic of Tree RCU">
</blockquote>

<p>The following section presents a high-level view of the RCU state machine.

</p><h3><a name="State Machine">State Machine</a></h3>

<p>At a sufficiently high level, Linux-kernel RCU implementations can
be thought of as high-level state machines as shown in the following
schematic:

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/GenericRCUStateMachine.png" WIDTH="256"
ALT="Tree-RCU classic view of state machine." height=587>
</blockquote>

<p>The common-case path through this state machine on a busy system
goes through the two uppermost loops, initializing at the
beginning of each grace period (GP),
waiting for quiescent states (QS), and noting when each CPU passes through
its first quiescent state for a given grace period.
On such a system, quiescent states will occur on each context switch,
or, for CPUs that are either idle or executing user-mode code, each
scheduling-clock interrupt.
CPU-hotplug events will take the state machine through the
&ldquo;CPU Offline&rdquo; box, while the presence of &ldquo;holdout&rdquo;
CPUs that fail to pass through quiescent states quickly enough will exercise
the path through the &ldquo;Send resched IPIs to Holdout CPUs&rdquo; box.
RCU implementations that avoid unnecessarily awakening dyntick-idle
CPUs will mark those CPUs as being in an extended quiescent state,
taking the &ldquo;Y&rdquo; branch out of the &ldquo;CPUs in dyntick-idle
Mode?&rdquo; decision diamond (but note that CPUs in dyntick-idle mode
will <i>not</i> be sent resched IPIs).
Finally, if <code>CONFIG_RCU_CPU_STALL_DETECTOR</code> is enabled,
truly excessive delays in reaching quiescent states will exercise the
&ldquo;Complain About Holdout CPUs&rdquo; path.

<p>The events in the above state schematic interact with different
data structures, as shown below:

<P><blockquote>
<P><IMG src="https://static.lwn.net/images/ns/kernel/hrcu/TreeRCUStateMachine.png" WIDTH="366"
height=587  ALT="Tree-RCU data-structure-oriented view of state machine.">
</blockquote>

<p>However, the state schematic does not directly translate into C code
for any of the RCU implementations.
Instead, these implementations are coded as an event-driven system within
the kernel.
Therefore, the following section describes some &ldquo;use cases&rdquo;,
or ways in which the RCU algorithm traverses the above state schematic
as well as the relevant data structures.

</p><h3><a name="Use Cases">
Use Cases</a></h3>

<p>This section gives an overview of several &ldquo;use cases&rdquo;
within the RCU implementation, listing the data structures touched
and the functions invoked.
The use cases are as follows:

</p>
<ol>
<li>	<a href="#Start a new grace period.">
	Start a new grace period.</a></li>
<li>	<a href="#Pass through a quiescent state.">
	Pass through a quiescent state.</a></li>
<li>	<a href="#Announce a quiescent state to RCU.">
	Announce a quiescent state to RCU.</a></li>
<li>	<a href="#Enter and leave dynticks idle mode.">
	Enter and leave dynticks idle mode.</a></li>
<li>	<a href="#Interrupt from dynticks idle mode.">
	Interrupt from dynticks idle mode.</a></li>
<li>	<a href="#NMI from dynticks idle mode.">
	NMI from dynticks idle mode.</a></li>
<li>	<a href="#Note that a CPU is in dynticks idle mode.">
	Note that a CPU is in dynticks idle mode.</a></li>
<li>	<a href="#Offline a CPU.">
	Offline a CPU.</a></li>
<li>	<a href="#Online a CPU.">
	Online a CPU.</a></li>
<li>	<a href="#Detect a too-long grace period.">
	Detect a too-long grace period.</a></li>
</ol>

<p>Each of these use cases is described in the following sections.

</p><h4><a name="Start a new grace period.">
Start a New Grace Period</a></h4>

<p>The <code>rcu_start_gp()</code> function starts a new grace period.
This function is invoked when a CPU having callbacks waiting for a
grace period notices that no grace period is in progress.

<p>The <code>rcu_start_gp()</code> function updates state in
the <code>rcu_state</code> and <code>rcu_data</code> structures
to note the newly started grace period,
acquires the <code>-&gt;onoff</code> lock (and disables irqs) to exclude
any concurrent CPU-hotplug operations,
sets the
bits in all of the <code>rcu_node</code> structures to indicate
that all CPUs (including this one) must pass through a quiescent
state,
and finally
releases the <code>-&gt;onoff</code> lock.

<p>The bit-setting operation is carried out in two phases.
First, the non-leaf <code>rcu_node</code> structures' bits are set without
holding any additional locks, and then finally each leaf <code>rcu_node</code>
structure's bits are set in turn while holding that structure's
<code>-&gt;lock</code>.

</p><p><a name="Quick Quiz 6"><b>Quick Quiz 6</b>:</a>
But what happens if a CPU tries to report going through a quiescent
state (by clearing its bit) before the bit-setting CPU has finished?

</p><p><a name="Quick Quiz 7"><b>Quick Quiz 7</b>:</a>
And what happens if <i>all</i> CPUs try to report going through a quiescent
state before the bit-setting CPU has finished, thus ending the new
grace period before it starts?

</p><h4><a name="Pass through a quiescent state.">
Pass Through a Quiescent State</a></h4>

<p>The rcu and rcu_bh flavors of RCU have different sets of quiescent
states.
Quiescent states for rcu are context switch, idle (either dynticks or
the idle loop), and user-mode execution, while quiescent states for
rcu_bh are any code outside of softirq with interrupts enabled.
Note that an quiescent state for rcu is also a quiescent state
for rcu_bh.
Quiescent states for rcu are recorded by invoking <code>rcu_qsctr_inc()</code>,
while quiescent states for rcu_bh are recorded by invoking
<code>rcu_bh_qsctr_inc()</code>.
These two functions record their state in the current CPU's
<code>rcu_data</code> structure.

<p>These functions are invoked from the scheduler, from
<code>__do_softirq()</code>, and from <code>rcu_check_callbacks()</code>.
This latter function is invoked from the scheduling-clock interrupt,
and analyzes state to determine whether this interrupt occurred within
a quiescent state, invoking <code>rcu_qsctr_inc()</code> and/or
<code>rcu_bh_qsctr_inc()</code>, as appropriate.
It also raises <code>RCU_SOFTIRQ</code>, which results in
<code>rcu_process_callbacks()</code> being invoked on the current
CPU at some later time from softirq context.

</p><h4><a name="Announce a quiescent state to RCU.">
Announce a Quiescent State to RCU</a></h4>

<p>The afore-mentioned <code>rcu_process_callbacks()</code> function
has several duties:

</p><ol>
<li>	Determining when to take measures to end an over-long grace period
	(via <code>force_quiescent_state()</code>).
<li>	Taking appropriate action when some other CPU detected the end of
	a grace period (via <code>rcu_process_gp_end()</code>).
	&ldquo;Appropriate action&ldquo; includes advancing this CPU's
	callbacks and recording the new grace period.
	This same function updates state in response to some other
	CPU starting a new grace period.
<li>	Reporting the current CPU's quiescent states to the core RCU
	mechanism (via <code>rcu_check_quiescent_state()</code>, which
	in turn invokes <code>cpu_quiet()</code>).
	This of course might mark the end of the current grace period.
<li>	Starting a new grace period if there is no grace period in progress
	and this CPU has RCU callbacks still waiting for a grace period
	(via <code>cpu_needs_another_gp()</code> and
	<code>rcu_start_gp()</code>).
<li>	Invoking any of this CPU's callbacks whose grace period has ended
	(via <code>rcu_do_batch()</code>).
</ol>

<p>These interactions are carefully orchestrated in order to avoid
buggy behavior such as reporting a quiescent state from the previous
grace period against the current grace period.

</p><h4><a name="Enter and leave dynticks idle mode.">
Enter and Leave Dynticks Idle Mode</a></h4>

<p>The scheduler invokes <code>rcu_enter_nohz()</code> to
enter dynticks-idle mode, and invokes <code>rcu_exit_nohz()</code>
to exit it.
The <code>rcu_enter_nohz()</code> function increments a per-CPU
<code>dynticks_nesting</code> variable and
also a per-CPU <code>dynticks</code> counter, the latter of which which must
then have an even-numbered value.
The <code>rcu_exit_nohz()</code> function decrements this same
per-CPU <code>dynticks_nesting</code> variable,
and again increments the per-CPU <code>dynticks</code>
counter, the latter of which must then have an odd-numbered value.

<p>The <code>dynticks</code> counter can be sampled by other CPUs.
If the value is even, the first CPU is in an extended quiescent state.
Similarly, if the counter value changes during a given grace period,
the first CPU must have been in an extended quiescent state at some
point during the grace period.
However, there is another <code>dynticks_nmi</code> per-CPU variable
that must also be sampled, as will be discussed below.

</p><h4><a name="Interrupt from dynticks idle mode.">
Interrupt from Dynticks Idle Mode</a></h4>

<p>Interrupts from dynticks idle mode are handled by
<code>rcu_irq_enter()</code> and <code>rcu_irq_exit()</code>.
The <code>rcu_irq_enter()</code> function increments the
per-CPU <code>dynticks_nesting</code> variable, and, if the prior
value was zero, also increments the <code>dynticks</code>
per-CPU variable (which must then have an odd-numbered value).

<p>The <code>rcu_irq_exit()</code> function decrements the
per-CPU <code>dynticks_nesting</code> variable, and, if the new
value is zero, also increments the <code>dynticks</code>
per-CPU variable (which must then have an even-numbered value).

<p>Note that entering an irq handler exits dynticks idle mode
and vice versa.
This enter/exit anti-correspondence can cause much confusion.
You have been warned.

</p><h4><a name="NMI from dynticks idle mode.">
NMI from Dynticks Idle Mode</a></h4>

<p>NMIs from dynticks idle mode are handled by <code>rcu_nmi_enter()</code>
and <code>rcu_nmi_exit()</code>.
These functions both increment the <code>dynticks_nmi</code> counter,
but only if the aforementioned <code>dynticks</code> counter is even.
In other words, NMI's refrain from manipulating the
<code>dynticks_nmi</code> counter if the NMI occurred in non-dynticks-idle
mode or within an interrupt handler.

<p>The only difference between these two functions is the error checks,
as <code>rcu_nmi_enter()</code> must leave the <code>dynticks_nmi</code>
counter with an odd value, and <code>rcu_nmi_exit()</code> must leave
this counter with an even value.

</p><h4><a name="Note that a CPU is in dynticks idle mode.">
Note That a CPU is in Dynticks Idle Mode</a></h4>

<p>The <code>force_quiescent_state()</code> function implements a
two-phase state machine.
In the first phase (<code>RCU_SAVE_DYNTICK</code>), the
<code>dyntick_save_progress_counter()</code> function scans the CPUs that
have not yet reported a quiescent state, recording their per-CPU
<code>dynticks</code> and <code>dynticks_nmi</code> counters.
If these counters both have even-numbered values, then the corresponding
CPU is in dynticks-idle state, which is therefore noted as an extended
quiescent state (reported via <code>cpu_quiet_msk()</code>).
In the second phase (<code>RCU_FORCE_QS</code>), the
<code>rcu_implicit_dynticks_qs()</code> function again scans the CPUs
that have not yet reported a quiescent state (either explicitly or
implicitly during the <code>RCU_SAVE_DYNTICK</code> phase), again checking the
per-CPU <code>dynticks</code> and <code>dynticks_nmi</code> counters.
If each of these has either changed in value or is now even, then
the corresponding CPU has either passed through or is now in dynticks
idle, which as before is noted as an extended quiescent state.

<p>If <code>rcu_implicit_dynticks_qs()</code> finds that a given CPU
has neither been in dynticks idle mode nor reported a quiescent state,
it invokes <code>rcu_implicit_offline_qs()</code>, which checks to see
if that CPU is offline, which is also reported as an extended quiescent
state.
If the CPU is online, then <code>rcu_implicit_offline_qs()</code> sends
it a reschedule IPI in an attempt to remind it of its duty to report
a quiescent state to RCU.

<p>Note that <code>force_quiescent_state()</code> does not directly
invoke either <code>dyntick_save_progress_counter()</code> or
<code>rcu_implicit_dynticks_qs()</code>, instead passing these functions
to an intervening <code>rcu_process_dyntick()</code> function that
abstracts out the common code involved in scanning the CPUs and reporting
extended quiescent states.

</p><p><a name="Quick Quiz 8"><b>Quick Quiz 8</b>:</a>
And what happens if one CPU comes out of dyntick-idle mode and then
passed through a quiescent state just as another CPU notices that the
first CPU was in dyntick-idle mode?
Couldn't they both attempt to report a quiescent state at the same
time, resulting in confusion?

</p><p><a name="Quick Quiz 9"><b>Quick Quiz 9</b>:</a>
But what if <i>all</i> the CPUs end up in dyntick-idle mode?
Wouldn't that prevent the current RCU grace period from ever ending?

</p><p><a name="Quick Quiz 10"><b>Quick Quiz 10</b>:</a>
Given that <code>force_quiescent_state()</code> is a two-phase state
machine, don't we have double the scheduling latency due to scanning
all the CPUs?

</p><h4><a name="Offline a CPU.">
Offline a CPU</a></h4>

<p>CPU-offline events cause <code>rcu_cpu_notify()</code> to invoke
<code>rcu_offline_cpu()</code>, which in turn invokes
<code>__rcu_offline_cpu()</code> on both the rcu and the rcu_bh
instances of the data structures.
This function clears the outgoing CPU's bits so that future grace
periods will not expect this CPU to announce quiescent states,
and further invokes <code>cpu_quiet()</code> in order to announce
the offline-induced extended quiescent state.
This work is performed with the global <code>-&gt;onofflock</code>
held in order to prevent interference with concurrent grace-period
initialization.

</p><p><a name="Quick Quiz 11"><b>Quick Quiz 11</b>:</a>
But the other reason to hold <code>-&gt;onofflock</code> is to prevent
multiple concurrent online/offline operations, right?

</p><h4><a name="Online a CPU.">
Online a CPU</a></h4>

<p>CPU-online events cause <code>rcu_cpu_notify()</code> to invoke
<code>rcu_online_cpu()</code>, which initializes the incoming CPU's
dynticks state, and then invokes <code>rcu_init_percpu_data()</code>
to initialize the incoming CPU's <code>rcu_data</code> structure,
and also to set this CPU's bits (again protected by
the global <code>-&gt;onofflock</code>) so that future grace periods
will wait for a quiescent state from this CPU.
Finally, <code>rcu_online_cpu()</code>
sets up the RCU softirq vector for this CPU.

</p><p><a name="Quick Quiz 12"><b>Quick Quiz 12</b>:</a>
Given all these acquisitions of the global <code>-&gt;onofflock</code>, won't there
be horrible lock contention when running with thousands of CPUs?

</p><h4><a name="Detect a too-long grace period.">
Detect a Too-Long Grace Period</a></h4>

<p>When the <code>CONFIG_RCU_CPU_STALL_DETECTOR</code> kernel parameter
is specified, the <code>record_gp_stall_check_time()</code> function
records the time and also a timestamp set three seconds into the future.
If the current grace period still has not ended by that time, the
<code>check_cpu_stall()</code> function will check for the culprit,
invoking <code>print_cpu_stall()</code> if the current CPU is the
holdout, or <code>print_other_cpu_stall()</code> if it is some other CPU.
A two-jiffies offset helps ensure that CPUs report on themselves
when possible, taking advantage of the fact that a CPU can normally
do a better job of tracing its own stack than it can tracing some other
CPU's stack.

</p><h3><a name="Testing">Testing</a></h3>

<p>RCU is fundamental synchronization code, so any failure of RCU
results in random, difficult-to-debug memory corruption.
It is therefore extremely important that RCU be <i>highly</i> reliable.
Some of this reliability stems from careful design, but at the
end of the day we must also rely on heavy stress testing, otherwise
known as torture.

<p>Fortunately, although there has been some debate as to exactly
what populations are covered by the provisions of the
<a href="http://www.unhchr.ch/html/menu3/b/91.htm">Geneva Convention</a>,
it is still the case that it does not apply to software.
Therefore, it is still legal to torture your software.
In fact, it is strongly encouraged, because if you don't torture your
software, it will end up torturing <i>you</i> by crashing at the most
inconvenient times imaginable.

<p>Therefore, we torture RCU quite vigorously using the rcutorture module.

<p>However, it is not sufficient to torture the common-case uses of RCU.
It is also necessary to torture it in unusual situations, for example,
when concurrently onlining and offlining CPUs and when CPUs are concurrently
entering and exiting dynticks idle mode.
I use a
<a href="/Articles/305786/">script</a> to online and offline CPUs,
and use the <code>test_no_idle_hz</code> module parameter to rcutorture
to stress-test dynticks idle mode.
Just to be fully paranoid, I sometimes run a kernbench workload in parallel
as well.
Ten hours of this sort of torture on a 128-way machine seems sufficient
to shake out most bugs.

<p>Even this is not the complete story.
As Alexey Dobriyan and Nick Piggin demonstrated in early 2008, it is
also necessary to torture RCU with all relevant combinations of kernel
parameters.
The relevant kernel parameters may be identified using yet another
<a href="/Articles/305787/">script</a>, and are as follows:

</p><ol>
<li>    <code>CONFIG_CLASSIC_RCU</code>: Classic RCU.
<li>    <code>CONFIG_PREEMPT_RCU</code>: Preemptable (real-time) RCU.
<li>    <code>CONFIG_TREE_RCU</code>: Classic RCU for huge SMP systems.
<li>    <code>CONFIG_RCU_FANOUT</code>: Number of children for each
		<code>rcu_node</code>.
<li>    <code>CONFIG_RCU_FANOUT_EXACT</code>: Balance the
		<code>rcu_node</code> tree.
<li>    <code>CONFIG_HOTPLUG_CPU</code>: Allow CPUs to be offlined
		and onlined.
<li>    <code>CONFIG_NO_HZ</code>: Enable dyntick-idle mode.
<li>    <code>CONFIG_SMP</code>: Enable multi-CPU operation.
<li>    <code>CONFIG_RCU_CPU_STALL_DETECTOR</code>: Enable RCU to detect
		when CPUs go on extended quiescent-state vacations.
<li>    <code>CONFIG_RCU_TRACE</code>: Generate RCU trace files in debugfs.
</ol>

<p>We ignore the <code>CONFIG_DEBUG_LOCK_ALLOC</code> configuration
variable under the perhaps-naive assumption that hierarchical RCU
could not have broken lockdep.
There are still 10 configuration variables, which would result in
1,024 combinations if they were independent boolean variables.
Fortunately the first three are mutually exclusive, which reduces
the number of combinations down to 384, but <code>CONFIG_RCU_FANOUT</code>
can take on values from 2 to 64, increasing the number of combinations
to 12,096.
This is an infeasible number of combinations.

<p>One key observation is that only <code>CONFIG_NO_HZ</code>
and <code>CONFIG_PREEMPT</code> can be expected to have changed behavior
if either <code>CONFIG_CLASSIC_RCU</code> or
<code>CONFIG_PREEMPT_RCU</code> are in effect, as only these portions
of the two pre-existing RCU implementations were changed during this effort.
This cuts out almost two thirds of the possible combinations.

<p>Furthermore, not all of the possible values of
<code>CONFIG_RCU_FANOUT</code> produce significantly different results,
in fact only a few cases really need to be tested separately:

<ol>
<li>	Single-node &ldquo;tree&rdquo;.
<li>	Two-level balanced tree.
<li>	Three-level balanced tree.
<li>	Autobalanced tree, where <code>CONFIG_RCU_FANOUT</code>
	specifies an unbalanced tree, but such that it is auto-balanced
	in absence of <code>CONFIG_RCU_FANOUT_EXACT</code>.
<li>	Unbalanced tree.
</ol>

<p>Looking further, <code>CONFIG_HOTPLUG_CPU</code> makes sense only
given <code>CONFIG_SMP</code>, and <code>CONFIG_RCU_CPU_STALL_DETECTOR</code>
is independent, and really only needs to be tested once (though someone
even more paranoid than am I might decide to test it both with
and without <code>CONFIG_SMP</code>).
Similarly, <code>CONFIG_RCU_TRACE</code> need only be tested once,
but the truly paranoid (such as myself) will choose to run it both with
and without <code>CONFIG_NO_HZ</code>.

This allows us to obtain excellent coverage of RCU with only 15
test cases.
All test cases specify the following configuration parameters in order
to run rcutorture and so that <code>CONFIG_HOTPLUG_CPU=n</code> actually
takes effect:

</p><pre>
CONFIG_RCU_TORTURE_TEST=m
CONFIG_MODULE_UNLOAD=y
CONFIG_SUSPEND=n
CONFIG_HIBERNATION=n
</pre>

<p>The 15 test cases are as follows:

</p><ol>
<li>	Force single-node &ldquo;tree&rdquo; for small systems:
	<pre>
	CONFIG_NR_CPUS=8
	CONFIG_RCU_FANOUT=8
	CONFIG_RCU_FANOUT_EXACT=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Force two-level tree for large systems:
	<pre>
	CONFIG_NR_CPUS=8
	CONFIG_RCU_FANOUT=4
	CONFIG_RCU_FANOUT_EXACT=n
	CONFIG_RCU_TRACE=n
	</pre>
<li>	Force three-level tree for huge systems:
	<pre>
	CONFIG_NR_CPUS=8
	CONFIG_RCU_FANOUT=2
	CONFIG_RCU_FANOUT_EXACT=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test autobalancing to a balanced tree:
	<pre>
	CONFIG_NR_CPUS=8
	CONFIG_RCU_FANOUT=6
	CONFIG_RCU_FANOUT_EXACT=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test unbalanced tree:
	<pre>
	CONFIG_NR_CPUS=8
	CONFIG_RCU_FANOUT=6
	CONFIG_RCU_FANOUT_EXACT=y
	CONFIG_RCU_CPU_STALL_DETECTOR=y
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Disable CPU-stall detection:
	<pre>
	CONFIG_SMP=y
	CONFIG_NO_HZ=y
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=y
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Disable CPU-stall detection and dyntick idle mode:
	<pre>
	CONFIG_SMP=y
	CONFIG_NO_HZ=n
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=y
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Disable CPU-stall detection and CPU hotplug:
	<pre>
	CONFIG_SMP=y
	CONFIG_NO_HZ=y
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Disable CPU-stall detection, dyntick idle mode, and CPU hotplug:
	<pre>
	CONFIG_SMP=y
	CONFIG_NO_HZ=n
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Disable SMP, CPU-stall detection, dyntick idle mode, and CPU hotplug:
	<pre>
	CONFIG_SMP=n
	CONFIG_NO_HZ=n
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=n
	CONFIG_RCU_TRACE=y
	</pre>
	<p>This combination located a number of compiler warnings.
<li>	Disable SMP and CPU hotplug:
	<pre>
	CONFIG_SMP=n
	CONFIG_NO_HZ=n
	CONFIG_RCU_CPU_STALL_DETECTOR=n
	CONFIG_HOTPLUG_CPU=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test Classic RCU with dynticks idle but without preemption:
	<pre>
	CONFIG_NO_HZ=y
	CONFIG_PREEMPT=n
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test Classic RCU with preemption but without dynticks idle:
	<pre>
	CONFIG_NO_HZ=n
	CONFIG_PREEMPT=y
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test Preemptable RCU with dynticks idle:
	<pre>
	CONFIG_NO_HZ=y
	CONFIG_PREEMPT=y
	CONFIG_RCU_TRACE=y
	</pre>
<li>	Test Preemptable RCU without dynticks idle:
	<pre>
	CONFIG_NO_HZ=n
	CONFIG_PREEMPT=y
	CONFIG_RCU_TRACE=y
	</pre>
</ol>

<p>For a large change that affects RCU core code, one should run
rcutorture for each of the above combinations, and concurrently
with CPU offlining and onlining for cases with
<code>CONFIG_HOTPLUG_CPU</code>.
For small changes, it may suffice to run kernbench in each case.
Of course, if the change is confined to a particular subset of
the configuration parameters, it may be possible to reduce the
number of test cases.

<p>Torturing software: the Geneva Convention does not (yet) prohibit
it, and I strongly recommend it!!!

</p><h3><a name="Conclusion">Conclusion</a></h3>

<p>This hierarchical implementation of RCU reduces lock contention,
avoids unnecessarily awakening dyntick-idle sleeping CPUs, while
helping to debug Linux's hotplug-CPU code paths.
This implementation is designed to handle single systems with
thousands of CPUs, and on 64-bit systems has an architectural
limitation of a quarter million CPUs, a limit I expect to be
sufficient for at least the next few years.

<p>This RCU implementation of course has some limitations:

<ol>
<li>	The <code>force_quiescent_state()</code> can scan the full
	set of CPUs with irqs disabled.
	This would be fatal in a real-time implementation of RCU,
	so if hierarchy ever needs to be introduced to preemptable
	RCU, some other approach will be required.
	It is possible that it will be problematic on 4,096-CPU
	systems, but actual testing on such systems is required
	to prove this one way or the other.
	<p>
	On busy systems, the <code>force_quiescent_state()</code> scan
	would not be expected to happen,
	as CPUs should pass through quiescent states within three
	jiffies of the start of a quiescent state.  On semi-busy
	systems, only the CPUs in dynticks-idle mode throughout would
	need to be scanned.
	In some cases, for example when a dynticks-idle CPU is handling
	an interrupt during a scan, subsequent scans are required.
	However, each such scan is performed separately, so scheduling
	latency is degraded by the overhead of only one such scan.
	<p>
	If this scan proves problematic, one straightforward solution
	would be to do the scan incrementally.
	This would increase code complexity slightly and would also
	increase the time required to end a grace period, but would
	nonetheless be a likely solution.
	<p>
<li>	The <code>rcu_node</code> hierarchy is created at compile
	time, and is therefore sized for the worst-case <code>NR_CPUS</code>
	number of CPUs.
	However, even for 4,096 CPUs, the <code>rcu_node</code>
	hierarchy consumes only 65 cache lines on a 64-bit machine
	(and just you try accommodating 4,096 CPUs on a 32-bit machine!).
	Of course, a kernel built with <code>NR_CPUS=4096</code>
	running on a 16-CPU machine would use a two-level tree when
	a single-node tree would work just fine.
	Although this configuration would incur added locking overhead,
	this does not affect hot-path read-side code, so should not be a
	problem in practice.
	<p>
<li>	This patch does increase kernel text and data somewhat:
	the old Classic RCU implementation consumes 1,757 bytes of
	kernel text and 456 bytes of kernel data for a total of 2,213 bytes,
	while the new hierarchical RCU implementation consumes 4,006
	bytes of kernel text and 624 bytes of kernel data for a total
	of 4,630 bytes on a <code>NR_CPUS=4</code> system.
	This is a non-problem even for most embedded systems, which
	often come with hundreds of megabytes of main memory.
	However, if this is a problem for tiny embedded systems, it may
	be necessary to provide both &ldquo;scale up&rdquo; and
	&ldquo;scale down&rdquo; implementations of RCU.
</ol>

<p>This hierarchical RCU implementation should nevertheless be a vast
improvement over Classic RCU for machines with hundreds of CPUs.
After all, Classic RCU was designed for systems with only 16-32 CPUs.

<p>At some point, it may be necessary to also apply hierarchy to the
preemptable RCU implementation.
This will be challenging due to the modular arithmetic used on the
per-CPU counter pairs, but should be doable.

</p><h3>Acknowledgements</h3>

<p>I am indebted to Manfred Spraul for ideas, review comments,
bugs spotted, as well as some good healthy competition,
to Josh Triplett, Ingo Molnar, Peter Zijlstra, Mathieu Desnoyers,
Lai Jiangshan, Andi Kleen, Andy Whitcroft, Gautham Shenoy,
and Andrew Morton for review comments,
and to Thomas Gleixner for much help with timer issues.
I am thankful to Jon M. Tollefson, Tim Pepper, Andrew Theurer,
Jose R. Santos, Andy Whitcroft, Darrick Wong, Nishanth Aravamudan, Anton
Blanchard, and Nathan Lynch for keeping machines alive despite
my (ab)use for this project.
We all owe thanks to Peter Zijlstra, Gautham Shenoy, Lai Jiangshan,
and Manfred Spraul for helping (in some cases unwittingly) render
this document at least partially human readable.
Finally, I am grateful to Kathy Bennett for her support of this effort.

</p><p>This work represents the view of the authors and does not necessarily
represent the view of IBM.

</p><p>Linux is a registered trademark of Linus Torvalds.

</p><p>Other company, product, and service names may be trademarks or
service marks of others.

</p><h3><a name="Answers to Quick Quizzes">
Answers to Quick Quizzes</a></h3>

<p><b>Quick Quiz 1</b>:
Wait a minute!  With all those new locks, how do you avoid deadlock?

</p><p><b>Answer</b>:
Deadlock is avoided by never holding more than one of the
<code>rcu_node</code> structures' locks at a given time.
This algorithm uses two more locks, one to prevent CPU hotplug operations
from running concurrently with grace-period advancement
(<code>onofflock</code>) and another
to permit only one CPU at a time from forcing a quiescent state
to end quickly (<code>fqslock</code>).
These are subject to a locking hierarchy, so that
<code>fqslock</code> must be acquired before
<code>onofflock</code>, which in turn must be acquired before
any of the <code>rcu_node</code> structures' locks.

</p><p>Also, as a practical matter, refusing to ever hold more than
one of the <code>rcu_node</code> locks means that it is unnecessary
to track which ones are held.
Such tracking would be painful as well as unnecessary.

</p><p><a href="#Quick%20Quiz%201"><b>Back to Quick Quiz 1</b>.</a>


</p><p><b>Quick Quiz 2</b>:
Why stop at a 64-times reduction?
Why not go for a few orders of magnitude instead?

</p><p><b>Answer</b>: RCU works with no problems on
systems with a few hundred CPUs, so allowing 64 CPUs to contend on
a single lock leaves plenty of headroom.
Keep in mind that these locks are acquired quite rarely, as each
CPU will check in about one time per grace period, and grace periods
extend for milliseconds.

</p><p><a href="#Quick%20Quiz%202"><b>Back to Quick Quiz 2</b>.</a>


</p><p><b>Quick Quiz 3</b>:
But I don't care about McKenney's lame excuses in the answer to
Quick Quiz 2!!!
I want to get the number of CPUs contending on a single lock down
to something reasonable, like sixteen or so!!!

</p><p><b>Answer</b>:
OK, have it your way, then!!!
Set <code>CONFIG_RCU_FANOUT=16</code> and (for <code>NR_CPUS=4096</code>)
you will get a
three-level hierarchy with with 256 <code>rcu_node</code> structures
at the lowest level, 16 <code>rcu_node</code> structures as intermediate
nodes, and a single root-level <code>rcu_node</code>.
The penalty you will pay is that more <code>rcu_node</code> structures
will need to be scanned when checking to see which CPUs need help
completing their quiescent states (256 instead of only 64).

</p><p><a href="#Quick%20Quiz%203"><b>Back to Quick Quiz 3</b>.</a>

</p><p><b>Quick Quiz 4</b>:
OK, so what is the story with the colors?

</p><p><b>Answer</b>:
Data structures analogous to <code>rcu_state</code> (including
<code>rcu_ctrlblk</code>) are yellow,
those containing the bitmaps used to determine when CPUs have checked
in are pink,
and the per-CPU <code>rcu_data</code> structures are blue.
Later on, we will see that data structures used to conserve energy
(such as <code>rcu_dynticks</code>) will be green.

</p><p><a href="#Quick%20Quiz%204"><b>Back to Quick Quiz 4</b>.</a>


</p><p><b>Quick Quiz 5</b>:
Given such an egregious bug, why does Linux run at all?

</p><p><b>Answer</b>:
Because the Linux kernel contains device drivers that are (relatively)
well behaved.
Few if any of them spin in RCU read-side critical sections for the
many milliseconds that would be required to provoke this bug.
The bug nevertheless does need to be fixed, and this variant of
RCU does fix it.

</p><p><a href="#Quick%20Quiz%205"><b>Back to Quick Quiz 5</b>.</a>


</p><p><b>Quick Quiz 6</b>:
But what happens if a CPU tries to report going through a quiescent
state (by clearing its bit) before the bit-setting CPU has finished?

</p><p><b>Answer</b>:
There are three cases to consider here:

<ol>
<li>	A CPU corresponding to a non-yet-initialized leaf <code>rcu_node</code>
	structure tries to report a quiescent state.
	This CPU will see its bit already cleared, so will give up on
	reporting its quiescent state.
	Some later quiescent state will serve for the new grace period.
<li>	A CPU corresponding to a leaf <code>rcu_node</code> structure that
	is currently being initialized tries to report a quiescent state.
	This CPU will see that the <code>rcu_node</code> structure's
	<code>-&gt;lock</code> is held, so will spin until it is
	released.
	But once the lock is released, the <code>rcu_node</code>
	structure will have been initialized, reducing to the
	following case.
<li>	A CPU corresponding to a leaf <code>rcu_node</code> that has
	already been initialized tries to report a quiescent state.
	This CPU will find its bit set, and will therefore clear it.
	If it is the last CPU for that leaf node, it will
	move up to the next level of the hierarchy.
	However, this CPU cannot possibly be the last CPU in the system to
	report a quiescent state, given that the CPU doing the initialization
	cannot yet have checked in.
</ol>

<p>So, in all three cases, the potential race is resolved correctly.

</p><p><a href="#Quick%20Quiz%206"><b>Back to Quick Quiz 6</b>.</a>

</p><p><b>Quick Quiz 7</b>:
And what happens if <i>all</i> CPUs try to report going through a quiescent
state before the bit-setting CPU has finished, thus ending the new
grace period before it starts?

</p><p><b>Answer</b>:
The bit-setting CPU cannot pass through a
quiescent state during initialization, as it has irqs disabled.
Its bits therefore remain non-zero, preventing the grace period from
ending until the data structure has been fully initialized.

</p><p><a href="#Quick%20Quiz%207"><b>Back to Quick Quiz 7</b>.</a>

</p><p><b>Quick Quiz 8</b>:
And what happens if one CPU comes out of dyntick-idle mode and then
passed through a quiescent state just as another CPU notices that the
first CPU was in dyntick-idle mode?
Couldn't they both attempt to report a quiescent state at the same
time, resulting in confusion?

</p><p><b>Answer</b>:
They will both attempt to acquire the lock on the same leaf
<code>rcu_node</code> structure.
The first one to acquire the lock will report the quiescent state
and clear the appropriate bit, and the second one to acquire the
lock will see that this bit has already been cleared.

</p><p><a href="#Quick%20Quiz%208"><b>Back to Quick Quiz 8</b>.</a>

</p><p><b>Quick Quiz 9</b>:
But what if <i>all</i> the CPUs end up in dyntick-idle mode?
Wouldn't that prevent the current RCU grace period from ever ending?

</p><p><b>Answer</b>:
Indeed it will!
However, CPUs that have RCU callbacks are not permitted to enter
dyntick-idle mode, so the only way that <i>all</i> the CPUs could
possibly end up in dyntick-idle mode would be if there were
absolutely no RCU callbacks in the system.
And if there are no RCU callbacks in the system, then there is no
need for the RCU grace period to end.
In fact, there is no need for the RCU grace period to even <i>start</i>.

<p>RCU will restart if some irq handler does a <code>call_rcu()</code>,
which will cause an RCU callback to appear on the corresponding CPU,
which will force that CPU out of dyntick-idle mode, which will in turn
permit the current RCU grace period to come to an end.

</p><p><a href="#Quick%20Quiz%209"><b>Back to Quick Quiz 9</b>.</a>

</p><p><b>Quick Quiz 10</b>:
Given that <code>force_quiescent_state()</code> is a two-phase state
machine, don't we have double the scheduling latency due to scanning
all the CPUs?

</p><p><b>Answer</b>:
Ah, but the two phases will not execute back-to-back on the same CPU.
Therefore, the scheduling-latency hit of the two-phase algorithm is no
different than that of a single-phase algorithm.
If the scheduling latency becomes a problem, one approach would be to
recode the state machine to scan the CPUs incrementally.
But first show me a problem in the real world, <i>then</i>
I will consider fixing it!

</p><p><a href="#Quick%20Quiz%2010"><b>Back to Quick Quiz 10</b>.</a>

</p><p><b>Quick Quiz 11</b>:
But the other reason to hold <code>-&gt;onofflock</code> is to prevent
multiple concurrent online/offline operations, right?

</p><p><b>Answer</b>:
Actually, no!
The CPU-hotplug code's synchronization design prevents multiple
concurrent CPU online/offline operations, so only one CPU online/offline
operation can be executing at any given time.
Therefore, the only purpose of <code>-&gt;onofflock</code> is to prevent a CPU
online or offline operation from running concurrently with grace-period
initialization.

</p><p><a href="#Quick%20Quiz%2011"><b>Back to Quick Quiz 11</b>.</a>

</p><p><b>Quick Quiz 12</b>:
Given all these acquisitions of the global <code>-&gt;onofflock</code>,
won't there
be horrible lock contention when running with thousands of CPUs?

</p><p><b>Answer</b>:
Actually, there can be only three acquisitions of this lock per grace
period, and each grace period lasts many milliseconds.
One of the acquisitions is by the CPU initializing for the current
grace period, and the other two onlining and offlining some CPU.
These latter two cannot run concurrently due to the CPU-hotplug
locking, so at most two CPUs can be contending for this lock at any
given time.

<p>Lock contention on <code>-&gt;onofflock</code> should therefore
be no problem, even on systems with thousands of CPUs.

</p><p><a href="#Quick%20Quiz%2012"><b>Back to Quick Quiz 12</b>.</a><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Read-copy-update">Read-copy-update</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#McKenney_Paul_E.">McKenney, Paul E.</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/305782/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor306080"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 3:15 UTC (Thu)
                               by <b>kev009</b> (guest, #43906)
                              [<a href="/Articles/306080/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
This was an incredible write up, rich in technical data yet still compelling and informative to read.  Many thanks!  <br>
<p>
PS: you should consider authoring a book on kernel development.. :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306080/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306248"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 23:48 UTC (Thu)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/306248/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Glad you liked it, and thank you for the encouragement.  But...  Be careful what you wish for.  :-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306248/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor306114"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 8:54 UTC (Thu)
                               by <b>kleptog</b> (subscriber, #1183)
                              [<a href="/Articles/306114/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Agreed. This was a very readable explanation of RCU. Keep these sorts of articles coming.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306114/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor306135"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 11:19 UTC (Thu)
                               by <b>melo@simplicidade.org</b> (subscriber, #4380)
                              [<a href="/Articles/306135/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
This type or articles are a reason to keep paying for LWN.<br>
<p>
Thanks,<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306135/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor306212"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - state schematic</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 18:49 UTC (Thu)
                               by <b>ds2horner</b> (subscriber, #13438)
                              [<a href="/Articles/306212/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
In state machine schematic : "Mark CPU as being in Extended QS" from "CPU Offline" immediately returns to checking if "all cpus passed though QS".<br>
<p>
However the "Mark CPU as being in Extended QS" for a CPU in dyntick-idle still gets rescheduled. <br>
<p>
Is a dyntick-idle CPU actually a "hold out"?<br>
I thought it was in Extended QS and no further checking/scheduling was required.<br>
<p>
So is the arrow to "Send Resched IPI ..." incorrect, or should this be another Quick (or perhaps no so) Quiz?<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306212/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306225"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - state schematic</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 6, 2008 19:49 UTC (Thu)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/306225/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The trick with that part of the diagram is that the code is actually dealing with groups of CPUs.  So a given group of CPUs might have some that are in dyntick-idle state and others that have somehow avoided passing through a quiescent state, despite the fact that they are online and running.  We send a reschedule IPI only to these latter CPUs, not to the dyntick-idle CPUs.<br>
<p>
Now that you mention it, this might indeed be a good quick-quiz candidate.  :-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306225/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306255"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - state schematic</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 7, 2008 1:55 UTC (Fri)
                               by <b>ds2horner</b> (subscriber, #13438)
                              [<a href="/Articles/306255/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I think the leveling confusion in the diagram starts with the exit from the "wait for QS" state.<br>
<p>
"CPU passes through QS" and "CPU offline" are considered singleton events, but the "GP too long" is considered a bulk event with all blocking CPUs being addressed.<br>
<p>
This, and the explanation of the initiation of the check for the dynaticks state, clarified for me (indeed corrected a misconception I had ) that the dyntick counters are not captured at the beginning of each GP, but only after a "time out" (a reasonable optimization for a low occurrence event).<br>
<p>
Speaking of the time out - <br>
in "Detect a Too-Long Grace Period" the "record_gp_stall_check_time() function records the time and also a timestamp set three seconds into the future." timeframe seemed excessive. I believe jiffies was meant (not seconds) which would be consistent with the later reference "A two-jiffies offset helps ensure that CPUs report on themselves when possible".<br>
<p>
Each article clarifies details I missed before.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306255/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306262"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - state schematic</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 7, 2008 5:52 UTC (Fri)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/306262/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Glad it helped!<br>
<p>
There are indeed two levels of tardiness.  Reschedule IPIs are sent to hold-out CPUs after three jiffies, which, in absence of kernel bugs, should end the grace period.  These reschedule IPIs are considered "normal" rather than "errors".<br>
<p>
There is a separate "error" path enabled by CONFIG_RCU_CPU_STALL_DETECTOR that does in fact have a three-second timeout (recently upped to 10 seconds based on the fact that three-second stalls appear to be present in boot-up code).  There is also a three-second-and-two-jiffies timeout as part of this "error" path so that CPUs will normally report on themselves rather than being reported on by others, given that self-reported stack traces are usually more reliable.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306262/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306268"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - unification suggestion</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 7, 2008 15:50 UTC (Fri)
                               by <b>ds2horner</b> (subscriber, #13438)
                              [<a href="/Articles/306268/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thank you again.<br>
<p>
And now that I hope I believe I understand the process adequately I have a suggestion.<br>
<p>
Why not unify the CPU hotplug and "dyntick suspend" status tracking by using the same counter as dyntick for both?<br>
<p>
If the CPU offline / online advanced the (now renamed rcu_cpu_awake ( with the LBS matching CPU state)) counter, the even / odd state will satisfy both scenarios.<br>
<p>
And I am not advocating this solely for the purpose of reducing the state variables and reducing code (if not complexity): <br>
Conceptually, an offline CPU (a powered down engine which cannot receive interrupts ) is equivalent to a dyntick CPU (powered down state) that receives no interrupts in the grace period.<br>
<p>
I realize there are, of course, other reasons to track online/offline CPUs, that the offline check is immediately available.<br>
<p>
However, the unification would allow a simpler "no off line" option to the this intended "dyntick replacement" to CLASSIC RCU.<br>
And a simpler "no NO_HZ" option that would be useful for virtual CPUs.<br>
<p>
I know - "show me the code!".<br>
<p>
OK reading your posts <br>
"v6 scalable classic RCU implementation" "Tue, 23 Sep 2008 16:53:40 -0700"<br>
indicate the magnitude of code that is present to handle a CPU offline activity.<br>
Concerns like moving outstanding rcu work from the "forced" offline CPU to the current CPU, make the offlining distinct.<br>
<p>
However, it appears to be performed (at least in this version of the code) just before sending the tardy CPUs the IPI reschedule. Which, back to the schematic, makes it part of the "GP too long" path?<br>
<p>
And reading your further post "rcu-state" "Mon, 27 Oct 2008 20:52:01 +0100" <br>
It looks like this version is using a unifying indicator "rcu_cpumode" - still with 2 distinct substates, that are separate from  RCU_CPUMODE_PERIODIC which requires the notifications each Grace Period.<br>
<p>
How does one keep up?  <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/306268/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor306337"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hierarchical RCU - unification suggestion</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 8, 2008 1:03 UTC (Sat)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/306337/">Link</a>] 
      </p>
      
      </div>
      </summary>
      Looks like I should have had yet another Quick Quiz on unifying dyntick-idle and offline detection.

<p>First, it might well turn out to be the right thing to do.  However, the challenges include the following:

<ol>
<li> As you say, 
<li> As you say, although CPUs are not allowed to go into dyntick-idle state while they have RCU callbacks pending, CPUs -are- allowed to go offline in this state.  This means that the code to move their callbacks is still requires.  We cannot simply let them silently go offline.
<li> Present-day systems often run with NR_CPUS much larger than the actual number of CPUs, so the unified approach could waste time scanning CPUs that never will exist.  (There are workarounds for this.)
<li> CPUs get onlined one at a time, so RCU needs to handle onlining.
<li> A busy system with offlined CPUs would always take three ticks to figure out that the offlined CPUs were never going to respond.
<li> Switching into and out of dyntick-idle state can happen extremely frequently, so we cannot treat a dyntick-idle CPU as if it was offline due to the high overhead of offlining CPUs.
</ol>

Under normal circumstances, offline CPUs are not included in the bitmasks indicating which CPUs need to be waited on, so that normally RCU never waits on offline CPUs.
However, there are race conditions that can occur in the online and offline processes that can result in RCU believing that a given CPU is online when it is in fact offline.
Therefore, if RCU sees that the grace period is extending longer than expected (jiffies, not seconds), it will check to see if some of the CPUs that it is waiting on are offline.
This situation corrects itself after a few grace periods: RCU will get back in sync with which CPUs really are offline.
So the offline-CPU checks invoked from force_quiescent_state() are only there to handle rare race conditions.
Again, under normal circumstances, RCU never waits on offline CPUs.

<p>At this point, when the code is still just a patch, and therefore subject to change, the only way I can see to keep up is to ask questions.  Which you are doing.  :-)
      
          <div class="CommentReplyButton">
            <form action="/Articles/306337/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2008, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
