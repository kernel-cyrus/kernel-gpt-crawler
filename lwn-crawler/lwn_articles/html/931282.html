        <!DOCTYPE html>
        <html lang="en">
        <head><title>A storage standards update at LSFMM+BPF [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/931282/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/931576/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/931282/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>A storage standards update at LSFMM+BPF</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 11, 2023</br>
           <hr>
<a href="/Articles/lsfmmbpf2023">LSFMM+BPF</a>
</div>
Storage technology may seem like a slow-moving area, but there is, instead,
a lot of development activity happening there.  An early session at the
<a href="/Articles/lsfmmbpf2023">2023 Linux Storage, Filesystem,
Memory-management and BPF Summit</a>, 
led by Martin Petersen and Vincent Haché, updated the assembled group on
the latest changes to the storage landscape, with an emphasis on the
Compute Express Link (CXL) 3.0 specification.
<p>
<h4>Linux storage-stack projects</h4>
<p>
Petersen started with a quick overview of a number of areas of interest to
the storage community, the first of which is "flexible data placement".
That, he said, is "the new cloud-vendor favorite way" to address
write-amplification issues; there is a new favorite every year, he added.
Flexible data placement allows the kernel to tell storage devices which
blocks belong together so that they can be updated in a single operation.
That should make the device's garbage-collection process easier.
<p>

<a href="/Articles/931299/"><img
src="https://static.lwn.net/images/conf/2023/lsfmm/MartinPetersen-sm.png" alt="[Martin Petersen]"
title="Martin Petersen" class="lthumb"></a>

Copy offload is a perennial subject in the storage world.  The SCSI
standard provides many ways of offloading copy operations, but NVMe has,
until now, only been able to offload copy operations within a single <a
href="https://nvmexpress.org/resource/nvme-namespaces/">NVMe namespace</a>.
Work is now happening on enabling cross-namespace copy offloading, which
complicates the situation in a number of ways.  One big challenge is simply
figuring out whether two NVMe devices are able to communicate with each
other.  The SCSI stack makes that determination ahead of time so that it
knows not to attempt offload an operation if the devices involved are
unable to talk to each other; NVMe will gain a similar capability.
<p>
A related area is computational storage — an NVMe namespace without any
actual storage associated with it.  Instead, these devices can offload
operations like compression and encryption, working directly with data
stored in other namespaces.
<p>
"Hinting" is telling storage devices more about the data that they are
holding; again, it is intended to allow the devices to make better
decisions about data placement.  There are a lot of devices that can
benefit from hinting, but developers have spent years trying to get it to
work properly.  That work will continue, he hinted.
<p>
Some types of devices can support atomic block-write operations, meaning
that either the entire operation succeeds, or none of it does.  The kernel
would like to make it possible for user space to use that capability where
it exists, but it's a complicated task.  The need to find some sort of
common ground between the SCSI and NVMe implementations of atomic writes
makes it even more so.
<p>
Checksums are often used to detect (and possibly correct) the corruption of
stored data, but the 16-bit checksums that have long been in use were
designed for a world of 512-byte blocks; they are too small for the larger
blocks used now.  Both SCSI and NVMe have added 32-bit and 64-bit checksum
formats that are being deployed, though an audience member commented that
it is not being pushed hard in the SCSI world.  This feature, Petersen
said, is most useful in cloud-storage environments, where data corruption
is more common.
<p>
Finally, Petersen mentioned NVMe live migration.  If a virtual machine that
is running with an NVMe-like device is migrated to a new physical host, the
device needs to migrate with it.  There are currently efforts afoot to
define a standards-based approach for that type of migration.
<p>
<h4>CXL 3.0</h4>
<p>
Haché then took over to present some highlights from the new CXL 3.0
standard.  The specification, he said, is 1,100 pages in length;
mercifully, he did not plan to cover the whole thing during the session.
Memory, he said, is traditionally a static resource physically attached to
the CPU, but we are heading into a world where it is dynamically pooled
instead.  The kernel will have to adapt to this world.
<p>
<a href="/Articles/931300/"><img
src="https://static.lwn.net/images/conf/2023/lsfmm/VincentHache-sm.png" alt="[Vincent Haché]"
title="Vincent Haché" class="rthumb"></a>


The 3.0 standard has added a long list of features designed to improve
scalability.  These include the ability to create fabrics of CXL devices,
and increased memory pooling and sharing capabilities.  A lot of work has
been done at the transaction layer to ensure coherency when multiple
systems are accessing the same memory; this is especially needed for
peer-to-peer operations, where devices can access CXL memory directly
without going through a host system.
<p>
The CXL 2.0 specification added "multiple logical device" support, where
each function supported by a CXL device would be attached to a single host
computer.  This mechanism works, but it requires a sophisticated switch to
implement, and that switch increases memory-access latency considerably.
To address this problem, the 3.0 specification added a "multi-headed
device" mode that adds more host ports and moves the switch into the
controller.  This mode, Haché said, does not scale as well, but it does
improve latency.
<p>
On top of this mechanism is the "dynamic capacity device" (DCD) layer,
which controls the allocation of CXL resources to hosts.  Before DCD,
changing a host's memory allocation was a disruptive act that required
remapping the host's physical address space.  In practice, that required
rebooting the host.  This kind of change was "better than popping out a
DIMM", he said, but was still painful.

With DCD, instead, the maximum capacity of each CXL resource is exposed to
the host from the beginning, and DCD tells the host what its actual memory
allocation is.  CXL memory is divided into blocks, then grouped into
extents that can be allocated to hosts.  The extent lists are small and
easy to update with the hosts.
<p>
The tree-like hierarchy of CXL controllers described in the 2.0
specification turned out to limit scalability, so version 3.0 has added a
way to organize CXL resources into fabrics, with switches densely
interconnected with each other.  There is a provision for
fabric-attached memory that is not part of any specific host; accelerators
can also sit on the fabric and access resources directly.  A CXL fabric,
Haché said, cannot provide the same  bandwidth that Ethernet
can, so it will not be possible to put an entire data center on one fabric.
But it will be possible to create large memory pools, with on the order of
1,000 hosts and 1,000 memory devices, and share that memory across the
hosts with around 100ns latency.
<p>
Haché suggested that attendees download the specification and read it for
themselves, it <a
href="https://www.computeexpresslink.org/download-the-specification">can be
had from computeexpresslink.org</a> after agreeing to a lengthy set of
terms and conditions.
<p>
David Howells said that CXL starts to sound a lot like InfiniBand, and
asked how the kernel was expected to actually use CXL resources.  Haché
answered that, on most systems, CXL memory would just show up as if it were
local memory, contained within a CPU-less NUMA node.  The kernel should not
need to do anything special with it other than realizing that it will be a
bit slower, much like persistent memory.  Some sort of tiering approach is
likely to be necessary at some point.
<p>
Dan Williams added that CXL has been defined so that the host does not
really even need to know what it is.  The BIOS can set everything up before
the system boots.  CXL memory, he said, is best used when it is normal and
uninteresting.  But it <i>can</i> be used in more interesting ways when the
need arises.
<p>
Matthew Wilcox complained that nobody was talking about contention.  What
happens when there are 1,000 hosts all accessing the same cacheline in CXL
memory?  That could lead to multi-second delays for cache-line access,
which could "break CPUs"; the prospect scares him.  Haché pointed out that
memory-pooling devices can have quality-of-service capabilities that allow
access limits to be set on a per-host basis; that could prevent the worst
problems.  Wilcox responded that it can often be hard to tell highly active
users from malicious users.  Another audience member said that, for now,
the most common use case will be cloud computing, where memory will be
allocated to a single host and contention issues will not arise.  There
will be high-performance computing applications in the future that want to
more fully use CXL's capabilities, though.
<p>
James Bottomley, at the close of the session, asked what the "killer app"
for CXL would be.  Haché responded that memory capacity and bandwidth are a
big problem for data centers currently.  There is only so much memory that
can be physically connected to a CPU before it runs out of DIMM slots;
there are efforts to increase memory densities, but they are expected to
double the price of memory.  In many data centers now, memory alone
represents 50-60% of the cost of a server, and that percentage will only go
up.  CXL offers the alternative of connecting terabytes of DDR4 memory and
making the resulting capacity available to a bunch of servers.  There are
other interesting use cases, but RAM costs are the biggest motivating
factor currently, he said.
<p>
Bottomley responded that this was the overcommit issue again; vendors are
trying to sell the same memory to multiple customers on the assumption that
they won't all use it all simultaneously.  CXL is a good way of not
getting caught at that game, he said.  Haché refused to comment on that
point.
<p>
The session ended there, but the changes discussed here were to reappear
many times throughout the conference, where they would be discussed in
greater detail.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Compute_Express_Link_CXL">Compute Express Link (CXL)</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_Memory-Management_and_BPF_Summit-2023">Storage, Filesystem, Memory-Management and BPF Summit/2023</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/931282/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor931714"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A storage standards update at LSFMM+BPF</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 12, 2023 11:33 UTC (Fri)
                               by <b>walex</b> (subscriber, #69836)
                              [<a href="/Articles/931714/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <blockquote>«"Hinting" is telling storage devices more about the data that they are holding; again, it is intended to allow the devices to make better decisions about data placement. There are a lot of devices that can benefit from hinting, but developers have spent years trying to get it to work properly. That work will continue, he hinted.»</blockquote>

<p>I remember some decades ago a discussion with Larry McVoy where I pointed out the lack of hinting and preallocation even in <tt>cp</tt>, where it is trivial (never mind <tt>ld.so</tt> etc. or <tt>stdio</tt> itself). Relatively recently at least <tt>rsync</tt> got <tt>--preallocate</tt>.</p>

<a href="https://www.sabi.co.uk/blog/anno05-4th.html?051012d#051012d"><tt>https://www.sabi.co.uk/blog/anno05-4th.html?051012d#051012d</tt></a>


      
          <div class="CommentReplyButton">
            <form action="/Articles/931714/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor932384"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A storage standards update at LSFMM+BPF</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 18, 2023 15:55 UTC (Thu)
                               by <b>flussence</b> (guest, #85566)
                              [<a href="/Articles/932384/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<span class="QuotedText">&gt; A related area is computational storage — an NVMe namespace without any actual storage associated with it. Instead, these devices can offload operations like compression and encryption, working directly with data stored in other namespaces.</span><br>
<p>
Seeing this reminded me the Sony PS5 does something similar; userspace access to the NVMe drive is mediated by a (proprietary) hardware compression chip so it appears faster than it is, which is already pretty fast since IIRC it has 12 PCIe gen4 lanes while the standard form factor only has 4.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932384/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2023, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
