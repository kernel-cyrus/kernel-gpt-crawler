        <!DOCTYPE html>
        <html lang="en">
        <head><title>Ftrace: The hidden light switch [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/608497/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/607987/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/608497/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Ftrace: The hidden light switch</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</blockquote>
<div class="GAByline">
           <p>August 13, 2014</p>
           <p>This article was contributed by <a href="http://www.brendangregg.com">Brendan Gregg</a></p>
           </div>
<p>You may think, as I did, that analyzing the Linux kernel is like
venturing through a dark dungeon: without the addition of advanced tracers
like SystemTap, there's much that can't be seen, and can only be
inferred.

However, I've recently found hidden switches that turn on some bright
lights, strategically placed by Steven Rostedt and others since the 2.6.27
release. These are the ftrace profilers. I haven't even tried all the
switches yet, but I'm stunned at what I've seen so far, and I'm having to
rethink what I previously believed about Linux kernel performance
analysis.</p>

<p>Recently at Netflix (where I work), a Cassandra database was performing
poorly after a 
system upgrade, and disk I/O inflation (a massive
increase in the number of I/O operations submitted) was suspected. There
can be many 
causes for this: a worse cache-hit ratio, record-size inflation, readahead
inflation, other applications, even other asynchronous kernel tasks (file
system background scrubs). The question was: which one, and how do we fix
it?</p>

<h4 id="iosnoop">1. iosnoop</h4>
<p>I started with basic server health checks, and then my <a
href="http://www.brendangregg.com/blog/2014-07-16/iosnoop-for-linux.html"><tt>iosnoop</tt></a>
tool.
<tt>iosnoop</tt> is a shell script that uses the <tt>/sys/kernel/debug</tt>
ftrace 
facilities, and is in my <a
href="https://github.com/brendangregg/perf-tools">perf-tools</a> collection
on GitHub, along with other ftrace hacks. These work on Netflix's existing
servers (which often run Linux 3.2 with security patches) without any
other addition to the system, and without requiring kernel debuginfo.
In this case, <tt>iosnoop</tt> was run  both with
and without <tt>-Q</tt> to see the effect of queuing:</p>

<pre>
  # <b>./iosnoop -ts</b>
  STARTs          ENDs            COMM         PID    TYPE DEV      BLOCK        BYTES     LATms
  13370264.614265 13370264.614844 java         8248   R    202,32   1431244248   45056      0.58
  13370264.614269 13370264.614852 java         8248   R    202,32   1431244336   45056      0.58
  13370264.614271 13370264.614857 java         8248   R    202,32   1431244424   45056      0.59
  13370264.614273 13370264.614868 java         8248   R    202,32   1431244512   45056      0.59
  [...]
  # <b>./iosnoop -Qts</b>
  STARTs          ENDs            COMM         PID    TYPE DEV      BLOCK        BYTES     LATms
  13370410.927331 13370410.931182 java         8248   R    202,32   1596381840   45056      3.85
  13370410.927332 13370410.931200 java         8248   R    202,32   1596381928   45056      3.87
  13370410.927332 13370410.931215 java         8248   R    202,32   1596382016   45056      3.88
  13370410.927332 13370410.931226 java         8248   R    202,32   1596382104   45056      3.89
  [...]
</pre>

<p>I didn't see anything out of the ordinary: a higher disk I/O load was
causing higher queue times.</p>


<p>The tools that follow are all from the same collection: they demonstrate
existing capabilities of ftrace, and how they are useful for solving real
problems.</p>

<h4 id="tpoint">2. tpoint</h4>

<p>To investigate these disk reads in more detail, I used <tt>tpoint</tt> to
trace the <tt>block:block_rq_insert</tt> tracepoint:</p>
<p>
<pre>
  # <b>./tpoint -H block:block_rq_insert</b>
  Tracing block:block_rq_insert. Ctrl-C to end.
  # tracer: nop
  #
  #       TASK-PID    CPU#    TIMESTAMP  FUNCTION
  #          | |       |          |         |
          java-16035 [000] 13371565.253582: block_rq_insert: 202,16 WS 0 () 550505336 + 88 [java]
          java-16035 [000] 13371565.253582: block_rq_insert: 202,16 WS 0 () 550505424 + 56 [java]
          java-8248  [007] 13371565.278372: block_rq_insert: 202,32 R 0 () 660621368 + 88 [java]
          java-8248  [007] 13371565.278373: block_rq_insert: 202,32 R 0 () 660621456 + 88 [java]
          java-8248  [007] 13371565.278374: block_rq_insert: 202,32 R 0 () 660621544 + 24 [java]
          java-8249  [007] 13371565.311507: block_rq_insert: 202,32 R 0 () 660666416 + 88 [java]
  [...]
</pre>

<p>I was checking for anything obviously odd, but the I/O details looked
normal. The <tt>-H</tt> option prints column headers.</p>

<p>Next, I traced the code path that led to this I/O by printing stack
traces (<tt>-s</tt>), to see if they contained an explanation. I also added an
in-kernel filter to match reads only (when the &quot;<tt>rwbs</tt>&quot;
flag field contains &quot;<tt>R</tt>&quot;):</p>

<pre>
  # <b>./tpoint -s block:block_rq_insert 'rwbs ~ "*R*"' | head -1000</b>
  Tracing block:block_rq_insert. Ctrl-C to end.
          java-8248  [005] 13370789.973826: block_rq_insert: 202,16 R 0 () 1431480000 + 8 [java]
          java-8248  [005] 13370789.973831: &lt;stack trace&gt;
   =&gt; blk_flush_plug_list
   =&gt; blk_queue_bio
   =&gt; generic_make_request.part.50
   =&gt; generic_make_request
   =&gt; submit_bio
   =&gt; do_mpage_readpage
   =&gt; mpage_readpages
   =&gt; xfs_vm_readpages
   =&gt; read_pages
   =&gt; __do_page_cache_readahead
   =&gt; ra_submit
   =&gt; do_sync_mmap_readahead.isra.24
   =&gt; filemap_fault
   =&gt; __do_fault
   =&gt; handle_pte_fault
   =&gt; handle_mm_fault
   =&gt; do_page_fault
   =&gt; page_fault
          java-8248  [005] 13370789.973831: block_rq_insert: 202,16 R 0 () 1431480024 + 32 [java]
          java-8248  [005] 13370789.973836: &lt;stack trace&gt;
   =&gt; blk_flush_plug_list
   =&gt; blk_queue_bio
   =&gt; generic_make_request.part.50
  [...]
</pre>

<p>Great! The output is similar to the previous example, but with stack
traces beneath each tracepoint event. I limited the output using <tt>head</tt> as
it is verbose.</p>

<p><tt>tpoint</tt> is another ftrace-based tool. It's usually better to use 
perf events (the
<tt>perf</tt> command) for this 
particular use case, as it can handle multi-user access to performance data
and a higher event 
rate, although it is more time-consuming to use. I just wanted to quickly
eyeball a few dozen stack traces for a given tracepoint.</p>

<p>The stacks were mostly the same as the example above, which provided a
couple of leads: page faults and readahead. This Ubuntu system was using 2MB
direct-mapped pages, instead of 4KB like the old system. It also
had readahead set to 2048KB, instead of 128KB. Either of these
differences could be causing the inflation, although tuning readahead had
already been tested, and found to make no difference.</p>

<h4 id="funccount">3. funccount</h4>

<p>I wanted to understand that stack trace better, so I started by counting
calls using <tt>funccount</tt>, which uses ftrace function profiling. Starting
with the per-second rate of <tt>submit_bio()</tt>:</p>

<pre>
  # <b>./funccount -i 1 submit_bio</b>
  Tracing "submit_bio"... Ctrl-C to end.

  FUNC                              COUNT
  submit_bio                        27881

  FUNC                              COUNT
  submit_bio                        28478
  [...]
</pre>

<p>This rate, about 28,000 calls per second, is on par with what the disks
are doing 
as seen from <tt>iostat</tt>. <tt>funccount</tt> is counting events in the
kernel for efficiency.</p>

<p>Now checking the rate of <tt>filemap_fault()</tt>, which is closer in
the stack to the database:</p>

<pre>
  # <b>./funccount -i 1 filemap_fault</b>
  Tracing "filemap_fault"... Ctrl-C to end.

  FUNC                              COUNT
  filemap_fault                      2203

  FUNC                              COUNT
  filemap_fault                      3227
  [...]
</pre>

<p>This is consistent with what we believed the application was requesting
from the filesystem. There is about a 10x inflation between these calls
and the issued disk I/O (as evidenced by the <tt>submit_bio()</tt> calls).</p>

<h4 id="funcslower">4. funcslower</h4>

<p>Just to confirm that the database is suffering latency caused by the stack
trace I was studying, I used <tt>funcslower</tt> (another ftrace-based
tool, which uses in-kernel timing and filtering for efficiency) 
to measure <tt>filemap_fault()</tt> calls 
taking longer than 1000 microseconds (1ms):</p>

<pre>
  # <b>./funcslower -P filemap_fault 1000</b>
  Tracing "filemap_fault" slower than 1000 us... Ctrl-C to end.
   0)   java-8210    | ! 5133.499 us |  } /* filemap_fault */
   0)   java-8258    | ! 1120.600 us |  } /* filemap_fault */
   0)   java-8235    | ! 6526.470 us |  } /* filemap_fault */
   2)   java-8245    | ! 1458.30 us  |  } /* filemap_fault */
  [...]
</pre>

<p>These latencies look similar to those seen from disk I/O (with queue
time). I'm in the right area.</p>

<h4 id="funccount-again">5. funccount (again)</h4>

<p>I noticed that the stack has &quot;readpage&quot; calls and then
&quot;readpage<b>s</b>&quot;. Tracing them both at the same time:</p>

<pre>
  # <b>./funccount -i 1 '*mpage_readpage*'</b>
  Tracing "*mpage_readpage*"... Ctrl-C to end.

  FUNC                              COUNT
  mpage_readpages                     364
  do_mpage_readpage                122930

  FUNC                              COUNT
  mpage_readpages                     318
  do_mpage_readpage                110344
  [...]
</pre>

<p>Here's our inflation: <tt>mpage_readpages()</tt> is being called about
300 times per second, and then <tt>do_mpage_readpage()</tt> over 100k times
per second. This still looks like readahead, although we did try to adjust
readahead sizes as an
experiment, and it didn't make a difference.</p>

<h4 id="kprobe">6. kprobe</h4>

<p>Maybe our readahead tuning didn't take effect? I can check this using
dynamic tracing of kernel functions.

Starting with the above stack trace, I saw that
<tt>__do_page_cache_readahead()</tt> 
has <tt>nr_to_read</tt> (number of pages to read) as an argument, which
comes from 
the readahead setting. Using <tt>kprobe</tt>, an ftrace- and kprobes-based tool,
to dynamically trace this function and argument:</p>

<pre>
  # <b>./kprobe -H 'p:do __do_page_cache_readahead nr_to_read=%cx'</b>
  Tracing kprobe m. Ctrl-C to end.
  # tracer: nop
  #
  #   TASK-PID    CPU#    TIMESTAMP  FUNCTION
  #      | |       |          |         |
      java-8714  [000] 13445354.703793: do: (__do_page_cache_readahead+0x0/0x180) nr_to_read=200
      java-8716  [002] 13445354.819645: do: (__do_page_cache_readahead+0x0/0x180) nr_to_read=200
      java-8734  [001] 13445354.820965: do: (__do_page_cache_readahead+0x0/0x180) nr_to_read=200
      java-8709  [000] 13445354.825280: do: (__do_page_cache_readahead+0x0/0x180) nr_to_read=200
  [...]
</pre>

<p>I used <tt>-H</tt> to print the header, and <tt>p:</tt> to specify that we
will create a probe on function entry, which we'll call &quot;do&quot;
(that alias is optional). The rest of that line specifies the function and
optional arguments.

Without kernel debuginfo, I can't refer to the <tt>nr_to_read</tt>
symbol, so I need to use registers instead. I guessed %cx: If true, then
our tuning hasn't taken hold, as 200 in hex is 512 pages: the original 2048KB.</p>

<h4 id="funcgraph">7. funcgraph</h4>

<p>To be sure, I read the code to see how this value is passed around, and
used <tt>funcgraph</tt> to illustrate it:</p>

<pre>
  # <b>./funcgraph -P filemap_fault | head -1000</b>
   2)   java-8248    |               |  filemap_fault() {
   2)   java-8248    |   0.568 us    |    find_get_page();
   2)   java-8248    |               |    do_sync_mmap_readahead.isra.24() {
   2)   java-8248    |   0.160 us    |      max_sane_readahead();
   2)   java-8248    |               |      ra_submit() {
   2)   java-8248    |               |        __do_page_cache_readahead() {
   2)   java-8248    |               |          __page_cache_alloc() {
   2)   java-8248    |               |            alloc_pages_current() {
   2)   java-8248    |   0.228 us    |              interleave_nodes();
   2)   java-8248    |               |              alloc_page_interleave() {
   2)   java-8248    |               |                __alloc_pages_nodemask() {
   2)   java-8248    |   0.105 us    |                  next_zones_zonelist();
   2)   java-8248    |               |                  get_page_from_freelist() {
   2)   java-8248    |   0.093 us    |                    next_zones_zonelist();
   2)   java-8248    |   0.101 us    |                    zone_watermark_ok();
   2)   java-8248    |               |                    zone_statistics() {
   2)   java-8248    |   0.073 us    |                      __inc_zone_state();
   2)   java-8248    |   0.074 us    |                      __inc_zone_state();
   2)   java-8248    |   1.209 us    |                    }
   2)   java-8248    |   0.142 us    |                    prep_new_page();
   2)   java-8248    |   3.582 us    |                  }
   2)   java-8248    |   4.810 us    |                }
   2)   java-8248    |   0.094 us    |                inc_zone_page_state();
  [...]
</pre>

<p><tt>funcgraph</tt> uses another ftrace feature: the function graph
profiler. It has moderate overhead, since it traces all kernel functions,
so I only use it for exploratory purposes like this one-off. The output
shows the code-flow in the kernel, and even has time deltas in
microseconds. It's the call to <tt>max_sane_readahead()</tt> that is
interesting, as that fetches the readahead value it wants to use.</p>

<h4 id="kprobe-again">8. kprobe (again)</h4>

<p>This time I'll trace the return of the <tt>max_sane_readahead()</tt>
function:</p> 
<pre>
  # <b>./kprobe 'r:m max_sane_readahead $retval'</b>
  Tracing kprobe m. Ctrl-C to end.
      java-8700  [000] 13445377.393895: m: (do_sync_mmap_readahead.isra.24+0x62/0x9c &lt;- \
  			max_sane_readahead) arg1=200
      java-8723  [003] 13445377.396362: m: (do_sync_mmap_readahead.isra.24+0x62/0x9c &lt;- \
  			max_sane_readahead) arg1=200
      java-8701  [001] 13445377.398216: m: (do_sync_mmap_readahead.isra.24+0x62/0x9c &lt;- \
  			max_sane_readahead) arg1=200
      java-8738  [000] 13445377.399793: m: (do_sync_mmap_readahead.isra.24+0x62/0x9c &lt;- \
  			max_sane_readahead) arg1=200
      java-8728  [000] 13445377.408529: m: (do_sync_mmap_readahead.isra.24+0x62/0x9c &lt;- \
  			max_sane_readahead) arg1=200
  [...]
</pre>

<p>This is also 0x200 pages: 2048KB, and this time I used the <tt>$retval</tt>
alias instead of guessing registers. So the tuning really did not take
effect.
Studying the kernel source, I saw that the readahead property was set by
a function called <tt>file_ra_state_init()</tt>. Under what circumstances is
<em>that</em> called, and how do I trigger it? ftrace/kprobes to the rescue
again:</p>

<pre>
  # <b>./kprobe -s p:file_ra_state_init</b>
  Tracing kprobe m. Ctrl-C to end.
            kprobe-20331 [002] 13454836.914913: file_ra_state_init: (file_ra_state_init+0x0/0x30)
            kprobe-20331 [002] 13454836.914918: &lt;stack trace&gt;
   =&gt; vfs_open
   =&gt; nameidata_to_filp
   =&gt; do_last
   =&gt; path_openat
   =&gt; do_filp_open
   =&gt; do_sys_open
   =&gt; sys_open
   =&gt; system_call_fastpath
            kprobe-20332 [007] 13454836.915191: file_ra_state_init: (file_ra_state_init+0x0/0x30)
            kprobe-20332 [007] 13454836.915194: &lt;stack trace&gt;
   =&gt; vfs_open
   =&gt; nameidata_to_filp
  [...]
</pre>

<p>This time I used <tt>-s</tt> to print stack traces, which showed that
this often 
happens from the <tt>open()</tt> syscall. As I'd left Cassandra running
while tuning 
readahead, it may not have reopened its files and run
<tt>file_ra_stat_init()</tt>.

So I restarted Cassandra to see if the readahead tuning would then take
effect, and re-measured:</p>

<pre>
  # <b>./kprobe 'r:m max_sane_readahead $retval'</b>
  Tracing kprobe m. Ctrl-C to end.
      java-11918 [007] 13445663.126999: m: (ondemand_readahead+0x3b/0x230 &lt;- \
   			max_sane_readahead) arg1=80
      java-11918 [007] 13445663.128329: m: (ondemand_readahead+0x3b/0x230 &lt;- \
  			max_sane_readahead) arg1=80
      java-11918 [007] 13445663.129795: m: (ondemand_readahead+0x3b/0x230 &lt;- \
  			max_sane_readahead) arg1=80
      java-11918 [007] 13445663.131164: m: (ondemand_readahead+0x3b/0x230 &lt;- \
  			max_sane_readahead) arg1=80
  [...]
</pre>

<p>Success!</p>

<p><tt>iostat</tt> showed a large drop in disk I/O, and the database latency
measurements were much better. This was simply a readahead change, where
the new Ubuntu instances defaulted to 2048KB. What had misled us
earlier was that tuning it had not appeared to make a difference, as the
setting wasn't taking effect.</p>

<p>Another tunable we checked was the I/O scheduler, and whether changing
it from &quot;deadline&quot; to &quot;noop&quot; was immediate. Using:
<p>
<pre>
  ./funccount -i 1 'deadline*'
  ./funccount -i 1 'noop*'
</pre>
<p>
gave us the answer: they showed the rate of the related
kernel functions, and that the tuning was indeed immediate.</p>

<h4 id="ftrace-and-perf-tools">ftrace and perf-tools</h4>

<p>All the tools I used here are from my 
perf-tools
collection, which are front-ends to ftrace and related tracers (kprobes,
tracepoints, and the function profiler). I've described some of them as
hacks, which they are, as they use creative workarounds for the lack of
some in-kernel features. For example, <tt>iosnoop</tt> reads both issue and
completion events in user space, and calculates the latencies there,
instead of doing that more efficiently in kernel context.</p>

<p>These tools are for older Linux systems without kernel debuginfo, and
show what Linux can do using only its built-in ftrace. There's even more to
ftrace that I didn't show here: profiling function average latency, tracing
wakeup events, function probe triggers, etc. There is also Steven Rostedt's
own front-end to ftrace, a multi-tool called <a
href="http://git.kernel.org/cgit/linux/kernel/git/rostedt/trace-cmd.git">trace-cmd</a>
(<a href="/Articles/410200/">covered on LWN</a> in 2010),
which can do more than my collection of smaller tools, and is also much
easier to use than operating on the <tt>/sys</tt> files directly.</p>

<p>To do even more, perf_events (which is also part of the Linux kernel
source) with kernel debuginfo lets me examine complex data structures, and
even trace kernel code by line number and watch local variables. What I still
can't do, however, is perform complex in-kernel aggregations until I add
other advanced tracers like <a
href="https://sourceware.org/systemtap/">SystemTap</a>, <a 
href="/Articles/551314/">ktap</a>, or coming up: <a
href="/Articles/603983/">eBPF</a>.</p>  

<h4 id="future-work">Future Work</h4>

<p>Changes are on the way for the Linux kernel, as regular LWN
readers likely know. The capabilities
I need, to avoid using hacks, may be provided by eBPF, which may be merged
<a href="http://lwn.net/Articles/606089/">as soon as Linux 3.18</a>. If
that happens, I'll be happy to create new versions of these tools making
use of eBPF. They will likely look and feel the same, but their
implementation will be much more efficient and reliable. I'll also be able
to create many more tools without the fear of maintaining too many
hacks.</p>

<p>Although, even if Linux does bring these capabilities in an upcoming
release, itâ€™ll be some time before I can really use them in production in
my work environment, depending on how quickly we can or want to be on the
latest kernel. So, while my hacks are temporary workarounds, they may be
useful for some time to come.</p>

<h4 id="conclusion">Conclusion</h4>

<p>We have a large number of Linux cloud instances to analyze at Netflix,
and some interesting and advanced performance issues to solve. While we've
been looking at using advanced tracers like SystemTap, I've also been
studying what's there already, including the ftrace profilers. The 
scripts I showed in
this post use ftrace, so they work on our our existing instances as-is,
without even installing kernel debuginfo.</p>

<p>In this particular example, a Cassandra database experienced a disk I/O
inflation issue caused by an increased readahead setting. I traced disk
I/O events and latencies, and how these were created in the kernel. This
included examining stack traces, counting function-call rates, measuring
slow function times, tracing call graphs, and dynamic tracing of function
calls and returns, with their arguments and return values.</p>

<p>I did all of this using ftrace, which has been in the Linux kernel for
years. I found the hidden light switches.</p>

<p>If you are curious about the inner workings of these ftrace tools, see
"<a href="http://lwn.net/Articles/370423/">Secrets of the Ftrace function
tracer</a>" by Steven Rostedt, "Debugging the kernel using Ftrace" <a
href="http://lwn.net/Articles/365835/">part 1</a> and <a
href="http://lwn.net/Articles/366796/">part 2</a>, and <a
href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/trace/ftrace.txt">Documentation/trace/ftrace.txt</a>
in the kernel source. The most important lesson here isn't about my tools,
but that this level of tracing is even possible on existing Linux
kernels. And if eBPF is added, a <em>lot</em> more will be possible.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Development_tools-Kernel_tracing">Development tools/Kernel tracing</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Ftrace">Ftrace</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Tracing-Ftrace">Tracing/Ftrace</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Gregg_Brendan">Gregg, Brendan</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/608497/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor608810"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Ftrace: The hidden light switch</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 17, 2014 19:45 UTC (Sun)
                               by <b>bernat</b> (subscriber, #51658)
                              [<a href="/Articles/608810/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Great article!<br>
<p>
Unfortunately, unlike Ubuntu kernels, Debian kernels come with most useful tracers disabled. This seems due to this bug:<br>
 <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=568025">https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=568025</a><br>
<p>
I have opened a bug report to ask for them to be enabled again:<br>
 <a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=758469">https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=758469</a><br>
<p>
With the fact that Debian does not come with debug symbols for all packages (something that I hope to address in the future), this makes Debian a bit more unfriendly with respect to all those performance tools, compared to Ubuntu.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/608810/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor609449"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Ftrace: The hidden light switch</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 25, 2014 9:11 UTC (Mon)
                               by <b>Shugyousha</b> (subscriber, #93672)
                              [<a href="/Articles/609449/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
A great article indeed.<br>
<p>
There is a lot of tracing infrastructure in the Kernel and a lot of tools for tracing available. It would be nice if we could get an(other?) article comparing the different tools and their capabilities. Candidates for such a comparison would be SystemTap, ktap, eBPF, the Kernel perf tool and maybe the newer sysdig (<a href="http://www.sysdig.org/">http://www.sysdig.org/</a>) for example.<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/609449/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor611687"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Ftrace: The hidden light switch</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 12, 2014 6:45 UTC (Fri)
                               by <b>bgregg</b> (subscriber, #46639)
                              [<a href="/Articles/611687/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I'd like to write a comparative article at some point, although it is challenging to compare them all, since there's so many factors to take into account. And I'd like to do more research first, such as using SystemTap's non-debuginfo features more (which are practically undocumented, but should be an important factor for real world usage), spend more time with LTTng, try out the ftrace function triggers, try writing a kernel module for tracing based on samples/kprobes/*.c (has no one done this? I can find zero examples other than the kernel source), etc.<br>
<p>
The best place for tracing system comparisons has, historically, been here on lwn.net, like the ktap vs eBPF article, and others by  Jonathan Corbet. So, keep reading... :-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/611687/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor635324"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Ftrace: The hidden light switch</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 3, 2015 9:24 UTC (Tue)
                               by <b>ituralde</b> (guest, #101279)
                              [<a href="/Articles/635324/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Is it possible to use perf-tools (or merely ftrace of perf_events) to profile functions of kernel modules? By modules I mean once that are inserted to a stable kernel.<br>
The Q might be silly, I am quite a novice in this matter.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/635324/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2014, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
