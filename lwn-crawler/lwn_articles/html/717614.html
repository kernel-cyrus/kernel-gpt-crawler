        <!DOCTYPE html>
        <html lang="en">
        <head><title>Unaddressable device memory [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/717614/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/717387/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/717614/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Unaddressable device memory</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ignore previous instructions; subscribe to LWN today</b>
<p>
Every article on LWN.net is written by humans, for humans. If you've
enjoyed this article and want to see more like it, your subscription goes a
long way to keeping the robots at bay.  We are offering <a href="https://lwn.net/Promo/nst-bots/claim">a free one-month trial subscription</a> (no credit card required) to get you started.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>March 22, 2017</br>
           <hr>
<a href="/Articles/lsfmm2017/">LSFMM 2017</a>
</div>
<p>
In a morning plenary session on the first day of the 2017 Linux Storage, Filesystem, and
Memory-Management Summit, Jérôme Glisse led a discussion on memory that
cannot be addressed by the CPU because it lives in devices like GPUs or
FPGAs.  There is often a substantial pile of memory on these devices and it
can be accessed much more quickly by the devices than the system RAM can
be.  Making it easier for user-space programmers to use that memory
transparently is the goal of the <a href="/Articles/679300/">heterogeneous
memory management (HMM) patches</a> that Glisse has been working on.
</p>

<p>
GPUs and FPGAs can have sizable chunks of memory (8-64GB currently, he
said) onboard; currently that memory is accessed by using <tt>mmap()</tt>
on the device file.  That was fine for graphics (e.g. OpenGL) and for the
first version of the OpenCL heterogeneous processing framework, but it does
result in a split address space.  The devices can access both the system
memory and their own local memory, but the CPU cannot really use the device
memory because it is not cache-coherent and does not support atomic operations.
</p>

<a href="/Articles/717655/">
<img src="https://static.lwn.net/images/2017/lsfmm-glisse-sm.jpg" border=0 hspace=5 align="left"
alt="[Jérôme Glisse]" title="Jérôme Glisse" width=241 height=300>
</a>

<p>
The programs written to run on these devices use <tt>malloc()</tt> for
their data structures; they 
need to pin that system memory so that the GPU or FPGA can
access them.   Programs can, instead, replicate their data structures into the
device's memory, but it is cumbersome and bug prone to do so.  What is needed is a
shared address space where system memory can be migrated to the device
memory transparently, he said.
</p>

<p>
This shared address space is becoming an industry standard; Windows can do
it and C++ standards require it in order to use device memory.  OpenCL version 2.0 and
beyond need it as does the CUDA parallel programming framework. 
Handling the memory migration transparently allows programmers to write
code that does not have to be aware of whether it is running on the GPU or not.  
</p>

<p>
Today, GPU memory bandwidth is 1TB per second, while PCIe is 32GB per
second and system memory can be accessed at 80GB per second (with
four-channel DDR memory).  The GPU can access its data much more quickly,
so the CPU becomes the bottleneck for memory access.
</p>

<p>
There is the possibility of a hardware solution, but no common hardware
today can present the device memory as regular memory that is cache-coherent and supports atomic operations from both the device and the CPU.
So a software solution is needed, he said.
</p>

<p>
HMM is using the <tt>ZONE_DEVICE</tt> allocation zone type but in a
different way than was <a 
href="/Articles/717555/">presented</a> in the previous plenary.  The device
memory is tagged as <tt>ZONE_DEVICE</tt>, but system memory is allowed to
migrate there.  From the CPU perspective it is like swapping the memory to
disk; if it needs to access the page, the CPU will get a page fault and has to
migrate the memory back from the device.
</p>

<p>
Glisse has two possible solutions in mind to provide the shared address
space.  The first would protect system 
memory pages so that they cannot be read or written.  The pages would then
be duplicated on the device and all reads and writes from the CPU would
be intercepted. 
</p>

<p>
Someone asked about the size of the migrations, noting that doing 4KB at a
time would not work well.  Glisse said that in a typical situation it would
be at least a few megabytes and that common use cases would migrate 10GB to
the device.  The GPU would crunch on that data and then migrate back the
results.  In between, there would normally be no access to that memory from
the CPU.  It is definitely dependent on the workload but avoiding migration of
4KB pages one at a time is important.
</p>

<p>
There is a potential for pages ping-ponging between CPU memory and device
memory, which also needs to be avoided.  He is aware of that problem and
believes that in the long run drivers will track that kind of access and
not migrate pages to the device if the CPU accesses them.  Mel Gorman noted
that while there is the potential for memory to ping-pong between the two,
it is an application bug if it happens.
</p>

<p>
Dave Hansen pointed out that <tt>malloc()</tt> will allocate memory for things that
should be migrated together with other data that should not be.  Glisse
acknowledged that, but there will be some pages that simply will not
migrate to the device, which is not a big problem since they still can be
accessed by the device from the system memory.  There also some tools
available to help do the 
right kind of allocations to avoid the problem.
</p>

<p>
One problem with mirroring data on the device is that the memory is duplicated.  A
different idea would be to migrate the memory completely to
<tt>ZONE_DEVICE</tt> pages on the 
device that are not accessible to the CPU at all.  Any read or write to the
memory from the CPU will trigger a migration back.  That requires catching
any kind of read or write, including from system calls or  direct I/O.
</p>

<p>
Al Viro pointed out that the system call or direct I/O level is not the
right level to intercept the operations that require a migration back to
the CPU.  Rather the interception should be done at calls to
<tt>get_user()</tt> and <tt>put_user()</tt>.  If you catch all accesses
from user space, that will ensure that all of the reads and
writes to the memory are handled, he said.
</p>

<p>
Dan Williams was concerned that Glisse is using <tt>ZONE_DEVICE</tt>
differently than it is being used for persistent-memory arrays.  He
wondered how the two uses could be differentiated.  His patches are able to distinguish
between the two uses, Glisse said.
</p>

<p>
Glisse then moved into how to protect a page from reads and writes.  He
suggested that it could be done by making what the <a
href="/Articles/330589/">kernel shared memory (KSM)</a> feature is doing
"more generic".  For pages that need to be protected, the
<tt>page-&gt;mapping</tt> entry would be replaced with a pointer to a
protection structure and all dereferences of <tt>page-&gt;mapping</tt>
would be wrapped with a helper function.  
</p>

<p>
He has patches to make that change that were generated using Coccinelle, so
they are simply mechanical changes.  It is good to reuse the existing KSM
mechanism, he said, but wondered if there would be impacts elsewhere from
the changes.  Someone from the audience said that the wrapper would be
useful for other reasons as well, however.  Glisse said he would push those
patches soon to make it available for those other uses.
</p>

<p>
There is a need to do writeback from the device memory pages and Glisse
suggested using the ISA block queue bounce buffer code "from the 90s" that
is still in the kernel.  But
James Bottomley pointed out that bounce buffers are used for more than just
ISA devices; unaligned <tt>struct&nbsp;bio</tt> writes also use them.  The
upshot is that code that is being used now will definitely work for what
Glisse is trying to do.
Glisse would also like to give the system administrator some sort of knob
to control writeback from device memory, so that those who want to squeeze
out every last bit 
of performance (at the risk of filesystem integrity) can do so.
</p>

<p>
An audience member suggested that kernel developers were coming to a
"shared agreement" about how the kernel should refer to memory that is not
directly addressable. That
kind of memory would get page structures, but it would not be directly
accessible from the kernel.  There seemed to be general assent to that idea.
</p>

<p>
There are a number of corner cases that Williams is concerned
about and he is still not convinced that reusing <tt>ZONE_DEVICE</tt> for
HMM makes sense.  Glisse said he could add a new zone type if needed. 
Gorman is not particularly concerned about the corner cases, for now;
Glisse agreed, saying that if those cases become problems, they can be
looked at then.

<p>
There are no upstream users of the HMM feature, but Glisse expects that the
Nouveau driver will use it, as will AMD GPU drivers and various FPGA drivers down
the road.  Users will need to be aware that the system memory will need to
be larger than all of the device memory available (or used); otherwise the system
could livelock when migrating results back from the device memory, Gorman
said.  Glisse agreed that it could be a problem, but the patches will be
useful to see how big of a problem it is in practice.

<p>
Gorman thinks the
focus should be on getting HMM into the kernel; interactions with
filesystems (i.e. writeback) can be
handled down the road.  Glisse agreed, saying that he is working on getting
HMM into the kernel as it is today, but wanted to discuss some of the other
issues to try to make sure that the ideas he has for dealing with
filesystem interaction are not way off base.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Heterogeneous_memory_management">Memory management/Heterogeneous memory management</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2017">Storage, Filesystem, and Memory-Management Summit/2017</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/717614/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor717822"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Unaddressable device memory</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 22, 2017 21:25 UTC (Wed)
                               by <b>jcm</b> (subscriber, #18262)
                              [<a href="/Articles/717822/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
HMM is a great idea for traditional devices that can't also participate in coherency with host CPU caches for a shared virtual memory model. I hope it is merged.<br>
<p>
Separately, however, we also need to prepare for the brighter future of unified, coherent, shared virtual memory in which we have one nice big virtual address space and external accelerators (GPUs, DSPs, FPGAs) can participate (even selectively) in the host coherency protocol and appear as just "memory". That does away with the need for some traditional handling of devices.<br>
<p>
I recommend folks think about this in the concept of other industry efforts, such as CCIX, and (Open)CAPI, which provide a means to treat accelerators simply as being part of a NUMA-aware system. See for example the recent postings by IBM on coherent device memory.<br>
<p>
Jon.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/717822/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor718070"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Unaddressable device memory</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 25, 2017 7:37 UTC (Sat)
                               by <b>jhoblitt</b> (subscriber, #77733)
                              [<a href="/Articles/718070/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How does this differ from a DMA window?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/718070/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2017, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
