        <!DOCTYPE html>
        <html lang="en">
        <head><title>AlacrityVM [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/345296/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/344261/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/345296/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>AlacrityVM</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>August 5, 2009</br>
           </div>
<p>
While virtualization has been a boon for many users and data centers, it
tends to suffer from performance problems, particularly I/O performance.
Addressing that problem is the goal of a newly <a
href="http://lwn.net/Articles/345016/">announced</a> project, <a
href="http://developer.novell.com/wiki/index.php/AlacrityVM">AlacrityVM</a>,
which has created a hypervisor based on KVM.  By shortening the I/O path
for guests, 
AlacrityVM seeks to provide I/O performance near that of "bare metal"
hardware. 
</p>

<p>
The project is in a "pre-alpha" stage, according to the web page, but it is
already reporting some fairly impressive results from a proof-of-concept network
driver.  Both for <a
href="http://developer.novell.com/wiki/index.php/Image:Netperf-10ge-tcp-throughput.jpg">throughput</a>
and <a
href="http://developer.novell.com/wiki/index.php/Image:Netperf-10ge-udp-latency.jpg">latency</a>,
the AlacrityVM guest performance compared favorably to that of 2.6.28 and
2.6.29-rc8 hosts.  It also clearly out-performed the virtio drivers in a
KVM guest. 
</p>

<p> The major change that allows AlacrityVM to achieve those gains come
from a new kernel-based virtual I/O scheme known as <a
href="http://developer.novell.com/wiki/index.php/Virtual-bus">Virtual-Bus</a>
(or vbus).  Currently, KVM guests use emulated devices&mdash;<a
href="http://developer.novell.com/wiki/index.php/Image:Kvm_interactions-emulation.jpg">implemented</a>
in user space by QEMU&mdash;in order to handle I/O requests.  That leads to
multiple 
kernel-to-user-space transitions for each I/O operation. The idea behind
vbus is to allow guests to directly access the host kernel driver, thus
reducing the overhead for I/O.</p>

<p>
Using vbus, a host administrator can define a virtual bus that contains
virtual devices&mdash;closely patterned on the Linux device
model&mdash;which allow access to the underlying kernel driver.  The 
guest accesses the bus through vbus guest drivers and will only be able to
use those devices that the administrator explicitly instantiates on
that vbus.  The vbus interface supports only two "verbs": <tt>call()</tt>
for synchronous requests, and <tt>shm()</tt> for asynchronous communication
using shared memory.
</p>

<p>
A <a
href="http://developer.novell.com/wiki/images/7/74/Vbus-introduction.pdf">document
[PDF]</a> by AlacrityVM developer Gregory Haskins describes
how to configure
and use vbus.  Vbus provides a sysfs interface that an administrator can
use to create container-like objects that will constrain
guests so that they can only access those devices specifically configured
for their use.  That helps alleviate one of the potential problems with
guests accessing kernel drivers more-or-less directly: security. 
</p>

<p>
The vbus web page has a look at the security issues and how they are
handled.  The main concerns are ensuring that guests cannot use the vbus
mechanism to escape their isolation from other guests and processes, as well
as making sure that guests cannot cause a denial of service on the host.
The bus can 
only be created and populated on the host side, 
and each lives in an isolated namespace, which reduces or eliminates the
risk of a cross-bus exploit to violate the isolation.  In addition, each task
can only be associated with one vbus&mdash;enforced by putting a vbus
reference in the task struct&mdash;so that a guest can only see the
device ids specified for that bus. 
</p>

<p>
Care was taken in the vbus implementation to punish guests for any
misbehavior, rather than the host.  The two areas mentioned are for guests
that, maliciously or otherwise, mangle data structures in the shared memory
or fail to service their ring buffer.  A na&iuml;ve implementation could
allow these conditions to cause a denial of service by stalling host OS
threads or by creating a condition that might normally be handled by a
<tt>BUG_ON()</tt>.  Vbus takes steps to ensure that the host to guest path
is resistant to stalling, while also aborting guests that write garbage to
the ring buffer data structures.
</p>

<p>
Haskins has <a href="http://lwn.net/Articles/345027/">posted</a> a series
of patches to add the vbus infrastructure, along with a driver for
accelerated ethernet.  So far, the patches seem to be fairly well-received,
though 
there are not, yet, very many comments.  The web page makes it clear
that the project's goal is "<q>to work towards upstream acceptance of
the project on a timeline that suits the community</q>".  The
flexibility shown in that goal should serve the project well in getting
mainline acceptance down the road.
</p>

<p> The project sums up its status and future plans on the web page as
well: "<q>we have a working design which includes the basic hypervisor,
linux-guest support, and accelerated networking. We will be expanding this
to include other areas of importance, such as accelerated disk-io, IPC,
real-time extensions, and accelerated MS Windows guest support.</q>" As
one might guess, the web page also has mailing lists for users and
developers as well as kernel and user-space git trees available for
interested folks.  </p>

<p>
AlacrityVM and vbus both look to be interesting projects, that are probably
worth investigating as potential virtualization solutions sometime in the
future.  The performance gains that come with vbus make it
likely to be useful to other projects as well.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#AlacrityVM">AlacrityVM</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Virtualization">Virtualization</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/345296/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor345592"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">AlacrityVM</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 6, 2009 7:56 UTC (Thu)
                               by <b>rahulsundaram</b> (subscriber, #21946)
                              [<a href="/Articles/345592/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Is this a series of patches on top of KVM or a fork of some sort? Seems to missing some more history or context. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345592/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345624"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">paravirtulised drivers?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 6, 2009 10:21 UTC (Thu)
                               by <b>alex</b> (subscriber, #1355)
                              [<a href="/Articles/345624/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I seem to remember a virtulisation session at OLS 2008 where the discussion was both Xen and KVM moving to common paravirtualised drivers for guests to speed up I/O. Isn't this basically the same thing?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345624/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345838"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">paravirtulised drivers?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 13:09 UTC (Fri)
                               by <b>ghaskins</b> (guest, #49425)
                              [<a href="/Articles/345838/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; I seem to remember a virtulisation session at OLS 2008 where the discussion was</font><br>
<font class="QuotedText">&gt; both Xen and KVM moving to common paravirtualised drivers for guests to speed</font><br>
<font class="QuotedText">&gt; up I/O. Isn't this basically the same thing?</font><br>
<p>
Its the same in terms of these are also PV drivers.  Its different in that this is much faster PV <br>
infrastructure than the currently deployed versions.  Ideally we will be able to use the same <br>
drivers and just swap out the inefficient part in the hypervisor side.  As of right now, the drivers <br>
are also different (venet vs virtio-net), but this may change in the future.<br>
<p>
In addition, it is also infrastructure that allows us to do new kinds of PV operations, such as <br>
supporting real-time guests.<br>
<p>
(Note: the graphs posted are against the virtio-net based PV drivers that were probably the <br>
result of that presentation you saw at OLS)<br>
<p>
-Greg<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345838/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor345643"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">AlacrityVM</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 6, 2009 13:12 UTC (Thu)
                               by <b>ghaskins</b> (guest, #49425)
                              [<a href="/Articles/345643/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The patches apply on top of KVM<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345643/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor345621"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">AlacrityVM</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 6, 2009 10:04 UTC (Thu)
                               by <b>dunlapg</b> (guest, #57764)
                              [<a href="/Articles/345621/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
And what kind of support does this require in the guest?  Is this another paravirtualized interface (along the lines of Xen's frontend/backend interface)?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345621/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345644"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">AlacrityVM</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 6, 2009 13:18 UTC (Thu)
                               by <b>ghaskins</b> (guest, #49425)
                              [<a href="/Articles/345644/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; what kind of support does this require in the guest?</font><br>
<p>
You load drivers in the guest for various IO subsystems (network, disk, etc)<br>
<p>
<font class="QuotedText">&gt; Is this another paravirtualized interface</font><br>
<p>
Yes, though it is not entirely orthogonal.  For instance, it is possible to tunnel existing PV protocols <br>
over it (e.g. virtio-net).  This means you swap out the low-level protocol (virtio-pci is exchanged <br>
for virtio-vbus) but the higher layer PV drivers (virtio-net, virtio-blk) remain unchanged.<br>
<p>
-Greg<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345644/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor345902"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 17:46 UTC (Fri)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/345902/">Link</a>] (14 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I'm surprised that although several virtualization implementations are looking at high-speed I/O that none of these is using the InfiniBand (IB) stack. With current IB hardware data rates of up to 2.4 GB/s are possible -- between different systems. This is because the IB stack has been designed for high throughput and low latency. Linux' IB stack already has implementations of networking and storage drivers. So implementing a single IB driver would allow virtualization software to reuse the drivers in the IB stack.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345902/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345912"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 18:04 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/345912/">Link</a>] (13 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
This is because the IB stack has been designed for high throughput and low latency.
</blockquote>
<p>
As opposed to what?  Doesn't every protocol seek to be fast?  Does IB make different tradeoffs than the alternatives?

      
          <div class="CommentReplyButton">
            <form action="/Articles/345912/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345915"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 18:37 UTC (Fri)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/345915/">Link</a>] (12 responses)
      </p>
      
      </div>
      </summary>
      > > This is because the IB stack has been designed for high throughput and low latency.<br><br>
> As opposed to what? Doesn't every protocol seek to be fast? Does IB make different tradeoffs than the alternatives?<br><br>

IB is a technology that comes from the supercomputing world and that has a higher throughput and a lower latency than any other popular storage or networking technology (IDE, SATA, 10 GbE, ...). Key features of IB are support for zero-copy I/O (RDMA) and the possibility of performing I/O without having to invoke any system call, even for user-space processes.<br><br>

Some impressive graphs can be found in this paper: <a href="http://www.cse.ohio-state.edu/~koop/pub/surs-hoti07.pdf">Performance Analysis and Evaluation of Mellanox ConnectX InfiniBand Architecture with Multi-Core Platforms</a>.<br><br>
Note: I'm not affiliated with any vendor of IB equipment.
      
          <div class="CommentReplyButton">
            <form action="/Articles/345915/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345936"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 20:31 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/345936/">Link</a>] (11 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
has a higher throughput and a lower latency than any other popular storage or networking technology (IDE [ATA], SATA, 10 GbE, ...)
</blockquote>
<p>
Nonetheless, ATA, SATA, and 10 GbE were all designed to have high throughput and low latency.  So one can't say that being designed for high throughput and low latency sets IB apart from them.
<P>
So what <em>does</em>?  Were the IB engineers just smarter?  Did they design for higher cost of implementation?  Did they design for implementation technology that wasn't available when the alternatives were designed?

      
          <div class="CommentReplyButton">
            <form action="/Articles/345936/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345942"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 7, 2009 21:05 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/345942/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
IB is faster and lower latency, but is significantly more expensive, has shorter cable length restrictions, (IIRC) many more wires in the cables (which make them more expensive and more fragile)<br>
<p>
IB was designed for a system interconnect within a rack (or a couple of nearby racks)<br>
<p>
ATA and SATA aren't general interconnects, they are drive interfaces<br>
<p>
10 GbE is a fair comparison for IB, but it was designed to allow longer cable runs, with fewer wires in the cable (being fairly compatible with existing cat5 type cabling)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345942/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor345986"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 7:24 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/345986/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
What you wrote above about cabling is correct but completely irrelevant in this discussion. What I proposed is to use the IB API's (RDMA) and software stack (IPoIB, SDP, iSER, SRP, ...) for communication between a virtual machine and the host system. In such a setup no physical cables are necessary. An additional kernel driver will be necessary in the virtual machine however that implements the RDMA API and allows communication between guest and host.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/345986/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346010"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 9:27 UTC (Sat)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/346010/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
if you are talking a virtual interface, why would you use either?<br>
<p>
define a driver that does page allocation tricks to move data between the client and the host for zero-copy communication. at that point you beat anything that's designed for a real network. <br>
<p>
then you can pick what driver to run on top of this interface, SCSI, IP, custom depending on what you are trying to talk to on the other side.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/346010/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346021"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 10:52 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/346021/">Link</a>] 
      </p>
      
      </div>
      </summary>
      As I wrote above, implementing an IB driver would allow to reuse a whole software stack (called <a href="http://www.openfabrics.org/downloads/OFED/">OFED</a>) and the implementation of several communication protocols. Yes it is possible to develop all this from scratch, but that is more or less like reinventing the wheel.
      
          <div class="CommentReplyButton">
            <form action="/Articles/346021/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor346812"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2009 4:54 UTC (Thu)
                               by <b>jgg</b> (subscriber, #55211)
                              [<a href="/Articles/346812/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
10GBASE-T is not compatible with Cat5e, it needs Cat6 cabling. It is also still a pipe dream, who knows what process node will be necessary to get acceptable cost+power. All 10GIGE stuff currently is deployed with CX-4 (Identical to SDR IB) or XFP/SFP+ (still surprisingly expensive).<br>
<p>
The big 10GIGE vendors are desperately pushing the insane FCoE stuff to try and get everyone to re-buy all their FC switches and HBAs since 10GIGe has otherwise been a flop. IB is 4x as fast, and 1/4th the cost of 10GIGE stuff from CISCO :|<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/346812/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor345988"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 7:42 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/345988/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      Yes, 10 GbE has been designed for low latency. But look at the numbers: the best performing 10 GbE interface today (Chelsio) has a latency of 3.8 us [1] while recent IB interfaces have a latency of 1.8 us [2]. The difference is small but it matters when communicating messages that are less than 64 KB in size. And IB interfaces do not cost more than 10 GbE interfaces that support iWARP.<br>
IB interfaces have a lower latency than 10 GbE interfaces because the whole IB stack has been designed for low latency while 10 GbE had to remain compatible with Ethernet.
<br>
<br>
References:
<ol>
<li><a href="http://www.chelsio.com/poster.html">Chelsio about The Cybermedia Center at Osaka University</a>.</li>
<li><a href="http://www.cse.ohio-state.edu/~koop/pub/surs-hoti07.pdf">Performance Analysis and Evaluation of Mellanox ConnectX InfiniBand Architecture with Multi-Core Platforms</a>.</li>
</ol>
      
          <div class="CommentReplyButton">
            <form action="/Articles/345988/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346007"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 9:24 UTC (Sat)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/346007/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
yes, IB has lower latencies than 10GB ethernet<br>
<p>
10G ethernet isn't low latencies at all costs. it benifits/suffers from backwards compatibility issues.<br>
<p>
it also allows for longer cable runs than IB<br>
<p>
it's not that one is alwaysbetter than the other, it's that each has it's use<br>
<p>
if you are wiring a cluster of computers and price is not an issue, then IB clearly wins<br>
<p>
if you are wiring a building, than IB _can't_ do the job, but 10G Ethernet can, so it clearly wins<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/346007/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346022"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 11:00 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/346022/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      As I wrote above, the physical limitations of IB are not relevant in the context of the AlacrityVM project. These limitations only apply to the physical layer of the IB protocol and not to the higher communication layers. By the way, the <a href="http://www.infinibandta.org/content/pages.php?pg=technology_download">InfiniBand Architecture Specification</a> is available online. And 
      
          <div class="CommentReplyButton">
            <form action="/Articles/346022/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor349766"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 29, 2009 12:18 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/349766/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      Note: there are already kernel drivers in the Linux kernel that use this concept for communication between a virtual machine and the hypervisor or another virtual machine. These drivers are ibmvscsi (initiator, runs in the virtual machine) and ibmvstgt (target, runs in the entity exporting the SCSI device). See also <a href="http://publib.boulder.ibm.com/infocenter/systems/scope/hw/index.jsp?topic=/iphat/iphatvirtualscsi.htm">Virtual SCSI adapters</a> for more information.
      
          <div class="CommentReplyButton">
            <form action="/Articles/349766/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor349773"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 29, 2009 17:13 UTC (Sat)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/349773/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <p>
You've given an example I actually know something about, so I can comment further.  You're talking about the mechanism used on IBM's System P processors (which come standard with virtual machines) to allow a server in virtual machine S to present a SCSI disk device to client virtual machine C.
<p>
The server in S bases its disk devices on real SCSI disk devices (e.g. it splits a 10G real SCSI disk into 5 2G disks, one for each of 5 client virtual machines), and the actual data transfer is conventional DMA done by the real SCSI HBA to the memory of C, using hypervisor facilities specifically designed for this I/O server VM application.
<p>
AFAICS the only infiniband-related part of this is SRP (SCSI RDMA (Remote DMA) Protocol).  SRP is how the program running in S initiates (by communicating with C) that DMA into memory C owns, much as a server at the end of an IB cable might set up to transmit data down the IB wire into the client's memory.
<p>
And as I recall, SRP is simple and not especially fast or low-latency -- just what anybody would design if he needed to communicate DMA parameters.  A person could be forgiven for just reinventing SRP for a particular application instead of learning SRP and reusing SRP code.

      
          <div class="CommentReplyButton">
            <form action="/Articles/349773/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor350289"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Virtualization and InfiniBand</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 2, 2009 8:02 UTC (Wed)
                               by <b>xoddam</b> (guest, #2322)
                              [<a href="/Articles/350289/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Wish *I* got a System P processor standard with *my* virtual machine!<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/350289/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor346027"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">High-performance I/O in Virtual Machines</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 8, 2009 11:31 UTC (Sat)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/346027/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      In the past research has been carried out about high-performance I/O in virtual machines running on the Xen hypervisor and in a system equipped with an InfiniBand HCA. See also Jiuxing Liu e.a., <a href="http://www.cc.gatech.edu/classes/AY2007/cs8803hpc_fall/papers/dk-vmmio.pdf"><em>High Performance VMM-Bypass I/O in Virtual Machines</em></a>, USENIX 2006, Boston.
      
          <div class="CommentReplyButton">
            <form action="/Articles/346027/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346811"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">High-performance I/O in Virtual Machines</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2009 4:43 UTC (Thu)
                               by <b>jgg</b> (subscriber, #55211)
                              [<a href="/Articles/346811/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The new IB HCAs when combined with the snazzy PCI express virtualization stuff let the guest safely talk directly to the hardware and this whole issue becomes fairly moot. I've heard some of the FCoE chips will be able to do the same thing too. Any serious deployment with shared storage will want to go that way.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/346811/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor346913"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">High-performance I/O in Virtual Machines</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2009 13:38 UTC (Thu)
                               by <b>mcmanus</b> (guest, #4569)
                              [<a href="/Articles/346913/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
"the snazzy PCI express virtualization stuff let the guest safely talk directly to the hardware"<br>
<p>
SR-IOV et al is really cool, but it still leaves open the problems of hairpin routing, firewall enforcement, etc where the alacrity approach really helps. There is talk of hardware answers to that too, but its further down the pipe. This kind of hardware is going to be substantially more expensive in the short term as well, so having more efficient answers in software is a plus for the ecosystem overall.<br>
<p>
Also interesting, not long after this article was published there was a patch made available to virtio (for pure kvm) that reduces the trips through userspace for that code too: <a href="https://lists.linux-foundation.org/pipermail/virtualization/2009-August/013525.html">https://lists.linux-foundation.org/pipermail/virtualizati...</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/346913/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2009, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
