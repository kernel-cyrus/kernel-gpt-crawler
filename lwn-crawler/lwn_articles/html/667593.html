        <!DOCTYPE html>
        <html lang="en">
        <head><title>Read-mostly research in 2015 [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/667593/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/667476/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/667593/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Read-mostly research in 2015</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>We're bad at marketing</b>
<p>
We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please <a href="/Promo/nsn-bad/subscribe">subscribe today</a> to help us keep doing that, and so
we don’t have to get good at marketing.
</blockquote>
<div class="GAByline">
           <p>December 16, 2015</p>
           <p>This article was contributed by Paul McKenney</p>
           </div>
[<i>Editor's note: this article was co-written by Paul McKenney and
Aravinda Prasad</i>].
<p>It has been just over one year since the last LWN article on
<a href="http://lwn.net/Articles/619355/">read-mostly research</a>.
However, it is good to see that there have been a number of
interesting papers since then related to RCU and other read-mostly
topics.
This article gives a quick summary of them.

<h4>Validation</h4>

<p>
Joseph Tassarotti (Carnegie-Mellon University), Derek Dreyer (Max Planck
Institute for Software Systems), 
and Viktor Vafeiadis
(also MPI-SWS) carried out a manual proof of correctness
of the quiescent-state-based reclamation (QSBR) variant of
<a href="http://liburcu.org">user-space RCU</a>.
This paper modeled <tt>rcu_dereference()</tt> with
C11 <tt>memory_order_acquire</tt> loads rather than the
<tt>memory_order_consume</tt> loads intended for that purpose.
However, given <a
href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0098r0.pdf">the
current parlous state of <tt>memory_order_consume</tt> [PDF]</a>,
this shortcut is quite understandable.
Their
paper
is titled &ldquo;<a
href="http://www.mpi-sws.org/~dreyer/papers/rcu/paper.pdf">Verifying
Read-Copy-Update in a Logic 
for Weak Memory [PDF]</a>&rdquo; and appeared in the
2015 Proceedings of the 36<sup>th</sup> annual ACM SIGPLAN conference
on Programming Language Design and Implementation.
Another researcher asked me if I felt that their assumptions were
adequate, and as previously reported
<a href="http://paulmck.livejournal.com/40129.html">here</a>,
I replied that, since they found no bugs, their assumptions clearly must
be unrealistic.
In the authors' defense, the proof is highly non-trivial, so the lack of bugs
was not due to lack of effort.

<p>
Iftekhar Ahmed of Oregon State University gave
<a href="https://linuxplumbersconf.org/2015/ocw/proposals/2751">a presentation</a>
to the Scalability microconference of the
<a href="https://linuxplumbersconf.org/2015/">2015 Linux Plumbers Conference</a>
describing a &ldquo;mutant&rdquo; technique for verifying test suites
that he is applying to Linux-kernel RCU.
Each mutant is a copy of the RCU code, but with a single change,
or &ldquo;mutation&rdquo;.
For example, a given statement might be deleted,
a comparison operator might be changed, or a constant might be changed.
The question is then whether <tt>rcutorture</tt> will complain about the
mutant.
Mutants that <tt>rcutorture</tt> does not complain about are said to have
&ldquo;survived&rdquo;.

<p>
Of course, some mutants will never produce a complaint.
For example, a mutation that removes a <tt>BUG_ON()</tt> or that changes
(say) &ldquo;<tt>while (1)</tt>&rdquo; to
&ldquo;<tt>while (2)</tt>&rdquo; produces a correct program.
Some manual inspection of surviving mutants is therefore required.
However, such manual inspection has been rewarded by the finding of
not one but two holes in <tt>rcutorture</tt>,
one of which was hiding a real bug&mdash;in Tiny RCU, believe it or not.
[Full disclosure: I am collaborating with Iftekhar and his professors,
Alex Groce and Carlos Jensen, and am co-author on a resulting
<a href="http://www.cs.cmu.edu/~agroce/ase15.pdf">paper [PDF]</a>.]

<p>
<div class="tlr">
<a name="Quick Quiz 1"><b>Quick Quiz 1</b>:</a>
Wait a minute!!!
How can you possibly create a logic expression that represents all
executions of a parallel program???
<br><a href="#qq1answer">Answer</a>
</div>

The past year also saw the release of version 5 of the University of Oxford
C Bounded Model Checker
(<a href="http://www.cprover.org/cbmc/">CBMC</a>), led by
Daniel Kröning.
CBMC takes a C program as input, and creates a logic expression that
takes the program's inputs as inputs, and that evaluates to <tt>true</tt>
if some combination of those inputs can trigger an assert or result
in an array-out-of-bounds condition.
This new version introduces multiprocessor capability (including
some weak-memory support), and is capable of
automatically verifying some trivial RCU implementations, including
the Linux kernel's
<a href="http://paulmck.livejournal.com/39343.html">Tiny RCU</a>.
It is early days for CBMC's multiprocessor feature, but its appearance
is a welcome development.


<h4>Using and implementing RCU</h4>

<p>
Mike Ash
<a href="https://www.mikeash.com/pyblog/friday-qa-2015-05-29-concurrent-memory-deallocation-in-the-objective-c-runtime.html">posted</a>
a description of an RCU-like primitive in Apple's Objective-C runtime.
Interestingly enough, this approach identifies read-side critical sections
via designated code ranges.
This of course means that it scans all CPUs' program counters in order to
identify grace periods.
This approach is interesting in being a distinct method of achieving
zero-overhead read-side critical sections, albeit one that poses some
interesting practical challenges for large read-side critical sections
that call many functions and for reliable sampling of other CPUs'
program counters without undue degradation of realtime response.

<p>

<div class="tlr">
<b>Editor's note</b>: for those wondering how an algorithm that takes locks
can be "lock-free," you're not alone.  According to Paul, in <a
href="https://en.wikipedia.org/wiki/Non-blocking_algorithm">common academic
usage</a>, "lock-free" means "at least one thread is guaranteed to make
progress."  Algorithms that do not take locks at all, instead, are "lockless."
</div>


Pedro Ramalhete and Andreia Correia took a much simpler approach,
using reader-writer locking to implement RCU, resulting in what they call
<a
href="https://github.com/pramalhe/ConcurrencyFreaks/blob/master/papers/poormanurcu-2015.pdf">poor-man's
URCU [PDF]</a>.
Although this is by no means the first lock-based implementation of RCU,
it is, as far as I know, the first lock-based implementation that boasts
lock-free readers.
Their trick is to use not one but two reader-writer locks.
Readers loop doing a read-side trylock on each of these two locks in
turn, exiting the loop upon successful acquisition of either of the
two locks.
Writers are serialized by another lock, and simply
write-acquire and write-release the two reader-writer locks in succession.
If a writer is delayed while write-holding one of these two locks, readers
can still make progress by acquiring the other lock.
Although poor man's RCU seems unlikely to become a production-quality
implementation of RCU, it is worth studying in its own right.
After all, it is not every day that someone achieves non-blocking
forward-progress guarantees with what is usually considered to be
a blocking primitive!

<p>
Maya Arbel and Adam Morrison, both of Technion, wrote a
paper
titled &ldquo;<a
href="http://www.cs.technion.ac.il/~mad/publications/ppopp2015-prcu.pdf">Predicate
RCU: An RCU for Scalable Concurrent Updates [PDF]</a>&rdquo;. 
This title might come as quite a surprise to those of us for whom
RCU has long provided eminently scalable updates.
However, Arbel and Morrison were working with an internal tree whose non-leaf
nodes can contain data, described
<a href="https://pdfs.semanticscholar.org/73e4/cd29273cf9d98d35bc184330e694ba798987.pdf">here [PDF]</a>.
It turns out that deleting an item from an internal tree is more complex
than in an external tree where only leaf nodes contain data.
This added complexity is especially vexing because RCU-protected readers
cannot be excluded, which is a particular problem when those readers
are finding the spot in which to do an insertion.
This interaction between complex deletions and
RCU lookup-insertions is handled by holding locks across grace periods, which
can degrade both performance and scalability.
Of course, holding locks across grace periods places those grace periods
on the critical path, which motivated the authors to work hard to reduce
grace-period duration, hence predicate RCU.

<p>
Predicate RCU allows
updaters to wait only on those readers that are involved with the update
in question, which should shorten grace periods.
This should in turn reduce the penalty for holding locks across
grace periods, however, there
is no free lunch:  Shorter grace periods mean fewer updates per grace period
and thus higher per-update overhead.
This effect is anything but subtle: a
<a href="https://www.usenix.org/conference/2004-usenix-annual-technical-conference/making-rcu-safe-deep-sub-millisecond-response">2004 USENIX paper</a>
notes that RCU has been observed satisfying more than 1,000 updates with a
single grace period while running an unremarkable workload.
Nevertheless, if you must hold a lock across a grace period, a shorter
grace period is going to be a good thing, as can be
seen in the Linux kernel's <tt>synchronize_net()</tt> function,
which uses <tt>synchronize_rcu_expedited()</tt> when called with
the networking layer's RTNL lock held.

<p>
<div class="tlr">
<a name="Quick Quiz 2"><b>Quick Quiz 2</b>:</a>
Given the performance penalties, shouldn't someone stop
researchers from holding locks across grace periods?
<br><a href="#qq2answer">Answer</a>
</div>

Therefore, given that Arbel and Morrison need to hold
locks across RCU grace periods, they need short grace periods.
To this end,
predicate-RCU readers associate themselves with an algorithm-specific value.
For example, readers of a hash table might associate themselves with the
lookup key, which would allow updaters to wait only for those
readers whose keys hashed to the hash bucket being updated.
This is in some ways similar to the Linux kernel's
<a href="http://lwn.net/Articles/202847/">SRCU</a>,
where the <tt>srcu_struct</tt> serves as the value associating readers
and updaters.
Either way, given that updaters only need to wait on a small subset of
the readers, one would expect grace periods to elapse more quickly,
which would in turn be expected to reduce the penalty for waiting for
a grace period while holding a lock.
Their performance results meet this expectation:
Although they do not achieve linear scalability,
shorter grace periods do improve their performance.
Of course, avoiding waiting for grace periods while holding locks would
likely improve performance and scalability even more!


<p>
Developers who assume that academics ignore their work will be happy
to see that this paper cites a couple of LWN articles.

<p>
Yujie Liu (Lehigh University), Victor Luchangco (Oracle Labs),
and Michael Spear (also Lehigh) wrote a
2013 paper
titled &ldquo;<a
href="http://www.cse.lehigh.edu/~spear/liu_icdcs_2013.pdf">Mindicators: A
Scalable Approach to Quiescence [PDF]</a>&rdquo;. 
This paper presses the
<a
href="http://dl.acm.org/citation.cfm?id=1281106&amp;dl=ACM&amp;coll=DL&amp;CFID=724480715&amp;CFTOKEN=99444769">scalable
non-zero indicator (SNZI) [$PDF]</a> technique
into service as a grace-period mechanism, and compares it to
several other approaches.
The intent appears to be to use this mechanism to implement
<a href="https://en.wikipedia.org/wiki/Transactional_memory">transactional memory (TM)</a>,
which also appears to require low-latency grace periods,
in part courtesy of TM's linearizability requirements, which in
turn seems to limit scalability.
They do call out the relationship to RCU.

<p>
Alexander Matveev (MIT), Nir Shavit (MIT and Tel-Aviv University),
Pascal Felber (University of Neuch&acirc;tel),
and Patrick Marlier (also University of Neuch&acirc;tel)
recently published a
<a href="http://www.ssrc.ucsc.edu/sosp15/index.html">Symposium on Operating Systems Principles</a> (SOSP)
paper
titled &ldquo;<a
href="http://sigops.org/sosp/sosp15/current/2015-Monterey/printable/077-matveev.pdf">Read-Log-Update:
A Lightweight Synchronization Mechanism 
for Concurrent Programming [PDF]</a>&rdquo;, which can be thought of as
a software transactional memory (STM) extension that includes explicitly
marked read-only transactions.
However, unlike RCU readers, these read-only transactions are guaranteed
to see a point-in-time snapshot of the union of all RLU-protected data
structures across multiple traversals.
Of course, this
does impose significant additional overhead on RLU updaters, as they
acknowledge in their Figure&nbsp;7, which shows RLU updaters being 2-5
times slower than RCU updaters.
That said, this figure shows a benchmark that favors RCU rather heavily.
With or without the point-in-time snapshots,
I believe that their realization of the importance of explicitly marking
read-only operations is a great step forward.
Updates to shared variables are also explicitly marked, providing
performance benefits over pure STM similar to those of
<a
href="http://infoscience.epfl.ch/record/136702/files/pldi127-dragojevic.pdf">SwissTM
[PDF]</a>.

<p>
<div class="tlr">
<a name="Quick Quiz 3"><b>Quick Quiz 3</b>:</a>
If RCU does not provide readers a guaranteed consistent snapshot of the
data structure, how can anyone successfully use it?
<br><a href="#qq3answer">Answer</a>
</div>

Their technique scales reasonably well up to 16 CPUs, however, this is a
very small system for modern non-mobile workloads.
They do have one graph (uppermost graph in Figure&nbsp;8) that goes up
to 80 CPUs, but this shows poor scalability.
My first thought was that this poor scalability was due to their single
global counter that is atomically incremented on each update, but this
did not make sense because RLU is outperforming RCU.
Instead, the culprit seems to be
the need to hold locks across grace periods.
Because RCU optimizes for low per-update overhead at the expense of
grace-period latency, and because <tt>synchronize_rcu()</tt> was used
(instead of <tt>call_rcu()</tt> or <tt>synchronize_rcu_expedited()</tt>),
holding locks across grace periods hurts RCU even more than it hurts RLU.
Once again, I recommend either releasing locks before waiting for
grace periods or using the asynchronous <tt>call_rcu()</tt> primitive:
Both approaches avoid degraded scalability.
In (thankfully rare) Linux-kernel cases where it is absolutely necessary
to wait for grace periods while holding a mutex, the
<tt>synchronize_rcu_expedited()</tt> APIs can be used,
though these are not particularly good for realtime applications
(with the exception of <tt>synchronize_srcu_expedited()</tt>).

<p>
Their performance testing includes both user-space and Linux-kernel scenarios.
In the Linux-kernel scenarios shown in Figure&nbsp;9, their list-traversal
code beat that of the Linux kernel by a surprisingly large margin.
In an impressive display of good sportsmanship, one of the authors
(Marlier) located the Linux-kernel performance bottleneck and <a
href="http://git.kernel.org/linus/8db70b132dd57696cfc7560203a72e90c51bfdda">submitted 
a fix</a> that causes the Linux kernel's lists to outperform those of the paper.
The problem was a single non-atomic store and load to an unshared location
in the running task's stack, with no memory barriers.
It appears that current microprocessors' pipelines can be a bit slow to
handle a load from a location that the current CPU just stored to.
Patrick eliminated this store and load, and his patch was accepted
during the v4.4 merge window.


<p>
As far as I know, this is the first academic use of the
<tt>rcutorture</tt> test suite to test an alternative RCU-like
implementation, which is a nice milestone.  Those wanting some more
detailed discussion on RLU, including graphics showing scalability issues
on larger systems, can find it <a href="/Articles/667720/">on
this page</a>.
<p>
At the end of the paper, the authors express hope that RLU will be
used both in kernel and in user-space programs.
This of course raises the question of what situations would be best
suited for an RLU solution.
Two possibilities come to mind:

<div class="tlr">
<a name="Quick Quiz 4"><b>Quick Quiz 4</b>:</a>
Shouldn't RLU be tried on complex RCU uses such as the Linux kernel's
VFS dentry cache-walk code?
<br><a href="#qq4answer">Answer</a>
</div>


<ul>
<li>	Situations where RCU readers are used in conjunction with
	sequence locking.
	These situations are already paying the complexity and
	performance costs of retries, so these RLU disadvantages
	might be less of a problem in this case.
<p>
<li>	Situations where a problematic reader-writer lock
	has proven difficult to convert to RCU.
	Perhaps RLU's snapshotted readers might better handle some of
	these situations.
</ul>

<p>
That said, I strongly suspect that successful application of RLU will
require the authors to carefully separate those semantics that are
actually required from those semantics that are merely fashionable.

<p>
It is easy to get irritated at academics' insistence on linearizability,
given the large performance and scalability penalties they pay to
achieve it.
On the other hand, they do use deferral to improve performance of RLU, and
perhaps further work along these lines will persuade them to let
go of linearizability.

<p>
Frans Kaashoek presented on the
<a
href="http://sigops.org/sosp/sosp15/history/08-kaashoek-slides.pdf">history
of parallelism and operating systems [PDF]</a>
during the
<a href="http://sigops.org/sosp/sosp15/history/index.html">SOSP 2015 History Day Workshop</a>
to mark SOSP's 50<sup>th</sup> anniversary.
Frans devoted a full slide of a 33-slide deck to RCU, which should be a
point of pride for the many developers who have applied RCU
within the Linux kernel.
(Yes, I am happy on behalf of my work on the RCU infrastructure, but let's
face it, the infrastructure is profoundly uninteresting without its many
uses.)

<p>
Peter Denning also presented in SOSP 2015 History Day, but on
<a href="http://sigops.org/sosp/sosp15/history/01-denning-slides.pdf">OS
Foundations [PDF]</a>.
Those who know me will not be surprised to hear that Peter's last bullet
on his slide 39 resonated with me: &ldquo;Theory follows practice&rdquo;.

<p>
So what have I been doing?
For my part, I have continued my work enabling complex atomic
updates to RCU-protected data structures with minimal copying, which
I recently
<a
href="http://www.rdrop.com/users/paulmck/scalability/paper/Updates.2015.01.16b.LCA.pdf">presented
[PDF]</a>
at
<a href="http://linux.conf.au/">linux.conf.au</a>.
This round dealt with some bottlenecks in user-mode memory allocators,
relearning the lesson that Glibc <tt>malloc()</tt> is not at all
scalable.
I have also been working to repair C11's and C++11's
<tt>memory_order_consume</tt> feature, which just might be nearly done.
I rewrote the Linux kernel's <tt>synchronize_rcu_expedited()</tt> and
<tt>synchronize_sched_expedited()</tt> to reduce OS jitter and, for
good measure, added kernel parameters to suppress expedited grace
periods entirely.
I also started work on design documentation for the Linux kernel's RCU
implementation,
starting with <a href="/Articles/652156/">its requirements</a>.
Finally, I have upgraded rcutorture in an attempt to keep up with
the Linux kernel's huge installed base.

<h4>Conclusions</h4>

<p>It is very good to see continued academic interest in read-mostly
techniques such as RCU.
I very much hope that the mutual learning process continues,
and that it benefits Linux developers and their users!

</p><h4>Acknowledgments</h4>

<p>We all owe thanks to Srivatsa Bhat, K. Gopinath,
Orran Krieger, Pedro Ramalhete, Mike Ash, Derek Dreyer,
Joe Tassarotti, Iftekhar Ahmed, Adam Morrison, and Hagit Attiya
for helping to make this human-readable.
I am grateful to Jim Wasko for his support of this effort.



<h4><a name="Answers to Quick Quizzes">
Answers to Quick Quizzes</a></h4>

<a name="qq1answer"></a>
<p><b>Quick Quiz 1</b>:
Wait a minute!!!
How can you possibly create a logic expression that represents all
executions of a parallel program?


</p><p><b>Answer</b>:
CBMC converts the parallel program into
<a href="https://en.wikipedia.org/wiki/Static_single_assignment_form">single static assignment (SSA)</a> form,
which results in each variable having a separate instance for each
assignment to it.
Each read from that variable is then wired up to one of the instances that
it might have read its value from, and all combinations are represented in the
resulting logic expression.

<p>
Of course the resulting logic expression might be quite large, but
modern computers and modern
<a href="https://en.wikipedia.org/wiki/Boolean_satisfiability_problem">SAT solvers</a>
have grown quite capable.
For example, in 1990, a world-class SAT solver might be able to handle
100 variables, that is, three 32-bit variables with four bits left over.
In contrast, in 2015, I have solved a 1.8 million variable SAT problem
on my laptop.

<p>
The solution took a full ten seconds.


</p><p><a href="#Quick%20Quiz%201"><b>Back to Quick Quiz 1</b>.</a>

<a name="qq2answer"></a>
<p><b>Quick Quiz 2</b>:
Given the performance penalties, shouldn't someone stop
researchers from holding locks across grace periods?


</p><p><b>Answer</b>:
No.

<p>
First, please note that holding locks (or, in the Linux kernel, mutexes)
across grace periods makes perfect sense in some cases, for example,
in the <tt>synchronize_net()</tt> example given earlier.
In addition, in some cases, holding locks across grace periods
on rarely executed slow paths can greatly reduce complexity.
Second, even though I recommend strongly against holding locks
across grace periods on fast paths, it is quite possible that researchers
exploring this technique will nevertheless come up with something useful.
That said, I do indeed suspect that they would make better progress
if they moved their grace periods outside of locks.
Third, waiting for highly optimized grace periods is not likely to
be a big problem on small systems.
(Pretty amazing that a 64-CPU system can now be considered
&ldquo;small&rdquo;, isn't it?)
Fourth and finally, it is their job to figure out what they work on.
I would no more wish to dictate what they do than I would wish them to
dictate what I do.


</p><p><a href="#Quick%20Quiz%202"><b>Back to Quick Quiz 2</b>.</a>

<a name="qq3answer"></a>
<p><b>Quick Quiz 3</b>:
If RCU does not provide readers a guaranteed consistent snapshot of the
data structure, how can anyone successfully use it???


</p><p><b>Answer</b>:
It turns out that RCU does in fact guarantee a consistent
snapshot&mdash;at zero cost&mdash;in some important special cases.
Perhaps the most common case is a data structure where updaters are
restricted to adding new elements or removing old ones, in other words,
where in-place updates are prohibited.
If each reader only ever looks up a single element, then readers
will automatically see a consistent snapshot of that single element&mdash;even
if a given reader looks up a different element each time it executes.

<p>
There are many other use cases where RCU provides consistent snapshots
for free, and quite a few of them may be found in the Linux kernel.
However, it is also the case that consistency guarantees are overrated.
After all, the finite speed of light sharply limits the degree of
consistency that a given data structure can usefully maintain with
its corresponding external real-world state.
Given that the whole point of many data structures is to track
external state, internal consistency all too often becomes nothing
more than an expensive fiction.


</p><p><a href="#Quick%20Quiz%203"><b>Back to Quick Quiz 3</b>.</a>

<a name="qq4answer"></a>
<p><b>Quick Quiz 4</b>:
Shouldn't RLU be tried on complex RCU uses such as the Linux kernel's
VFS dentry cache-walk code?


</p><p><b>Answer</b>:
That might well be the case.
I therefore have already pointed one of the authors (Matveev) at Neil
Brown's excellent LWN series
<a href="http://lwn.net/Articles/649115/">here</a>,
<a href="http://lwn.net/Articles/649729/">here</a>, and
<a href="http://lwn.net/Articles/650786/">here</a>.
Matveev's initial response was as follows:
&ldquo;The dcache/dentry + RCU + various locks is really a complex
structure... &rdquo;
I take this as a good sign, in that Matveev should not be blinded by
overconfidence.


</p><p><a href="#Quick%20Quiz%204"><b>Back to Quick Quiz 4</b>.</a>


</p><p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Read-copy-update">Read-copy-update</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#McKenney_Paul_E.">McKenney, Paul E.</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/667593/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor668602"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 19, 2015 16:54 UTC (Sat)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/668602/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Wonderful article, as usual. Lots of interesting papers to read through over Christmas! :)<br>
<p>
It is, at least, now *possible* to fix glibc malloc. Before a few years ago, all but the most trivial changes were precluded by the existence of malloc_get_state()/malloc_set_state(), which was used by exactly one program: Emacs. (In theory it could be used by other dumpy programs like Lisp interpreters, but in practice it was just Emacs). Now Emacs is the single biggest consumer of memory on most of my machines but even *I* think this is ridiculous: I'm glad it's changed, and Emacs now uses its own rather worse malloc() but frees up glibc's for future improvement :)<br>
<p>
(aside: while I was most interested in the existence-bits stuff in the lca slides, I will not be able to unsee slide 16. As far as I'm concerned, all the dataflows on my machines' buses now have angry faces painted on the front of them.)<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668602/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668610"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 19, 2015 18:22 UTC (Sat)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/668610/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Glad you liked it!  And glad to hear that glibc malloc() now has the potential to be fixed!<br>
<p>
On your aside, do systems with angry data run faster?  ;-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668610/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668630"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 20, 2015 1:06 UTC (Sun)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/668630/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Systems with angry data run hotter. (This is the real reason for the heat problems plaguing CPU designers these days -- insufficiently placated electrons.)<br>
<p>
The existence-bits stuff -- I was very proud, ten years ago now, of implementing what I now realise was about a third of that idea, only much less well thought out, less general, and doing just enough for the one data structure I needed it for. But that was a different employer and not open source and I no longer work there, thank goodness, so I *cannot fix it* to use your idea and this burns my soul.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668630/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668661"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 20, 2015 18:23 UTC (Sun)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/668661/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      Interesting!  So perhaps significant energy savings would accrue if the many organizations processing world news focused on feel-good stories?  And who would have guessed that the flood of too-cute cat photos on various social-media sites might be solving an important global problem!!!

<p>In your defense, there is a lot to be said for doing only that which is needed for the job at hand.  And your experience is encouraging in that I was simply solving a stated problem with no assurance that the solution would have any practical value.  I am guessing that you are constrained from sharing any details about the data structure in question, but if you can talk about it, I suspect that others would also be interested.

<p>And I bet that there is a <b>lot</b> more that can be done with existence structures!
      
          <div class="CommentReplyButton">
            <form action="/Articles/668661/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668807"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 21, 2015 23:06 UTC (Mon)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/668807/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I can share details because I cannot imagine that anyone much cares (believe me, it was not business-critical and in fact with modern memory hierarchies is probably a bad idea!)<br>
<p>
I was optimizing a hash table based on a splay tree variant: we had very large hash tables accessed by hundreds of threads simultaneously, but where only a few dozen entries were really frequently accessed, and I wanted stable iteration order, so rebalancing a bucket hash became monstrously difficult (I hate rebalancing anyway, it's hard to debug and faults in it lead to horrible intermittent bugs). I could have "cached the cache" with a subsidiary per-CPU hash, but this was in userspace so per-CPU stuff gets really rather difficult. I could have done per-accessor caches, but that makes cache invalidation a pig. So the idea was to use a binary tree (keyed by the hash function output, of course), rotating entries closer to the top of the tree on every access, making the next access faster. But multiple concurrent accesses were routine, and obviously if you want low latencies avoiding lock contention is even more important, and these hash tables were big enough that locks at every node were completely impractical: so I used something very similar to your existence structures (I called it a 'tie' because it 'tied down' a node for as long as it was there) to hold nodes into a consistent state, still accessible via their old route, while the rotation was going on, then release them later, once it was certain that no lookups that might have started before the rotation were still underway: each accessing thread had a pool of pre-prepared existence structures, so I could set them up with a couple of pointer assignments: necessary, because even internal nodes between the two rotation points needed to be tied down this way. One access to a rarely-used node could trigger dozens of ties.<br>
<p>
At the time I didn't know about RCU -- when I learned about it I felt like a right idiot :) you'll notice I even had grace periods, though I didn't call them that and they were totally tied to the implementation of my hashing machinery (I had a reference counter of live accesses in the hash *, the owner of the root node, and my "grace periods" ended when the count fell to zero).<br>
<p>
In hindsight this was probably a terrible data structure: it was very write-heavy (every read of something not near the top of the tree triggered multiple writes) and write-heaviness is bad on modern machines. But, hey, it worked! (And previous approaches, that used locks rather than the tie approach, were far slower). One major benefit from my perspective was that every conditional in the implementation was more or less guaranteed to have both branches executed quite often -- unlike a rebalancing approach, there were no big blocks of rarely-executed code in this one for bugs to build up in.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668807/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668840"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 22, 2015 6:10 UTC (Tue)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/668840/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I agree that modern systems are much less forgiving of frequent concurrent writes than was older hardware, but your approach does sound quite appropriate for the hardware at hand back in the day.  So good show, and thank you for the info!<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668840/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor668777"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 21, 2015 17:24 UTC (Mon)
                               by <b>blackwood</b> (guest, #44174)
                              [<a href="/Articles/668777/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Doesn't the poor-man's URCU have a fairness problem? I have no idea whether that's part of the academic lock-free definition, but for catastrophic amounts of bad luck where a reader just always trylocks the lock a writer is holding right at that moment, and gets stalled in-between, it won't make forward progress. And even when the underlying locking primitve guarantees some form of fairness that won't help, since the new locking primitive is implemented using trylocks on top of whatever's there. To fix this there would need to be a new "trylock 2 locks at once, acquire at most 1" primitive, and I have no idea how that could even be implemented without some form of retrying using existing cpu instructions.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668777/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668778"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 21, 2015 17:29 UTC (Mon)
                               by <b>andresfreund</b> (subscriber, #69562)
                              [<a href="/Articles/668778/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There's wait-free (simplified: each thread is guaranteed to make progress), lock-free (at least one thread makes progress) and obstruction free (the system as a whole can progress if some thread is stuck at any point). It sounds like you're looking for "wait-free"?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668778/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668781"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 21, 2015 18:41 UTC (Mon)
                               by <b>blackwood</b> (guest, #44174)
                              [<a href="/Articles/668781/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yeah I guess lock-free indeed has a very unusual (to me) definition in academics. Doesn't the "at least one thread makes progress" trivially hold if you never block while holding the lock? Of course that doesn't work in userspace where the scheduler can kick you off the cpu anytime it feels like, but in the kernel spin_lock_irq seems to fit this. Probably some twist somewhere, so is there some reading guide/canonical definitions of this stuff available somewhere?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668781/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor668787"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Read-mostly research in 2015</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 21, 2015 20:24 UTC (Mon)
                               by <b>andresfreund</b> (subscriber, #69562)
                              [<a href="/Articles/668787/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Yeah I guess lock-free indeed has a very unusual (to me) definition in academics</font><br>
<p>
Note I'm not an academic, nor really an expert in this stuff. I'm reading relevant literature because it's getting more an more relevant for stuff I work on.<br>
<p>
From reading papers or seems that the exact definitions change between papers, and not just in the phrasing.<br>
<p>
It sounds a bit like your  definition is what the article's note defined as lockless?<br>
<p>
<font class="QuotedText">&gt; . Doesn't the "at least one thread makes progress" trivially hold if you never block while holding the lock?</font><br>
<p>
Well, that was just the gist of the definition ;) I think it usually includes a) a fixed limit of the number of steps until the operation finished b) a  somehow stuck participant may nor lock up the system.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/668787/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2015, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
