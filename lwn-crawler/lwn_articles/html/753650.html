        <!DOCTYPE html>
        <html lang="en">
        <head><title>A mapping layer for filesystems [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/753650/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/753394/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/753650/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>A mapping layer for filesystems</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Please consider subscribing to LWN</b>
<p>
Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit
<a href="/Promo/nst-nag1/subscribe">this page</a> to join up and keep LWN on
the net.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>May 9, 2018</br>
           <hr>
<a href="/Articles/lsfmm2018/">LSFMM</a>
</div>
<p>
In a plenary session on the second day of the Linux Storage, Filesystem,
and Memory-Management Summit (LSFMM), Dave Chinner described his ideas for
a virtual block address-space layer.  It would allow "space accounting to be
shared and managed at various layers in the storage stack".  One of the
targets for this work is for filesystems on thin-provisioned devices, where
the filesystem 
is larger than the storage devices holding it (and administrators are
expected to add storage as needed); in current systems, running out of
space causes huge problems for filesystems and users because the filesystem
cannot communicate that error in a usable fashion.
</p>

<p>
His talk is not about block devices, he said; it is about a layer that
provides a managed logical-block address (LBA) space.  It will allow user
space to make <tt>fallocate()</tt> calls that truly reserve the space
requested.  Currently, a filesystem will tell a caller that the space was
reserved even though the underlying block device may not actually have
that space (or won't when user space goes to use it), as in a
thin-provisioned scenario.  He also said that he would not be talking about
his ideas for a snapshottable subvolume for XFS that was the subject of his
<a href="/Articles/747633/">talk at linux.conf.au 2018</a>.
</p>

<p>
The new layer will provide the address space, which is a representation of
an LBA range.  There will be a set of interfaces to manage the backend
storage for that range.  A filesystem will usually be the client of the
interface, while a block device or a separate filesystem can be the supplier of the
storage for the layer.
</p>

<a href="/Articles/753664/">
<img src="https://static.lwn.net/images/2018/lsf-chinner-sm.jpg" border=0 hspace=5 align="right"
alt="[Dave Chinner]" title="Dave Chinner" width=212 height=300>
</a>

<p>
The filesystem does not treat the virtual block address layer any
differently than 
it does a block device from a space-management perspective.  The supplier
provides allocation and space 
reservation; it could also provide copy-on-write (CoW) to the upper layer,
which would allow for snapshots at that level.
In order to read and write data, however, a mapping must be done to turn
the virtual LBA into a real LBA and block device for the I/O.  It is
similar to the export blocks feature of Parallel NFS (pNFS).
</p>

<p>
When the client wants to do I/O, it first maps the virtual LBA, then does
the operation directly to the block device where the data is stored.  Jan
Kara asked if it is simply a remapping layer for filesystems; Chinner
agreed that it was.  He was looking at adding this ability to XFS but
realized it was more widely applicable.  It is similar to what is done for
loopback devices, but he has chopped some layers out of that; instead of going
through the block device interface, it is going through the remapping layer.
</p>

<p>
One of the problems with space reservation is that there may be a delay
between the write of data and its associated metadata.  But
it is important that space reserved for that metadata does not disappear
when it comes time to write the metadata.  The upper layer filesystem needs
 to be able to ensure that a later writeback does not get an
<tt>ENOSPC</tt> error for something that it believes it can write.
</p>

<p>
Under this new scheme, the filesystem can ask the supplier for a
reservation, which will result in an opaque cookie that the filesystem can use to
indicate portions of the reservation.  Every object modification has the
cookie associated with it; when all of those modifications are done, the
reference count on the cookie drops to zero and any extra reservation goes
back to the backend.
</p>

<p>
This allows allocation based on the I/O that the filesystem is building.
It also can allow for write combining that is optimal for the
thin-provisioned devices.  Overall, it allows for optimal I/O for the
underlying structures, he said.
</p>

<p>
The client does not know anything about what the underlying backing store
actually does.  Similarly, the supplier does not know what the client is
doing; it is just allocating and mapping.  The idea is just to create an
abstraction that allows two different layers in the stack to manage blocks
in a way that can report errors properly.
</p>

<p>
When the BIO is formed for a read operation, the filesystem does
everything it does now, but it also calls out to the mapping layer to find
out which block device to do the read on.  It will issue I/O directly to
the underlying device, taking a shortcut around all of the layers that a
loopback 
device would use, he said.
</p>

<p>
A write operation would use a two-phase write that is similar to what XFS
uses for direct I/O.  It would get the block device and LBA from the
mapping layer and it would also attach any needed reservation cookies to the BIO.
If the target area is a hole, the system first allocates for those
blocks; if it is a CoW supplier, it allocates new blocks and returns the
mapping and reservation for those.  All of that behavior would be hidden in
the lower layers.  The BIOs are built and sent down to the block device;
when the write completes, the supplier must run its completion routines
first, then the client runs its completions to finish its two-phase write. 
</p>

<p>
At no time does the client know anything about what the underlying backing
store actually does, Chinner reiterated.  Similarly, the supplier does not know
what the client is actually doing; it simply handles allocation and
mapping.  Anything that can provide a 64-bit address space can be used as a
supplier, a file could be used, for example.
</p>

<p>
It is an abstract interface, he said, that is not specific to any
filesystem or block device.  It could be ext4 as a client with XFS as a
supplier, or vice versa if ext4 implements the supplier interface.  Ted
Ts'o said that he originally thought this was all simply targeting thin
provisioning, 
but having filesystems as the supplier "becomes interesting"; "that's
neat". 
Chinner said his actual motivation was for XFS subvolumes, not thin
provisioning. 
</p>

<p>
The problem has turned out to be fairly simple to solve.  It is about 1700
lines of code right now and he thinks it will grow to 3000 or so once he
gets it cleaned up and ready for posting.  He does think it will be
interesting for other filesystems.  Kara said that it resembled some things
that Btrfs does; Chinner agreed, he is not really doing anything new, but
is simply "repackaging and reimagining" ideas that are already out there.
</p>

<p>
One of the reasons he likes this approach is that it reuses the
infrastructure already available in the filesystem layer.  It can turn
snapshots into regular files, for example.  Chris Mason said that he uses
loopback devices for some containers, but that this mechanism would be
better.  Chinner acknowledged that and noted that he has some "wild plans"
for page-cache sharing that will make it even better.  There are lots of
use cases, he said, so will get his act together and post patches soon.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer">Block layer</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2018">Storage, Filesystem, and Memory-Management Summit/2018</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/753650/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor754031"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2018 17:06 UTC (Wed)
                               by <b>Cyberax</b> (<b>&#x272D; supporter &#x272D;</b>, #52523)
                              [<a href="/Articles/754031/">Link</a>] (8 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I never understood the need for thin provisioning. It's a freaking kludge with huge infrastructure costs across multiple layers.<br>
<p>
We already have LVM. So just use it as a substrate for a filesystem, monitor its free space and extend the filesystem as needed. ext4 and xfs both support online expansion.<br>
<p>
If free space/volumes in the LVM array are running low, simply add more. No need for any kernel-level features and can be implemented entirely in userspace.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754031/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor754060"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2018 0:31 UTC (Thu)
                               by <b>Paf</b> (subscriber, #91811)
                              [<a href="/Articles/754060/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Err, really?  Thin provisioning of storage for VMs is incredibly powerful and flexible and not a kludge.  Not everything is ext4 or XFS, and thin provisioning allows VM admins to not interact with the VMs themselves (for example, the VMs can stay up!).  Also, I don’t necessarily have root on those VMs, and I sure as hell don’t want to have it.<br>
<p>
Are you talking about some more specific instance of thin provisioning?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754060/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor754062"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2018 0:56 UTC (Thu)
                               by <b>ringerc</b> (subscriber, #3071)
                              [<a href="/Articles/754062/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thin provisioning is really useful in larger infrastructures and teams.<br>
<p>
"Just add more storage when you need it" is easy when you're managing individual servers and you're responsible for the storage and the server.<br>
<p>
It quickly becomes unfeasible when you're running an infrastructure where people can request (often virtual) servers and run them for $WHATEVER. The server is their problem. They may have one, they may be part of a team running hundreds or thousands of them. Numerous small sysadmin teams doesn't work well and leads to low server utilisation, excessive divergence in tooling and SOE between teams, overlapping software licensing spends, and more. So your organisation has moved toward centralising the underlying server infrastructure with virtualisation and SAN storage to improve utilisation, reduce hardware and power costs, and make staffing more efficient.<br>
<p>
As a result you have this big SAN with, say, 50TB of storage. You don't want to upgrade to 200TB and allocate 4x or more the needed storage for all your servers just for slush free space, in case they might need it. Most of them won't. But you don't want server admins coming to you many times a day asking for another small space increase and doing constant maintenance work to resize file systems just-in-time either.<br>
<p>
So you set the SAN up to lie about the real size of the LUNs it hands out. Sysadmin asks for 1TB for each of 10 servers? Sure, you can have 10TB, but you need to give me an estimate of likely actual growth for my capacity planning, because most of that storage won't exist when I give it to you. I'll add it progressively as it's actually needed. You don't have to resize your file systems all the time, I don't have to spend tons of power and money on empty disks. Their servers just have a FC HBA, 10G ethernet iSCSI or whatever to talk to the SAN.<br>
<p>
Over larger farms of servers growth tends to even out and you get fairly predictable, plannable capacity needs and plannable costs. Things managers just love. SAN vendors love it too, because they can ship you powered down racks of extra disks and charge you to turn them on - no need for a site visit.<br>
<p>
The support for thin provisioning on Linux can be used to implement similar schemes for things like a vm host that carries thousands or tens of thousands of VMs, most of which don't need much space. But where you can't easily predict which ones will, and when.<br>
<p>
But everything's an estimate. If you fail to capacity plan properly or monitor properly you can run out of thin provisioned space entirely. That's quite bad. Most systems offer safeguards like reserved space pools for subsets of critical servers, so it won't bring down everything. But centralising storage does centralise failure.<br>
<p>
That failure should still be limited to "oops, we can't write data to this volume" though. The problem is that right now, it isn't. Due to the fsync issues discussed on lwn recently and due to FS layer limitations, thin provisioning failures are not handled very gracefully, and write failures may turn into data loss.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754062/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor754078"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2018 9:31 UTC (Thu)
                               by <b>dgm</b> (subscriber, #49227)
                              [<a href="/Articles/754078/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
First let me tell you that I don't have experience with large server pools. <br>
<p>
That said, I fail to see how thin provisioning is better than allowing users to freely take more space as they need it. <br>
<p>
By your explanation, it seems that it is used like some kind of pre-granted quota that you can use without explicit permission, but with the undesirable side effect of unexpected errors when resources that are supposedly there fail to materialize. That in turn forces complexity on the applications, because you cannot make assumptions.<br>
<p>
If you want to have quotas, why don't you use quotas? Or is there some other reason?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754078/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor754080"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2018 10:38 UTC (Thu)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/754080/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>Thin provisioning moves the management overhead from the thousands or tens of thousands of VMs using the storage system, to the single storage system.
<p>Instead of having to ask for space in small chunks (say 10 GiB at most), and having to expand regularly to cope on each of the VMs, you can give the VMs virtual disks that appear big enough for (say) 5 years worth of predicted use. The server admin only has to check in and ask for more space when they have new projections showing that they need much more space than they have has assigned.
<p>In the meantime, though, you now have tens of thousands of machines that have enough space for the worst case 5 year projection; they're not going to need that up front, so you really don't want to buy that much disk space today, only to leave hundreds of disks empty. Instead, you thin-provision; each virtual disk is only allocated as much space on the real drives as it really needs today, and you set up monitoring so that you know when you're going to need more real disks and can order them in before you need them.
<p>This means that instead of tens of thousands of server admins allocating 10 GiB chunks every couple of days, you have a small number of storage admins buying and bringing online disks every few weeks or months as needed for your growth. Your servers think they have 500 PiB of space between them, but you've only bought 200 TiB so far, and bring another 10 TiB online whenever you run low on used disk.
<p>If it all works as designed, thin provisioning is completely transparent to the clients - you always have more real disk than you're using, so the impact of thin provisioning is that you neither buy a huge amount of otherwise idle disk nor have to keep allocating space to the servers that need it. You just wait for the storage array to run low, then add another chunk.
      
          <div class="CommentReplyButton">
            <form action="/Articles/754080/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor754160"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2018 18:20 UTC (Thu)
                               by <b>Cyberax</b> (<b>&#x272D; supporter &#x272D;</b>, #52523)
                              [<a href="/Articles/754160/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yes, I understand that. I still think it would make much more sense to add a "Hey, can you increase your size?" request to the iSCSI/whatever block devices rather than: "I pinky swear that I'm REALLY using this space".<br>
<p>
This way you don't need to have untested failure paths that can happen at any moment.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754160/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor754185"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 11, 2018 0:14 UTC (Fri)
                               by <b>dgc</b> (subscriber, #6611)
                              [<a href="/Articles/754185/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; I still think it would make much more sense to add a "Hey, can you increase</font><br>
<font class="QuotedText">&gt; your size?" request to the iSCSI/whatever block devices</font><br>
<p>
Read the article again - that's essentially what the infrastructure I was talking about provides us with. <br>
<p>
i.e. it provides us with the ability for the filesystem to request the amount of space it needs from the underlying storage before it starts doing operations that depend on that space being available. Hence if the device does not have space available, then the filesystem can back out and report ENOSPC to the application before getting into a state it can't sanely recover from if it receives device-based ENOSPC errors during IO....<br>
<p>
-Dave.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754185/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor755896"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 29, 2018 16:12 UTC (Tue)
                               by <b>zdzichu</b> (subscriber, #17118)
                              [<a href="/Articles/755896/">Link</a>] 
      </p>
      
      </div>
      </summary>
      That's why first thing you should do after getting such fake space is <tt>dd if=/dev/urandom of=/file</tt>. This would give you some guarantees that your space you need is there. And should avoid unpleasant surprises in the future.
      
          <div class="CommentReplyButton">
            <form action="/Articles/755896/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor754322"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">A mapping layer for filesystems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 12, 2018 17:41 UTC (Sat)
                               by <b>fyrchik</b> (guest, #124371)
                              [<a href="/Articles/754322/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thin provisioning is useful in combo with deduplication: you can have 10TB of available space backed by only 5TB of storage (which looks reasonable for hundreds of similar VMs). <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/754322/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2018, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
