        <!DOCTYPE html>
        <html lang="en">
        <head><title>New NFS to bring parallel storage to the masses [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/313437/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/315069/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/313437/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>New NFS to bring parallel storage to the masses</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>This article brought to you by LWN subscribers</b>
<p>
Subscribers to LWN.net made this article &mdash; and everything that
       surrounds it &mdash; possible.  If you appreciate our content, please
       <a href="/Promo/nst-nag3/subscribe">buy a subscription</a> and make the next
       set of articles possible.
</blockquote>
<div class="GAByline">
           <p>January 21, 2009</p>
           <p>This article was contributed by Joab Jackson</p>
           </div>
<p>
Sometime around the end of January or early February, the Internet Engineering Task Force will 
give its final blessing to the <a 
href="http://tools.ietf.org/html/draft-ietf-nfsv4-minorversion1-29">latest 
version</a> of the venerable Network File System (NFS), version 4.1. While the authors of the standard have stressed that this is a minor 
revision of NFS, it does have at least one seemingly radical new option, 
called <a href="http://www.pnfs.com/">Parallel NFS</a> (pNFS).
</p>

<p>
The "parallel" tag of pNFS means NFS clients can access 
large pools of storage directly, rather than go through the storage 
server. Unbeknown to the clients, what they store is striped across 
multiple disks, so when that data is needed it can be called back in 
parallel, cutting retrieval time even more. If you run a cluster 
computer system, you may immediately recognize the appeal of this approach.
</p>
<p>
"<q>We're starting the process of feeding all these patches up to the 
Linux NFS maintainers,</q>" said Brent Welch, the director of 
software architecture for Panasas who is also one of that storage 
company's contributors of the pNFS code. He noted that the work for the 
prototyping and implementing pNFS in Linux, as part of NFS, has been 
going on for about two years. Ongoing work has included updating both the NFS 
client and NFS server software.
</p>
<p>

The code will be proposed for the Linux kernel in two sets, according to
Welch. The first set will have the basic procedures for setting up and
tearing down pNFS sessions, using Remote Procedure Call (RPC) operations
for exchanging IDs and initiating and ending sessions. The development teams are gunning to have
this basic outline of pNFS included in the 2.6.30 version of the kernel. The second set, ready for the 2.6.31 version of the
kernel, will be a larger patch, including the I/O commands for accessing
and changing file layouts as well as reading and writing data. Given that it will take a few more months after the 2.6.31 Kernel for it to be picked up by the major distributions, pNFS probably won't start to be deployed by even the most ambitious IT shops at least until the early part of 2010.
</p>
<p>
We all know NFS. It allows client machines to mount Unix drives that 
reside across the network as if they were local disks. Many Network 
Attached Storage (NAS)-based storage arrays use NFS. With NAS, a lot of 
hard drives all lie behind a single IP address, the drives are all
managed by the NAS box.
NAS allows organizations to pool storage, so storage administrators 
can more fluidly (and hence efficiently) allocate that storage across 
all users.
</p>
<p>
In a 2004 <a 
href="http://www.pdl.cmu.edu/pNFS/archive/gibson-pnfs-problem-statement.html">problem 
statement</a>, two of the developers responsible for getting pNFS in 
motion, Panasas chief technology officer Garth Gibson and Network 
Appliance (NetApp) engineer Peter Corbett, explained the limitations of this 
approach, especially in high performance computing environments:
</p>
<p>
<div class="BigQuote"> The storage I/O bandwidth requirements of clients 
are rapidly outstripping the ability of network file servers to supply 
them. [...] The NFSv4 protocol currently requires that all the data in a 
single file system be accessible through a single exported network 
endpoint, constraining access to be through a single NFS server.</div>
</p>
<p>
In a nutshell, the potential roadblock with NAS, or any type of 
NFS-based network storage, is the NAS head, or server, they explained. 
If too many of your clients hit the NAS server at the same time, then the 
I/O slows for everyone. You could go back to direct access, but you lose 
the efficiencies of pooled storage. For cluster computer systems, in 
which dozens of nodes can be working on the same data set, such 
partitioned storage just isn't feasible. Nor are multiple storage 
servers: An NFS-based system can not support multiple servers writing to 
the same file system.

</p>
<p>
Gibson and Corbett were early champions of developing pNFS, along with 
Los Alamos National Laboratory's Gary Grider. Additional work was 
carried out by engineers at EMC, Panasas, NetApp and other companies. 
The University of Michigan's Center for Information 
Technology Integration (CITI), along with members of the IBM Almaden
Research Center are <a 
href="http://www.citi.umich.edu/projects/asci/pnfs/linux/">developing a 
pNFS implementation for Linux</a>, both for clients and storage
servers.

</p>
<p>
pNFS will allow clients to <a 
href="http://www.ibm.com/developerworks/linux/library/l-pnfs/">connect
directly</a> to the storage devices they  
need, rather than go through a storage gateway of some sort. The folks
behind pNFS like to say that their approach separates the  
control traffic from the data traffic. When a client requests a particular
file or block of storage,  
it sends a request to a server called the Metadata Server (MDS), which
returns a map of where all the data  
resides within the storage network. The client can then access that data directly, according to permissions set by the file system. Once that 
storage is altered, the client notifies the MDS of the changes, which updates the file layout.
</p>
<p>
Since pNFS allows clients to talk directly to the storage devices, as well as permitting client data to be 
striped across multiple storage devices, the client can enjoy a higher I/O rate than would be had simply by going through a single NAS head&mdash;or by 
communicating with a single storage server. In 2007, three developers from 
the IBM Almaden Research Center, Dean Hildebrand, Marc Eshel and Roger 
Haskin, <a 
href="http://www.linuxclustersinstitute.org/conferences/archive/2008/PDF/Hildebrand_98265.pdf">demonstrated</a> [PDF]   
at the <a href=http://sc07.supercomputing.org/>Supercomputing 2007</a> conference (SC07) how three clients could saturate a 10 gigabit 
link by drawing data from 336 Linux-based storage devices. Such 
throughput "<q>would be hard to achieve using standard NFS in terms of 
accessing a single file,</q>" Hildebrand said. "<q>We wanted to 
show that pNFS could scale to the network hardware available.</q>"
</p>
<p>
pNFS is largely made up of three sets of protocols. One protocol is for the
mapping, or layout, of resources, which resides on the client. It interprets and utilizes the data map returned from the 
metadata server. The second is the transport protocol, which also 
resides on the client. It coordinates data transfer between the clients 
and storage devices. The transport protocol handles the actual I/O with the 
storage devices. A control protocol will synchronize the metadata server 
with the storage devices. This last protocol is the only one not 
specified by NFS&mdash;It will be left to storage the vendors, though much of 
the work that this protocol will do can be codified in NFS commands.
</p>
<p>
pNFS can work with three types of storage&mdash;file-based storage, 
object-based storage and block-based storage. The NFSv4.1 protocol 
itself contains the file-based storage protocol. Additional RFCs are 
being developed for <a 
href="http://tools.ietf.org/html/draft-ietf-nfsv4-pnfs-obj-12">object</a> 
and <a 
href="http://tools.ietf.org/html/draft-ietf-nfsv4-pnfs-block-11">block</a> 
protocols. File-based storage is what most system administrators think of as storage; 
it is the standard approach of nesting files within a hierarchical set of directories. 
Block-based storage is used in Storage Area Networks (SANs), in which the applications access disk space directly, 
by sending the Small Computer System Interface (SCSI) commands over 
Fibre Channel, or, increasingly of late, TCP/IP via the Internet SCSI (iSCSI) protocol. 
Object-based storage is somewhat of a newer beast, a parallel approach that involves embedding the data itself with self-describing metadata.
</p>
<p>
A word on semantics: Keep in mind that just as NFS is not a file system itself, neither is pNFS. 
NFS provides the protocols to work with remote files as if they were local. Likewise, pNFS offer the 
ability to work with files managed by a parallel file system as if they were on a local drive, handling 
such tasks as setting permissions and ensuring data integrity. Fortunately, a number of parallel file systems have been 
spawned over the past few years that should work easily with pNFS. 
On the open source front, there is the the <a
href="http://www.pvfs.org/">parallel Virtual File  
System</a> (pVFS). Perhaps the most widely-used 
open-source parallel file system now in use is <a 
href="http://www.lustre.org">Lustre</a>, now overseen by Sun   
Microsystems. On the commercial front, Panasas' <a 
href="http://www.panasas.com/activescale.html">PanFS file system</a> has   
been successfully deployed in high performance computer clusters, as has IBM's <a 
href="http://www-03.ibm.com/systems/clusters/software/gpfs/index.html">General 
Parallel File System</a> (GPFS). All of these approaches use a similar idea&mdash;let the 
clients talk to the storage server's devices directly, while having some 
form of metadata server keep track of the storage layout. But most other 
options rely on using a single vendor's gear.
</p>
<p>
"<q>The main advantage [to using pNFS] is expected to be on the client 
side<q>", noted CITI programmer J. Bruce Fields, who does the NFS 4.1 
testing on Linux servers. With most parallel file systems you have to do some 
kernel reconfigurations on the clients so that they can work with the file systems. With the prototype 
Linux client, you can run a standard mount command and get the files you need. "<q>The client will automatically negotiate 
pNFS and find the data servers. By the time we're done that should work 
on any out-of-the-box Linux client from the distribution of your 
choice<q>", he says.
</p>
<p>
The advantage that pNFS will bring is familiarity, and that it will come 
already built in as part of NFS. Since NFS is a standard component in almost 
all Linux kernel builds, that will greatly reduce the amount of 
work administrators need to do to set up a parallel file system for 
Linux servers. Most administrators are more familiar with the 
general operating procedures of NFS, much more so than dealing directly with, say, Lustre, 
which requires numerous kernel patches and a different mindset when it 
comes to understanding commands.
</p>
<p>
pNFS should help storage vendors as well, as they will not have to port 
client software to numerous Linux distributions. Welch, for instance, noted that Panasas has to maintain code for dozens of different Linux distributions. Instead, they can 
rely on NFS and focus on storage devices. Already, Panasas, NetApp, EMC, 
IBM and have all <a 
href="http://www.pnfs.com/docs/sc08_pnfs_bof_slides.pdf">promised</a> [PDF]
to    
support pNFS in at least some of their storage products, according to a 
collective talk some of the developers gave last month at the <a
href=http://sc08.supercomputing.org/>SC08</a> conference. Sun Microsystems also plans to support pNFS in Solaris.
</p>
<p>
And while much of the early focus of pNFS has been for large scale 
cluster operations, one day it may be feasible that even workstations 
and desktops will use pNFS in some form. LANL's Gary Grider pointed out that, 
"<q>at some point, having several teraflops may even be possible in 
your office, in which case you may need something more than just NFS for 
data access for such a powerful personal system. pNFS may end up being 
handy in this environment as well.</q>"
</p>
<p>
Indeed. Once upon a time we were limited to working on files on our own machines, 
FTP'ing in anything that was located elsewhere. But NFS allowed us to mount drives across 
the network with a relatively simple command. Now, pNFS may take simplify things a step further, 
by allowing to us to pull in and write large files or myriad files with a speed that we can now only dream about. At least that is the promise of pNFS.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Clusters-Filesystems">Clusters/Filesystems</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Network_filesystems">Network filesystems</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Jackson_Joab">Jackson, Joab</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/313437/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor316021"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 4:21 UTC (Thu)
                               by <b>jwb</b> (guest, #15467)
                              [<a href="/Articles/316021/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Does a pNFS mount have the full POSIX semantics like a real local filesystem?  Can you safely deliver mail on it?  I've tested storage from vendors like Ibrix and Isilon and I've always found that they fail in simple scenarios, such as two clients that open the same file in O_APPEND mode.  On local unix filesystems this works fine, and you get coherent results, but on most commercial cluster storage you get gibberish.  The only place where I've successfully exercised the full POSIX feature set is Lustre, which works perfectly in my experience.  I hope that pNFS takes after Lustre more than it takes after NFSv4.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316021/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor316095"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 12:50 UTC (Thu)
                               by <b>epa</b> (subscriber, #39769)
                              [<a href="/Articles/316095/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Is there a test suite you can run to check a filesystem's POSIX compliance for cases like this?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316095/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor316122"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 16:18 UTC (Thu)
                               by <b>eli</b> (guest, #11265)
                              [<a href="/Articles/316122/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I don't know if fsx specifically tests that case, but it may: <br>
<a href="http://www.codemonkey.org.uk/projects/fsx/">http://www.codemonkey.org.uk/projects/fsx/</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316122/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor316120"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 16:21 UTC (Thu)
                               by <b>snitm</b> (guest, #4031)
                              [<a href="/Articles/316120/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Can you elaborate on your "two clients that open the same file in O_APPEND mode" scenario?  What are your expectations?  What is each clients' write workload?  Are they each just blasting N bytes into the same file without any higher-level application coordination?  What are you saying Lustre gets right and Ibrix, Isilon, etc. get wrong?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316120/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor316127"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 16:56 UTC (Thu)
                               by <b>jwb</b> (guest, #15467)
                              [<a href="/Articles/316127/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
When a file is opened with O_APPEND, a call to write() causes a seek to the end of the file and a write, atomically.  On a normal local filesystem, n-many clients can do this to the same file at once, and their writes will all be atomic.  This also works on Lustre.  It definitely does not work on ordinary NFS, and it also does not work on some of the other commercial distributed/cluster filesystems I have tested.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316127/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor316208"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 22, 2009 20:32 UTC (Thu)
                               by <b>felixfix</b> (subscriber, #242)
                              [<a href="/Articles/316208/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There used to be a string attached to those atomic writes.  If it was too many bytes, either by absolute limit (4096 bytes?) or crossed a page boundary, it was split into multiple atomic writes.  But I haven't had need to worry about this for many years, so I may misremember details.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316208/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor316511"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 24, 2009 14:14 UTC (Sat)
                               by <b>xav</b> (guest, #18536)
                              [<a href="/Articles/316511/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I don't think there has ever been a guarantee on write() to be atomic. What write() does is return the number of bytes it could store, that's all.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316511/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor316520"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 24, 2009 17:19 UTC (Sat)
                               by <b>jwb</b> (guest, #15467)
                              [<a href="/Articles/316520/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
"If set, then all write operations write the data at the end of the file, extending it, regardless of the current file position. This is the only reliable way to append to a file. In append mode, you are guaranteed that the data you write will always go to the current end of the file, regardless of other processes writing to the file. Conversely, if you simply set the file position to the end of file and write, then another process can extend the file after you set the file position but before you write, resulting in your data appearing someplace before the real end of file."<br>
<p>
<a href="http://theory.uwinnipeg.ca/gnu/glibc/libc_144.html">http://theory.uwinnipeg.ca/gnu/glibc/libc_144.html</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316520/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
<a name="CommAnchor316577"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 25, 2009 22:04 UTC (Sun)
                               by <b>job</b> (guest, #670)
                              [<a href="/Articles/316577/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
So how will a typical set up look? Do you run several pNFS nodes on individual block devices? On top of Lustre? Instead of Lustre?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/316577/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor317165"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 29, 2009 16:26 UTC (Thu)
                               by <b>malcolmparsons</b> (guest, #46787)
                              [<a href="/Articles/317165/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Unbeknown to the clients, what they store is striped across multiple discs</font><br>
<p>
s/disc/disk/<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/317165/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor317171"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">New NFS to bring parallel storage to the masses</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jan 29, 2009 16:48 UTC (Thu)
                               by <b>jake</b> (editor, #205)
                              [<a href="/Articles/317171/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; s/disc/disk/</font><br>
<p>
fixed, thanks!<br>
<p>
jake<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/317171/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2009, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
