        <!DOCTYPE html>
        <html lang="en">
        <head><title>Proactive compaction for the kernel [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/817905/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/817665/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/817905/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Proactive compaction for the kernel</h1>
</div>
<div class="ArticleText">
<div class="GAByline">
           <p>April 21, 2020</p>
           <p>This article was contributed by Nitin Gupta</p>
           </div>
<p>
Many applications benefit significantly from the use of huge
pages. However, huge-page allocations often incur a high latency or even
fail under fragmented memory conditions. Proactive compaction may provide an
effective solution to these problems by doing memory compaction in the
background. With my proposed proactive compaction implementation, typical huge-page allocation
latencies are reduced by a factor of 70-80 while incurring minimal CPU
overhead. 
</p>

<p><blockquote class="ad">
<b><tt>$ sudo subscribe today</tt></b>
<p>
Subscribe today and elevate your LWN privileges. You’ll have
access to all of LWN’s high-quality articles as soon as they’re
published, and help support LWN in the process.  <a href="https://lwn.net/Promo/nst-sudo/claim">Act now</a> and you can start with a free trial subscription.
</blockquote>
<p>
<a href="/Articles/368869/">Memory compaction</a> is a feature of the Linux kernel that makes larger,
physically contiguous blocks of free memory available. Currently, the
kernel uses an on-demand compaction scheme. Whenever a per-node <tt>kcompactd</tt>
thread is woken up, it compacts just enough memory to make available a
single page of the needed size. Once a page of that size is made
available, the thread goes back to sleep. This pattern of compaction
often causes a high latency for higher-order allocations and hurts
performance for workloads that need to burst-allocate a large number of
huge pages.
</p>

<p> Experiments where compaction is manually triggered on a system with a
fragmented memory state show that it could be brought to a fairly
compacted memory state within one second for a 32GB system. Such data suggests
that a proactive compaction scheme in the kernel could allow allocating a
significant fraction of memory as huge pages while keeping allocation
latencies low.  </p>

<p>My recent <a href="/ml/linux-kernel/20200310222539.1981-1-nigupta%40nvidia.com/">patch</a>
provides an implementation of the proactive compaction approach. It exposes
a single tunable: <tt>/sys/kernel/mm/compaction/proactiveness</tt>, which accepts
values in the range [0, 100], with a default value of 20. This tunable
determines how aggressively the kernel should compact memory in the
background. The patch reuses the existing, per-NUMA-node <tt>kcompactd</tt> threads to
do background compaction. Each of these threads periodically calculates a
per-node fragmentation score (an indicator of the memory fragmentation) and
compares it against a threshold, which is derived from the tunable.  </p>

<p> The per-node proactive (background) compaction process is started by
its corresponding <tt>kcompactd</tt> thread when the node's fragmentation score
exceeds the high threshold. The compaction process remains active till the
node's score falls below the low threshold, or one of the back-off
conditions (defined below) is met.  </p>

<p> Memory compaction involves bringing together "movable" pages at a
zone's end to create larger, physically contiguous, free regions at a
zone's beginning. If there are "unmovable" pages, like kernel allocations,
spread across the physical address space, this process is less
effective. Compaction has a non-trivial system-wide impact as pages
belonging to different processes are moved around, which could also lead to
latency spikes in unsuspecting applications. Thus, the kernel must not
overdo compaction and should have a good back-off strategy.  </p>

<p>
The patch implements back-offs in the following scenarios:
</p>

<ul class="spacylist">
<li>When the current node's <tt>kswapd</tt> thread is active (to avoid interfering
with the reclaim process).</li> 
<li>When there is contention on the per-node <a
href="https://elixir.bootlin.com/linux/v5.6/source/include/linux/mmzone.h#L769"><tt>lru_lock</tt></a>   
or per-zone <a
href="https://elixir.bootlin.com/linux/v5.6/source/include/linux/mmzone.h#L536"><tt>lock</tt></a> 
(to avoid hurting non-background, latency-sensitive contexts).</li> 
<li>When there is no progress (reduction in node's fragmentation score
value) after a round of compaction.</li> 
</ul>

<p> If any of these back-off conditions is true, the proactive compaction
process is deferred for a specific time interval.  </p>

<h4>Per-node fragmentation score and threshold calculation</h4>

<p> As noted earlier, this proactive compaction scheme is controlled by
a single tunable called proactiveness. All required values like per-node
fragmentation score and thresholds are derived from this tunable.  </p>

<p> A node's score is in the range [0, 100] and is defined as the sum of
all the node's zone scores, where a zone's score (Sz) is defined as: </p>

<pre>
    Sz = (Nz / Nn) * extfrag(zone, HUGETLB_PAGE_ORDER)
</pre>

where:

<ul class="spacylist">
<li><tt>Nz</tt> is the total number of pages in the zone.</li>
<li><tt>Nn</tt> is the total number of pages in the zone's parent node.</li>
<li><tt>extfrag(zone, HUGETLB_PAGE_ORDER)</tt> is the external fragmentation
with respect  to the huge-page order in this zone.</li>
</ul>

<p>
In general, this is the way to calculate
external fragmentation with respect to any order:

<pre>
    extfrag(zone, order) =  ((Fz - Hz) / Fz) * 100
</pre>

where:

<ul class="spacylist">
<li><tt>Fz</tt> is the total number of free pages in the zone.</li>
<li><tt>Hz</tt> is the number of free pages available in blocks of size &gt;= 2<sup>order</sup>.</li>
</ul>

<p>
This per-zone score value is in the range [0, 100], and is defined as 0
when <tt>Fz&nbsp;=&nbsp;0</tt>. The reason for calculating the per-zone score this way is to
avoid wasting time trying to compact special zones like <tt>ZONE_DMA32</tt> and
focus on zones like <tt>ZONE_NORMAL</tt>, which manage most of the memory
(<tt>Nz&nbsp;≈&nbsp;Nn</tt>). For smaller zones (<tt>Nz&nbsp;&lt;&lt;&nbsp;Nn</tt>), the score
tends to 0, and thus can never 
cross the low threshold value (defined below). 
</p>

<p> The low (<tt>Tl</tt>) and high (<tt>Th</tt>) thresholds, against which these scores are
compared, are defined as follows: </p>

<pre>
    Tl = 100 - proactiveness
    Th = min(10 + Tl, 100)
</pre>

<p>
These thresholds are in the range [0, 100].  Once a zone's score exceeds
<tt>Th</tt>, proactive compaction will be done until the score drops below <tt>Tl</tt>.
</p>

<h4>Performance evaluation</h4>

<p> For a true test of memory compaction efficacy, the first step is
to fragment the system memory such that no huge pages are directly
available for allocation (i.e., an initial fragmentation score of 100 for
all NUMA nodes). With the system in such a fragmented memory state, any run
of a huge-page-heavy workload would highlight the effects of the kernel's
compaction policy. With on-demand compaction, a majority of huge-page
allocations hit the direct-compaction path, leading to high allocation
latencies. With proactive compaction, the expectation is to avoid these
latencies except when the compaction process is unable to catch up with the
huge-page allocation rate.  </p>

<p> For evaluating proactive compaction, a high level of external
fragmentation (as described above) was triggered by a user-space program
that allocated almost all memory as huge pages and then freed 75% of
pages from each huge-page aligned chunk. All the tests were done on an
x86_64 system with 1TB of RAM and two NUMA nodes, using kernel version
5.6.0-rc4. The first huge-page-heavy workload was a test kernel driver that
allocated as many huge pages as possible, measuring latency for each
allocation, until it hit an allocation failure. The driver was able to
allocate almost 98% of free memory as huge pages with or without the
patch. However, with the vanilla kernel (without the proactive compaction
patch), 95-percentile latency was 33799µs while with the patch (with
proactiveness tunable set to 20), this latency was 429µs &mdash; a
reduction by a factor of 78.  </p>

<p> To further measure the performance, a
Java heap allocation test was used, which allocated 700GB of heap space,
after fragmenting the memory as described above. This workload shows the
effect of reduced allocation latencies in the run time of huge-page-heavy
workloads. On the vanilla kernel, the run time was ~27 minutes, while with the
patch (proactiveness=20), the run time came down to roughly four minutes. Tests with
higher proactiveness values did not show any further speedups or slowdowns.
</p>

<p> Some <a
href="/ml/linux-kernel/ae306f4d-4dff-a97b-00b1-71d7ab54f68b%40suse.cz/">questions</a>
were 
raised by Vlastimil Babka, who has <a href="/Articles/717656/">worked on
proactive compaction</a> along the way and helpfully reviewed some of these
patches: </p>

<div class="BigQuote"> By your description it seems to be a one-time
fragmentation event followed by a one-time stream of allocations? So
kcompactd probably did the proactive work just once? That works as a smoke 
test, but what I guess will be more important is behavior under more
complex workloads, where we should also check the vmstat compact_daemon*
stats and possibly also kcompactd kthreads CPU utilizations.  </div>

<p>The comment led to further investigation into the proactive <tt>kcompactd</tt>
behavior. For the Java heap test, per-node fragmentation scores were
recorded during the program's runtime, together with <tt>kcompactd</tt> threads' CPU
usage. The data clearly shows that proactive compaction is active
throughout the runtime of the test program and that a <tt>kcompactd</tt> thread
takes 100% of one of the CPU cores while it is active.</p>

<blockquote>
<img class="photo" src="https://static.lwn.net/images/2020/proactive-compaction.svg"
alt="[Proactive compaction performance graph]"
title="Proactive compaction performace graph">
</blockquote>

<p>The above plot shows changes in per-node fragmentation scores as a Java
process tries to allocate 700GB of the heap area on a two-node system with
512GB memory each. The proactiveness tunable is set to 20, so the low and
high thresholds are 80 and 90, respectively. Before the test program was
run, the system memory was fragmented, such that no huge pages were
available for direct allocation. The heap allocation started on Node-0,
where the score rose as huge pages are used. When the score gets above 90,
proactive compaction is triggered on the node, bringing the score back to
80. Eventually, all memory on Node-0 was exhausted (around 90 seconds into the
run), at which point the allocation started from Node-1, where the same
compaction pattern repeated.</p>

<p>As described earlier, compaction is an expensive operation, and we don't
want to pay the cost unless it is able to reduce the external
fragmentation. To evaluate the back-off behavior, another test was created
where unmovable pages were scattered throughout the physical address
space. With the system in such a memory state, compaction cannot do much
apart from warming the CPU. The back-off mechanism implemented in this
patch correctly identifies such a situation by checking that a node's score
does not go down after a round of compaction. When that happens, further
rounds of proactive compaction are deferred for some time.  </p>

<h4>Future work</h4>


<p> The patch has already been through some review cycles, which have helped
refine many of its aspects. Some kernel developers recognize the need for a
more proactive compaction, and given these encouraging numbers, the patch
will hopefully be accepted, probably after a few more iterations.  </p>

<p> As a future direction, I am focusing on refining the per-node fragmentation
score and threshold calculations, which drive the background proactive
compaction process. Currently, the score calculation does not take into
account per-node characteristics like differing TLB latencies, which would
be important in heterogeneous systems. Future patches will likely add
scaling factors to both score and threshold calculations to account for
these per-node characteristics.  </p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Large_allocations">Memory management/Large allocations</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Gupta_Nitin">Gupta, Nitin</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/817905/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor818253"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 21, 2020 17:17 UTC (Tue)
                               by <b>josh</b> (subscriber, #17465)
                              [<a href="/Articles/818253/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
To what extent could the kernel attempt to keep its unmovable pages together, and separate from movable allocations?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/818253/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor818257"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Segregating movable pages</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 21, 2020 17:59 UTC (Tue)
                               by <b>corbet</b> (editor, #1)
                              [<a href="/Articles/818257/">Link</a>] 
      </p>
      
      </div>
      </summary>
      The kernel already does that, it's what <a href="/Articles/224255/"><tt>ZONE_MOVABLE</tt></a> is for.  Of course, sometimes things that were thought to be movable (or short-lived, which is almost the same thing) turn out not to be...
      
          <div class="CommentReplyButton">
            <form action="/Articles/818257/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor818289"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 22, 2020 0:11 UTC (Wed)
                               by <b>djbw</b> (subscriber, #78104)
                              [<a href="/Articles/818289/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How much of this effect is from proactive operations vs arranging for more CPU to be deployed on the problem? For example, if you controlled for the CPU utilization for compaction being identical between proactive vs direct would the Java heap test still take 27 minutes in the direct case? Basically how to determine if the result is an argument to do proactive compaction vs more aggressive direct compaction?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/818289/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor818320"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 22, 2020 10:47 UTC (Wed)
                               by <b>jtaylor</b> (subscriber, #91739)
                              [<a href="/Articles/818320/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
From the description and the current patch it seems the backoff is absolute only if no progress is made.<br>
<p>
This would mean it is always compacting with the same frequency even if only very little progress is made because some process constantly fragments some pages.<br>
<p>
Wouldn't it be better to have the backoff time depend on how much progress was made last time (or some exponential decaying value of the last iterations progresses)?<br>
So if there is much to compact with the current workload it runs more often, but if there is not much to do it runs less often.<br>
<p>
One does have to be careful this does not negatively effect performance so that distributions actually enable it.<br>
Back when redhat backported anonymous transparent huge pages to their rhel7(?) a few years back the compaction was terrible for performance and as a consequence and many blog posts which said turn of thp (instead of only the compaction) it seems many distributions still have thp turned off by default today.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/818320/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor818328"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 22, 2020 12:00 UTC (Wed)
                               by <b>scientes</b> (guest, #83068)
                              [<a href="/Articles/818328/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
My hunch would be that compaction should never be done on-demand (but the background process should be influenced by demand). Perhaps if the VM addresses were assigned contiguously it would be possible to convert the allocations later to a huge page in a background process.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/818328/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor818393"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 22, 2020 19:00 UTC (Wed)
                               by <b>jccleaver</b> (subscriber, #127418)
                              [<a href="/Articles/818393/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; From the description and the current patch it seems the backoff is absolute only if no progress is made.</font><br>
<font class="QuotedText">&gt; This would mean it is always compacting with the same frequency even if only very little progress is made because some process constantly fragments some pages.</font><br>
<font class="QuotedText">&gt; Wouldn't it be better to have the backoff time depend on how much progress was made last time (or some exponential decaying value of the last iterations progresses)?</font><br>
<font class="QuotedText">&gt; So if there is much to compact with the current workload it runs more often, but if there is not much to do it runs less often.</font><br>
<p>
Agreed. This definitely calls out for some sort of efficiency barrier, where if there was less than a (arbitrary number) 2% improvement, then increase back off in most cases.<br>
<p>
There should be a visible setting (say backoff values &gt;95) that cause all but the tiniest improvements to be considered worthwhile, though, especially if the system truly is mostly idle.<br>
<p>
Unrelated side-note: Does compaction happen at hibernation time, or before dropping to deep sleep states? Seems like a good opportunity.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/818393/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor819343"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Priority of the kcompactd thread</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 4, 2020 11:07 UTC (Mon)
                               by <b>polyp</b> (guest, #53146)
                              [<a href="/Articles/819343/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How is the priority of the kcompactd thread handled? It seems to me that depending on what mode it is used in (on-demand or proactive) it would need to run using different thread priorities? Maybe same prio as the process waiting for it during on-demand compaction and low prio when doing background/proactive compaction?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/819343/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor954574"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Proactive compaction for the kernel</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 12, 2023 9:25 UTC (Tue)
                               by <b>WuYang</b> (guest, #166224)
                              [<a href="/Articles/954574/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The patch implements back-offs in the following scenarios:<br>
<p>
When the current node's kswapd thread is active (to avoid interfering with the reclaim process).<br>
When there is contention on the per-node lru_lock or per-zone lock (to avoid hurting non-background, latency-sensitive contexts).<br>
When there is no progress (reduction in node's fragmentation score value) after a round of compaction.<br>
<p>
hello, for your described backoff mechanism, can you share your achievement from patch.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/954574/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2020, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
