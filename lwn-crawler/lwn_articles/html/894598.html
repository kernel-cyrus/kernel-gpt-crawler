        <!DOCTYPE html>
        <html lang="en">
        <head><title>CXL 1: Management and tiering [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/894598/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/894984/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/894598/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>CXL 1: Management and tiering</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 13, 2022</br>
           <hr>
<a href="/Articles/lsfmm2022/">LSFMM</a>
</div>
<a href="https://www.computeexpresslink.org/about-cxl">Compute Express
Link</a> (CXL) is an upcoming memory technology that is clearly on the
minds of Linux memory-management developers; there were five sessions
dedicated to the topic at the <a
href="https://events.linuxfoundation.org/lsfmm/">2022 Linux Storage,
Filesystem, Memory-management and BPF Summit</a> (LSFMM).  The first three
sessions, on May&nbsp;3, covered various aspects of memory management in the
presence of CXL.  It seems that
CXL may bring some welcome capabilities, especially for cloud-service
providers, but that will come at the cost of some headaches on the
kernel-development side.
<p>
At its core, CXL is a new way to connect memory to a CPU.  That memory need
not be on the local memory bus; indeed, it is likely to be located on a
different device entirely.  CXL vendors seemingly envision "memory
appliances" that can provide memory to multiple systems in a flexible
manner.  Supporting CXL raises a number of interesting issues around
system boot, memory hotplug, memory tiering, and more.
<p>
<h4>A CXL memory interface for containers</h4>
<P>
The first session was led by Hongjian Fan over a remote link; it was
focused on how to use CXL memory to support containers.  Figuring this out,
he said, is complicated by the fact that CXL is new technology and there
are no real devices to play with yet.  So, much of the work being done is at
the conceptual level.  The Kubernetes <a
href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">container
storage interface</a> provides a flexible way to allocate storage to
containers; he is working on a "container memory interface" (CMI) to do the same
thing with CXL memory.
<p>
Systems can use CMI to provide functionality like <a
href="/Articles/857133/">memory tiering</a> and to 
manage resources in a pooled-memory system.  There are a few scenarios that
Fan envisions for how this would all work.  One would be that containers
would have access to all of the memory available to the system (though
managed by the control-group memory controller, of course); in this case,
CXL would bring little change.  If, instead, the container implements 
tiered memory, then CMI will control access to
the different memory types.  There are also pooled-memory scenarios, where
the memory is located on an appliance somewhere.
<p>
Fan had a series of questions he was seeking to answer.  The first would be
whether it is possible to create a common CMI standard that would work
across all CXL vendors.  With regard to memory tiering, he asked, is it
better to do it within the containers, or instead at the host level?  There
are also open questions about how to manage pooled-memory servers.  An
attendee started the discussion by asking whether all of this could be
managed with control groups, with different types of data packaged as if
they were CPUless NUMA nodes.  That might be the simplest place to start,
Fan answered, but he was not sure that control groups had sufficient
flexibility.
<p>
Michal Hocko said that cpusets could perhaps help with the management, but
they provide no way to control how memory is distributed across nodes.
Dave Hansen said that there is interest in providing control over memory
allocation; providers could charge lower rates for access to slower memory,
for example.  The problem exists now, and people try to manage things with
the <tt>numactl</tt> utility, but it's not up to the task.  It can block
users from certain types of RAM, he said, but it's an all-or-nothing deal.
It can't provide the finer quality-of-service control that providers want.
<p>
Dan Williams said that the current work has been focused on DRAM and slower
types of memory.  CXL is going to bring a broader spectrum of vendors and
speeds, and multiple performance classes.  While it might make sense to
design a system to handle two tiers of memory service now, developers
should be thinking about five tiers in the future.  Matthew Wilcox said
that enterprise vendors are unlikely to want to manage that many tiers,
though.
<p>
Adam Manzanares suggested starting with well-defined uses cases and just
two tiers.  Otherwise, he worries that things will get out of control
quickly.  Wilcox said that there is a sane three-tier case consisting of
CXL memory, DRAM, and persistent memory.  But Hansen warned of multiple
CXL-attached tiers, and that developers should expect "a lot of weird CXL
devices".  It is an open standard, and vendors are free to do interesting
things with it.
<p>
Fan said that, for any sort of management to work, the kernel will need
some idea of the relative performance of each available memory tier.
Hansen answered that there is a lot of standards work in this area.  ACPI
has a way of enumerating NUMA latency, for example, and other mechanisms
are under development.  The Heterogeneous Memory Attribute Table (HMAT),
for example, can provide bandwidth information for each memory type.  UEFI,
meanwhile, has specified the <a
href="https://uefi.org/sites/default/files/resources/Coherent%20Device%20Attribute%20Table_1.01.pdf">Coherent
Device Attribute Table</a> (CDAT) with CXL memory, among other types, in mind.
<p>
Williams said that Linux is too dependent on the notion of NUMA distance as
a way of describing memory capabilities.  There is better information about
memory available from the firmware now, but the memory-management code does
not make use of it.  A baby step might be to boil that information down
into a single distance value to at least make some use of it.  Manzanares
said that distance doesn't work for persistent memory, though, since it
cannot capture the asymmetry between read and write speeds.
<p>
Hansen said that the relevant information is available now if an
application knows where to look.  The harder problem is making decisions
about memory placement in the kernel.  Different workloads may have
different preferences depending on their access patterns; currently,
applications have to figure out which memory they want and set up an
appropriate NUMA policy.  But the kernel could be using memory information
to make smarter decisions; moving frequently written pages off of
persistent memory, for example.
<p>
There was some discussion about where decisions on tiering should be made.
Putting the logic into the kernel makes life easy for applications that
don't care about NUMA placement, which is most of them, Williams said, but
he worried that there could be fights between the kernel and user space
about tiering.  Hansen said those fights could happen now, but the kernel's
NUMA-placement logic mostly stays out of the way if user space has set an
explicit policy.  That may be sufficient for future needs as well.
<p>
Williams asked for an explanation of the perceived deficiencies in the
current NUMA API.  Fan answered that there needs to be a way to set memory
limits on a per-node basis; that will require a new control-group or
<tt>numactl</tt> knob.  Manzanares suggested adding better tiered-memory
support to QEMU so that this work could go forward, but Davidlohr Bueso
pointed out that it's not possible to get real performance numbers that
way.  The concern at this point, Manzanares said, is to work out the
interface issues rather than to optimize performance.  Hansen said that a lot can be done by putting some
persistent memory into a system and treating it like another tier; the
result "kind of looks like CXL if you squint at it funny".  That would give
ways to play with interfaces and get some initial performance data.
<p>
Fan thanked the group for having provided a bunch of good information for
him to work with, and the session drew to a close.
<p>
<h4>Managing CXL memory</h4>
<p>

<a href="/Articles/894621/"><img
src="https://static.lwn.net/images/conf/2022/lsfmm/JonTrantham-sm.png" alt="[Jon Trantham]"
title="Jon Trantham" class="rthumb"></a>

The next session, led by Jon Trantham, delved into some of the other issues
that come up when trying to manage CXL memory.  CXL, he said, is a way to
attach memory devices that cannot go onto the DDR memory bus.  Putting DDR
interfaces onto devices can be hard for manufacturers, and DDR does not
work all that well with persistent memory.  But CXL memory has different
performance characteristics than normal RAM.  Its latency and bandwidth
will differ, and they can change as the device ages.  Persistence,
endurance, and reliability can all differ as well.
<p>
There are various ways of reporting the characteristics and status of CXL
memory, starting with the above-mentioned CDAT table.  The CDAT is useful
in that it can be updated as performance changes.  CXL devices can also
produce a stream of event records, indicating that maintenance is required
or that performance is falling, for example.  CXL 2.0 enables switches that
can sit between memory and the computer, allowing memory to live in a
different enclosure entirely.  That makes actions like hot unplugging
possible, but it will be necessary to figure out how to communicate that to
the kernel.
<p>
CXL devices must implement decisions about how much memory to
allocate to each processor; this can involve an "out-of-band fabric
manager" to control the switches.  Memory can be interleaved at a
granularity as small as 64&nbsp;bytes, which is great for performance but
harder for error recovery; memory failure can leave small holes
in the address space.  Wilcox answered that the usual
management technique in such situations is crashing.
<p>
On the security side, there are access and encryption keys shared between
hosts and devices; that brings in the whole key-management problem.  The
sum of all this, he said, is that help is needed.  How is all of this to be
managed?  Should it be done in the kernel or in user space?
<p>
Williams asked if the encryption features were only for persistent memory.
Evidently CXL can provide link encryption for DRAM, but does not encrypt
data at rest.  Hansen said that it will never be possible for the kernel to
recover from errors on a 64-byte boundary; it only handles memory at the
page level.  He suggested looking at the existing mechanisms and asking
whether anything different was really needed; perhaps all of those CXL
capabilities aren't really necessary.
<p>
Williams said that CXL makes it possible to turn bare metal into virtual
machines; techniques like memory ballooning become possible.  So it seems
that the same interfaces should be used.  Hocko says that ballooning relies
on memory hotplug, which "mostly works", but shrinking memory is hard.  The
memory to be removed can only be used for movable allocations.  This is
equivalent to a return to the old high-memory systems, where much of the
installed memory could not be used by the kernel.
<p>
Hansen answered that the kernel does a reasonable job of emptying a memory
area that is to be removed, but there is always the case where a few pages
simply cannot be cleared.  If there were some way to retain those pages
after the memory goes, he said, life would be easier and the whole
mechanism would be more reliable.
<p>
The session closed with Manzanares suggesting more coordination between
developers and vendors.  Perhaps there needs to be some sort of regular
group call where these issues are worked out.  Chances are that something
like that will be set up soon.
<p>
<h4>Tiering</h4>
<p>
<a href="/Articles/894622/"><img
src="https://static.lwn.net/images/conf/2022/lsfmm/JongminGim-sm.png" alt="[Jongmin Gim]"
title="Jongmin Gim" class="rthumb"></a>

The final CXL session on Tuesday was led by Jongmin Gim, who wanted to talk
about tiering in particular.  A lot of things are changing in the CXL 2.0
specification, he began, including the addition of a number of memory
types.  Tiering will allow the system to make the best use of those memory
types, putting frequently used pages in fast memory while using slower
memory to hold pages that are not needed as often.
<p>
Support for tiering is not currently upstream, but developers are working
on it.  There are various issues around promotion and demotion of pages
between tiers to be worked out.  The demotion side is easy, he said; if
there is not enough fast memory available, kick out some pages.  Promotion
turns out to be harder, though.  Current patches (described in <a
href="/Articles/893024/">this article</a>) use the NUMA-balancing scan to
try to determine which pages in slower memory are currently being used.
When hot pages are found, they can be migrated to faster memory.  A
heuristic requiring two accesses before promoting a page helps to prevent
rapid bouncing of pages between memory types.
<p>
One possible optimization might be to promote contiguous groups of pages
together in a single operation.  There was some discussion of implementing
some sort of predictive algorithm to improve page promotion, but it was all
at a fairly high level.
<p>
Manzanares said that the kernel's NUMA balancing was designed when all
nodes in a system were more-or-less equal, and it is CPU-centric.  He
wondered whether the assumptions built into NUMA balancing are still valid
in the CXL world.  Gorman said that there is no assumption that nodes are
the same size in the current code.  Hansen said that NUMA balancing is used
now for moving data to and from slower persistent-memory nodes, which are
always mismatched in size, and it seems to be working now.
<p>
The discussion wandered around the details of NUMA balancing with no real
conclusion.  At the end of the session, though, there were two points of
agreement: CXL devices are highly diverse, and that tiering is the way to
manage them.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Compute_Express_Link_CXL">Compute Express Link (CXL)</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Tiered-memory_systems">Memory management/Tiered-memory systems</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_Memory-Management_and_BPF_Summit-2022">Storage, Filesystem, Memory-Management and BPF Summit/2022</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/894598/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor895225"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2022 20:37 UTC (Fri)
                               by <b>MattBBaker</b> (guest, #28651)
                              [<a href="/Articles/895225/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Will there be an option to bypass the kernel and program the memory directly? It&#x27;s nice when the kernel and hide the details, but for some applications it&#x27;s really better if it&#x27;s aware of the different tiers of memory access and can explicitly pass messages instead of getting surprised when memory access is magically slow.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895225/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895279"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2022 16:01 UTC (Sat)
                               by <b>Paf</b> (subscriber, #91811)
                              [<a href="/Articles/895279/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
As someone who’s worked in HPC and watched the shared memory machines be displaced by true clustered systems, despite the intense and remarkable engineering that went in to keeping those shared memory machines coherent at scale … yeah.<br>
<p>
So this is the thing we’re doing again this week.<br>
<p>
That doesn’t mean it’s not worth it - those machines made a lot of sense for a while and shifting trends may make that true again - but the costs of coherency across links like these is *intense*.  I wonder where and how much use this will see, what cases it will be fast enough for, etc.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895279/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895284"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2022 20:16 UTC (Sat)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/895284/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I don&#x27;t think we&#x27;re going to see 2048-node clusters built on top of CXL. The physics just doesn&#x27;t support it.<br>
<p>
The use cases I&#x27;m seeing are:<br>
<p>
 - Memory-only devices. Sharing (and cache coherency) is handled by the CPUs that access them. Basically CXL as a replacement for the DDR bus.<br>
<p>
 - GPU/similar devices. They can access memory coherently, but if you have any kind of contention between the CPU and the GPU, performance will tank. Programs are generally written to operate in phases of GPU-only and CPU-only access, but want migration handled for them.<br>
<p>
Maybe there are other uses, but there&#x27;s no getting around physics.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895284/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895315"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2022 19:46 UTC (Sun)
                               by <b>Paf</b> (subscriber, #91811)
                              [<a href="/Articles/895315/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
This makes more sense - it’s mostly a way to make the clean cases easier and well supported by hardware.<br>
<p>
Looking it up, I’m seeing a lot of stuff about disaggregated systems which just seems crazy.  But marketing doesn’t have to match reality of intent for the main implementers.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895315/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895324"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2022 21:58 UTC (Sun)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/895324/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Oh yes, the CXL boosters have a future where everything becomes magically cheap. I don&#x27;t believe that world will come to pass. I think the future of HPC will remain as one-two socket CPU boxes with one-two GPUs, much more closely connected over CXL, but the scale-out interconnect isn&#x27;t going away, and I doubt the scale-out interconnect will be CXL. Maybe it will; I&#x27;ve been wrong before.<br>
<p>
I have no faith in disaggregated systems. You want your memory close to your [CG]PU. If you upgrade your [CG]PU, you normally want to upgrade your interconnect and memory at the same time. The only way this makes sense is if we&#x27;ve actually hit a limit in bandwidth and latency, and that doesn&#x27;t seem to have happened yet, despite the sluggish adoption of PCIe4.<br>
<p>
The people who claim &quot;oh you want a big pool of memory on the fabric behind a switch connected to lots of front end systems&quot; have simply not thought about the reality of firmware updates on the switch or the memory controller. How do you schedule downtime for the N customers using the [CG]PUs? Tuesday Mornings 7-9am Are Network Maintenance Windows just aren&#x27;t a thing any more.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895324/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor895296"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2022 3:17 UTC (Sun)
                               by <b>marcH</b> (subscriber, #57642)
                              [<a href="/Articles/895296/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; instead of getting surprised when memory access is magically slow.</font><br>
<p>
Well, it&#x27;s not like single-thread performance is deterministic either. However I agree shared memory is really crossing a line, it&#x27;s the biggest programming footgun ever invented by hardware engineers.<br>
<p>
Message passing requires an extra programming effort but unlike shared memory, performance and correctness issues can be traced and debugged in a reasonable amount of time.<br>
<p>
<a href="https://queue.acm.org/detail.cfm?id=3212479">https://queue.acm.org/detail.cfm?id=3212479</a><br>
<p>
<font class="QuotedText">&gt; Caches are large, but their size isn&#x27;t the only reason for their complexity. The cache coherency protocol is one of the hardest parts of a modern CPU to make both fast and correct. Most of the complexity involved comes from supporting a language in which data is expected to be both shared and mutable as a matter of course.</font><br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895296/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895404"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 16, 2022 16:30 UTC (Mon)
                               by <b>ballombe</b> (subscriber, #9523)
                              [<a href="/Articles/895404/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Message passing requires an extra programming effort but unlike shared memory, performance and correctness issues can be traced and debugged in a reasonable amount of time.</font><br>
<p>
But message passing often requires more total memory for the same task.<br>
And often the performance issue is traced to &quot;not enough memory by node&quot;.<br>
So...<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895404/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor895479"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">CXL 1: Management and tiering</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2022 8:27 UTC (Tue)
                               by <b>sdalley</b> (subscriber, #18550)
                              [<a href="/Articles/895479/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Personally, I&#x27;m more than happy to trade a little more memory for a little more sanity.<br>
<p>
And message passing needn&#x27;t consume more memory if the message buffer simply changes hands between owners rather than being copied. A good interface to such a message interface can also ensure, under the hood, that references to buffers no longer owned are always NULLed.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/895479/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2022, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
