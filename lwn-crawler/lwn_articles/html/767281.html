        <!DOCTYPE html>
        <html lang="en">
        <head><title>Device-to-device memory-transfer offload with P2PDMA [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/767281/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/766919/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/767281/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Device-to-device memory-transfer offload with P2PDMA</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>LWN.net needs you!</b>
<p>
Without subscribers, LWN would simply not exist.  Please consider
       <a href="/Promo/nst-nag2/subscribe">signing up for a subscription</a> and helping
       to keep LWN publishing.
</blockquote>
<div class="GAByline">
           <p>October 2, 2018</p>
           <p>This article was contributed by Marta Rybczy≈Ñska</p>
           </div>
One of the most common tasks carried out by device drivers is setting
up DMA operations for data transfers between main memory and the device. Often,
data read into memory from one device will be immediately written, unchanged,
to another device. Common examples include carrying the image between the
camera and screen on a mobile phone, or downloading files to be saved on a
disk. Those transfers have an impact on the CPU even if it does not use the
data directly, due to higher memory use and effects like
cache trashing. There are cases where it is possible to avoid usage of the
system memory completely, though. A <a
href="/Articles/764716/">patch set</a> (posted by Logan Gunthorpe with
contributions by Christoph Hellwig and Steve Wise)
has been in the works for some time that addresses this case for PCI
devices using peer-to-peer (P2P) transfers, with a focus on offering an
offload option for the NVMe fabrics target subsystem.</p>

<h4>PCI peer-to-peer memory concepts</h4>

<p>PCI devices expose memory to the host system in form of memory regions
defined by <a href="https://wiki.osdev.org/PCI#Base_Address_Registers">base
address registers</a> (BARs). Those are regions mapped into the host's
physical memory space.  All regions are mapped into the same address space,
and PCI DMA operations can use those addresses directly. It is thus
possible for a driver to configure a PCI DMA operation to perform transfers
between the memory zones of two devices while bypassing system memory
completely.
The memory region might be on a third device, in which case two transfers
are still required, but even in that case 
the advantage is lower load on the system CPU, decreased memory
usage, and possibly lower PCI bandwidth usage. In the specific case of the
<a
href="https://nvmexpress.org/wp-content/uploads/NVMe_over_Fabrics_1_0_Gold_20160605-1.pdf">NVMe
fabrics target [PDF]</a>, the data is transferred from a <a
href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">remote
direct memory access</a> (RDMA) network interface to a special memory
region, then to the NVMe drive directly.</p>

<p>The difficulty is in obtaining the addresses and communicating them to
the devices. This has been solved by introducing a new interface, called
"p2pmem",
that allows drivers to register suitable memory zones, discover zones that are
available, allocate from them, and map them to the devices.

Conceptually, drivers using P2P memory can play one or more of three
roles: provider, client, and orchestrator:
<p>
<ul class="spacylist">
<li>Providers publish P2P resources
(memory regions) to other drivers.  In
the NVMe fabrics implementation, this is the done by the NVMe PCI
driver that exports zones of the NVMe devices.

<li>Clients make use of the resources, setting up DMA transfers from
and to them.  In the NVMe fabrics implementation there are two
clients: the NVMe PCI driver accepts buffers in P2P memory, and the
RDMA driver uses it for DMA operations.


<li>Finally, orchestrators manage flows between providers and
clients; in particular, they collect the list of available memory regions and
choose the one to use.  In this implementation there are
also two orchestrators: NVMe PCI again, and the NVMe target that sets
up the connection between the RDMA driver and the NVMe PCI device.
</ul>
<p>
Other scenarios are
possible with the proposed interface; in particular, the memory region may be
exposed by a third device. In this case two transfers will still be 
required, but without the use of the system memory.</p>

<h4>Driver interfaces</h4>

<p>For the provider role, registering device memory as being available for
P2P transfers takes place using:</p>
<pre>
    int pci_p2pdma_add_resource(struct pci_dev *pdev, int bar, size_t size,
				u64 offset);
</pre>

<p>The driver specifies the parameters of the memory region (or parts of it).
The zone will be represented by <tt>ZONE_DEVICE</tt> <tt>page</tt>
structures associated with the device. 
When all resources are registered, the driver may publish them to make them
available to orchestrators with:</p>
<pre>
    void pci_p2pmem_publish(struct pci_dev *pdev, bool publish);
</pre>

<p>In the orchestrator role, the driver must create a list of all clients
participating in a specific transaction so that a suitable range of P2P
memory can be found.  To that end, it should build that list with:
<p>
<pre>
    int pci_p2pdma_add_client(struct list_head *head, struct device *dev);
</pre>

<p>The orchestrator can also remove the clients with
<tt>pci_p2pdma_remove_client()</tt>
and free the list completely with <tt>pci_p2p_client_list_free()</tt>:</p>
<pre>
    void pci_p2pdma_remove_client(struct list_head *head, struct device *dev);
    void pci_p2pdma_client_list_free(struct list_head *head);
</pre>

<p>When the list is finished, the orchestrator can locate a suitable memory
region available for all client devices with:</p>
<pre>
    struct pci_dev *pci_p2pmem_find(struct list_head *clients);
</pre>
<p>
The choice of provider is determined by its "distance",
defined as the number of hops in the PCI tree between two devices.
It is zero if the two devices are the same, four if they are behind the same
switch (up to the downstream port of the switch, up to the common upstream,
then down to the other downstream port and the final hop to the device).
The closest (to all clients) suitable provider will be chosen; if there
is more than one at the same distance, one will be chosen at random
(<a href="/ml/linux-kernel/e9739430-b238-2078-a51f-d76bae5342d1%40deltatee.com/">to
avoid using the same one for all devices</a>). Adding new clients to the
list after locating the provider is possible if they are compatible; adding
incompatible clients will fail.</p>

<p>There is a different path for the orchestrators that know which
provider to use or that want to use different criteria for the choice. In such
case, the driver should verify that the provider has available P2P memory
with:</p>
<pre>
    bool pci_has_p2pmem(struct pci_dev *pdev);
</pre>

<p>Then it can calculate the cumulative distance from its clients to the
memory with:</p>
<pre>
    int pci_p2pdma_distance(struct pci_dev *provider, struct list_head *clients,
			    bool verbose);
</pre>

<p>When the orchestrator has found the desired provider, it can assign that
provider
to the client list using:</p>
<pre>
    bool pci_p2pdma_assign_provider(struct pci_dev *provider,
    				    struct list_head *clients);
</pre>
<p>
This call returns false if any of the clients are unsupported.
After the provider has been selected, the driver can allocate and
free memory for DMA 
transactions from that device using:</p>
<pre>
    void *pci_alloc_p2pmem(struct pci_dev *pdev, size_t size);
    void pci_free_p2pmem(struct pci_dev *pdev, void *addr, size_t size);
</pre>

<p>Additional helpers exist for allocating scatter-gather lists with P2P
memory:</p>
<pre>
    pci_bus_addr_t pci_p2pmem_virt_to_bus(struct pci_dev *pdev, void *addr);
    struct scatterlist *pci_p2pmem_alloc_sgl(struct pci_dev *pdev, unsigned int *nents,
 					     u32 length);
    void pci_p2pmem_free_sgl(struct pci_dev *pdev, struct scatterlist *sgl);
</pre>

<p>While passing the P2P memory for DMA, the addresses must be PCI bus
addresses. The users of the memory (clients) need to change their DMA
mapping routine to:</p>
<pre>
    int pci_p2pdma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
			  enum dma_data_direction dir);
</pre>
A driver using P2P memory will use <tt>pci_p2pmem_map_sg()</tt> instead of
<tt>dma_map_sg()</tt>. This routine is lighter, it just adjusts the bus
offset, as the P2P uses bus addresses. To determine which mapping functions
to use, drivers can benefit from this helper:</p>
<pre>
    bool is_pci_p2pdma_page(const struct page *page);
</pre>

<h4>Special properties</h4>

<p>One of the most important tradeoffs the authors faced was finding out
which hardware system configurations can be expected to work for P2P DMA
operations. In PCI, each <a
href="https://en.wikipedia.org/wiki/Root_complex">root complex</a> 
defines its own hierarchy. Some complexes do not support peer-to-peer
transfers between different hierarchies and there is no reliable way to
find out if they do (see the PCI Express specification r4.0, section 1.3.1).
The authors have decided to allow the P2P functionality only if all
devices involved are behind the same PCI host bridge; otherwise the user would
be required to understand their PCI topology and understand all devices
in their system. This restriction may be lifted with time.</p>

<p>Even so, the configuration requires user intervention, as it is
necessary to to
pass the kernel parameter <tt>disable_acs_redir</tt> that was <a
href="/ml/linux-kernel/20180730161840.13733-1-logang%40deltatee.com/">introduced
in 4.19</a>. This disables certain parts of the PCI access control services
functionality that might redirect P2P requests (the low-level details have
been deeply <a
href="/ml/linux-kernel/20180423233046.21476-5-logang%40deltatee.com/">discussed
earlier</a> in the development of this patch set).</p>

<p>P2P memories have special properties &mdash; they are I/O memories
without side effects (they are not device-control registers) and they are
not cache coherent. The code handling
those memories should be prepared and avoid passing this memory to code
that is not. <tt>iowrite*()</tt> and <tt>ioread*()</tt> are not necessary,
as there are no side effects, but if the driver needs a spinlock to protect
its accesses, it should use <tt>mmiowb()</tt> before unlocking. There are
currently no checks in the kernel to ensure the correct usage of this
memory.</p>

<h4>Other subsystem changes</h4>

<p>Using P2P transfers with the NVMe subsystem required some changes in
other subsystems, too. The block layer gained an additional flag,
<tt>QUEUE_FLAG_PCI_P2P</tt>, to indicate that the specific queue can target
P2P memory. A driver that submits a request using P2P memory should make
sure that this flag is set on the target queue. There has been a discussion
if there should be an additional check, but the developers <a
href="/ml/linux-kernel/34d9b2f7-9e6d-4a0a-77e5-ec0e99610f5c@kernel.dk/">decided
against it</a>.</p>

<p>The NVMe driver was modified to use the new infrastructure; it also
serves as an example of the implementation. The NVMe controller memory
buffer (CMB) functionality, which is memory in the NVMe device that can be
used to store commands or data, has been
changed to use P2P memory. This means that, if P2P memory is not
supported, the NVMe CMB functionality won't be available. The
authors find that reasonable, since CMB is designed for P2P operations in
the first place.
Another change is that the request queues can benefit from P2P memory
too.</p>

<p>RDMA, which is used for the NVMe fabrics, is now using flags to
indicate if it should use P2P or regular allocations.
The NVMe fabrics target itself allows the system administrator to choose
to use P2P memory and specify the memory device using a configuration
attribute that can be a boolean or PCI device name. In the first case it
will use any suitable P2P memory, in the second &mdash; only from the
specific P2P memory device.</p>

<h4>Current state</h4>

<p>The patch set has been under review for months now (see <a
href="https://www.snia.org/sites/default/files/SDC/2018/presentations/Storage_Architecture/Bates_Stephen_Accelerating_Storage_with_NVM_Express_SSDs_and_P2PDMA.pdf">this
presentation [PDF]</a>), and the authors provide a long list of hardware it
has been tested with.  The pace of this patch set (up to <a
href="/ml/linux-kernel/20180927165420.5290-1-logang@deltatee.com">version&nbsp;8</a>
as of this writing) is fast; it seems that it might be merged in the near
future.

<p>The patch set allows use cases that were not possible with the mainline
kernel before and opens a window for other use cases (P2P can be used with
graphics cards, for example). At this stage, the support is basic and there
are numerous modifications and extensions to be added in the future; one
direction will be to extend the range of supported configurations. Others
would be to hide the API behind the generic DMA operations and use the
optimization with other types of devices.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Device_drivers-Support_APIs">Device drivers/Support APIs</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#PCI">PCI</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Rybczynska_Marta">Rybczynska, Marta</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/767281/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor767487"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 2, 2018 21:51 UTC (Tue)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/767487/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<p>
Thanks Marta for the excellent article summing up where we are with P2PDMA. I also gave a summary talk of P2PDMA at SNIA's Storage Developer Conference in September. The slides for that talk should be available at this link <a href="https://tinyurl.com/y8sazb79">https://tinyurl.com/y8sazb79</a> and you might want to update the article to point to these slides as well as the older ones you mention.<br>
<p>
Stephen<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767487/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor767502"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">P2PDMA vs dmabuf?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2018 8:16 UTC (Wed)
                               by <b>shalem</b> (subscriber, #4062)
                              [<a href="/Articles/767502/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I wonder how this relates to dmabuf, esp. given the comment about using P2P with GPU-s where dmabuf is already used ?<br>
<p>
I guess dmabuf is tied to dmaing from/to main memory? So does P2PDMA allow (through e.g. some simple helpers) to use a dmabuf as source/dest of the P2P transfer?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767502/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767652"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">P2PDMA vs dmabuf?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 5, 2018 1:42 UTC (Fri)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/767652/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hey<br>
<p>
As I understand it dmabuf is all about exposing these buffers to userspace. P2PDMA is not quite ready to go that far but as we start looking at userspace interfaces we will definitely look at dmabuf.<br>
<p>
Oh and if you want to look at extending P2PDMA to tie into dmabuf we'd be more than happy to review that work!<br>
<p>
Cheers<br>
<p>
Stephen<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767652/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor767504"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">PCI devices</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2018 10:17 UTC (Wed)
                               by <b>epa</b> (subscriber, #39769)
                              [<a href="/Articles/767504/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
When the article talks about PCI devices, does it really mean the old style 32- or 64-bit wide, 33MHz or 66MHz bus?  Or should it be taken to include PCI Express (PCIe) as using the same kind of register setup, even though it's electrically rather different?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767504/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767544"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">PCI devices</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2018 16:22 UTC (Wed)
                               by <b>mrybczyn</b> (subscriber, #81776)
                              [<a href="/Articles/767544/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
In the PCI subsystem it is often understood as all variants, currently mainly PCI Express. The NVMe drives are only PCI Express, for example.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767544/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor767509"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2018 11:38 UTC (Wed)
                               by <b>dullfire</b> (guest, #111432)
                              [<a href="/Articles/767509/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There seems to be a mistake in the article.<br>
While I have not read the patch set, it would not make sense to require "all devices involved are behind the same PCI bridge".<br>
I suspect the term "PCI host bridge" was intended (because that would have the effect that the paragraph describes). Furthermore, since in PCIe all devices have their own PCI bridge (devices, not functions. Also, as a quick overview, a PCIe switch is made up of set of PCIe bridges, one for upstream... and one for each downstream port), it would effectively be impossible to have two PCIe device ever use this functionality. Which would render it moot.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767509/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767545"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2018 16:25 UTC (Wed)
                               by <b>mrybczyn</b> (subscriber, #81776)
                              [<a href="/Articles/767545/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yes, you're right. It would be more accurate to say "behind a host bridge". You will find more about it in the last part of the article when it talks about the use cases.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767545/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767636"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 4, 2018 21:13 UTC (Thu)
                               by <b>jgg</b> (subscriber, #55211)
                              [<a href="/Articles/767636/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
'behind the same bridge' is the right language, if not a little confusing. It doesn't mean 'behind the last bridge' but simply any bridge. Ie the upstream bridge of a switch is sufficient to satisfy the condition, even though there are later bridges before reaching the device.<br>
<p>
Behind the same root port (for PCI-E) is not quite the same thing, ie two functions on the same device cannot do P2P DMA with this patch series if they are plugged directly into the root port.<br>
<p>
All that aside, this series does have the requirement that the devices be behind a switch. You can't use it on a GPU and a NVMe drive plugged directly into root ports on your CPU, for instance. This greatly limits the utility, and hopefully will go away eventually when people can figure out how to white list root complexes and BIOSs that support this functionality.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767636/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor1001294"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Dec 7, 2024 11:51 UTC (Sat)
                               by <b>sammythesnake</b> (guest, #17693)
                              [<a href="/Articles/1001294/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<span class="QuotedText">&gt; two functions on the same device cannot do P2P DMA with this patch series if they are plugged directly into the root port</span><br>
<p>
That seems like a fairly likely use case - passing off some data from one stage of processing to another, so hopefully this restriction is lifted soon. I imagine that's a direction in the developers' sights, though - I'm happy to assume that my negligible level of domain knowledge is outdone by theirs ;-)<br>
<p>
A couple of possible factors that might make it less of an urgent need occur to me, though, how likely are these, I wonder...?<br>
<p>
1. How common would it be for these related functions to be plugged into the root, rather than sharing a (device internal?) bridge? <br>
<p>
2. I imagine such devices might simply share the memory between the stages and not need DMA at all for this kind of stage-to-stage handover...?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/1001294/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor767625"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 4, 2018 15:57 UTC (Thu)
                               by <b>willy</b> (subscriber, #9762)
                              [<a href="/Articles/767625/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Just because a parameter is called 'size' does not mean it should have type 'size_t'.  In this case, it's a length of a (subset of a) BAR, and it can easily be 64-bit on a 32-bit kernel. Should probably be phys_addr_t (even though it's a length, not an address).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767625/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767651"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 5, 2018 1:40 UTC (Fri)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/767651/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Willy<br>
<p>
Logan just submitted v9 today. Perhaps comment on that with your size_t concerns. All input gratefully received ;-).<br>
<p>
Stephen<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767651/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor767653"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 5, 2018 1:48 UTC (Fri)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/767653/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hey Marta<br>
<p>
One thing the article did not comment on is the ARCH specific nature of P2PDMA. While the framework is ARCH agnostic we do rely on devm_memremap_pages() which relies on ZONE_DEVICE which *is* ARCH specific (and in turn relies on MEMORY_HOTPLUG). Right now this includes x86_64 but not (for example) aarch64. Interestingly for some, we are looking at adding ARCH_HAS_ZONE_DEVICE for riscv because we see that ARCH as an interesting candidate for P2PDMA. <br>
<p>
Of course patches that add support for ZONE_DEVICE to other ARCH would be very cool.<br>
<p>
Cheers<br>
<p>
Stephen<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767653/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor767758"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Device-to-device memory-transfer offload with P2PDMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 6, 2018 15:23 UTC (Sat)
                               by <b>mrybczyn</b> (subscriber, #81776)
                              [<a href="/Articles/767758/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hello Stephen,<br>
You're right, there is the dependency on ZONE_DEVICE that I didn't mention as I think it's not going to matter for most potential users. The addition of support for other architectures and future integration with other subsystems (enabling usage with GPUs...) may be a subject for a follow-up.<br>
<p>
Cheers<br>
Marta<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/767758/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor992365"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">size requirement for pci_p2pdma_add_resource()?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 1, 2024 3:50 UTC (Tue)
                               by <b>KCLWN</b> (guest, #173781)
                              [<a href="/Articles/992365/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I am calling pci_p2pdma_add_resource() with a portion of a 32MB BAR memory.  I've been successful using size values of 16MB, 28MB, 30MB and 32MB.  For size values of 29MB and 31MB, I get the following failure. Does the size value to pci_p2pdma_add_resource() need to be multiple of 2MB (large page size)?<br>
<p>
[  472.762396] ------------[ cut here ]------------<br>
[  472.762400] Misaligned __add_pages start: 0x600da000 end: 0x600dbeff<br>
[  472.762409] WARNING: CPU: 30 PID: 199 at mm/memory_hotplug.c:395 __add_pages+0x121/0x140<br>
[  472.762420] Modules linked in: dre_drv(OE+) qrtr cfg80211 intel_rapl_msr intel_rapl_common amd64_edac edac_mce_amd kvm_amd kvm binfmt_misc irqbypass dax_hmem cxl_acpi rapl cxl_core nls_iso8859_1 ipmi_ssif ast i2c_algo_bit acpi_ipmi i2c_piix4 ccp k10temp ipmi_si ipmi_devintf ipmi_msghandler joydev input_leds mac_hid dm_multipath msr efi_pstore nfnetlink dmi_sysfs ip_tables x_tables autofs4 btrfs blake2b_generic raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c raid1 raid0 hid_generic rndis_host usbhid cdc_ether usbnet hid mii crct10dif_pclmul crc32_pclmul polyval_clmulni polyval_generic ghash_clmulni_intel sha256_ssse3 sha1_ssse3 i40e nvme nvme_core ahci nvme_auth xhci_pci libahci xhci_pci_renesas aesni_intel crypto_simd cryptd [last unloaded: dre_drv(OE)]<br>
[  472.762567] CPU: 30 PID: 199 Comm: kworker/30:0 Tainted: G        W  OE      6.8.0-45-generic #45-Ubuntu<br>
[  472.762573] Hardware name: Supermicro AS -2025HS-TNR/H13DSH, BIOS 1.6a 03/28/2024<br>
[  472.762576] Workqueue: events work_for_cpu_fn<br>
[  472.762584] RIP: 0010:__add_pages+0x121/0x140<br>
[  472.762591] Code: bc c6 05 aa 6b 5c 01 01 e8 2c e4 f7 fe eb d3 49 8d 4c 24 ff 4c 89 fa 48 c7 c6 70 57 84 bc 48 c7 c7 50 42 e8 bc e8 ef ed ec fe &lt;0f&gt; 0b eb b4 0f b6 f3 48 c7 c7 50 02 84 bd e8 0c e8 6f ff eb b6 66<br>
[  472.762595] RSP: 0018:ff6b7dd90cedfbc0 EFLAGS: 00010246<br>
[  472.762600] RAX: 0000000000000000 RBX: 00000000600da000 RCX: 0000000000000000<br>
[  472.762604] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000<br>
[  472.762607] RBP: ff6b7dd90cedfbf0 R08: 0000000000000000 R09: 0000000000000000<br>
[  472.762609] R10: 0000000000000000 R11: 0000000000000000 R12: 00000000600dbf00<br>
[  472.762612] R13: ff6b7dd90cedfca0 R14: 0000000000000000 R15: 00000000600da000<br>
[  472.762615] FS:  0000000000000000(0000) GS:ff3edc4137a00000(0000) knlGS:0000000000000000<br>
[  472.762619] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033<br>
[  472.762623] CR2: 00007ffcfc44b9c0 CR3: 0000005453a26001 CR4: 0000000000f71ef0<br>
[  472.762626] PKRU: 55555554<br>
[  472.762629] Call Trace:<br>
[  472.762632]  &lt;TASK&gt;<br>
[  472.762638]  ? show_regs+0x6d/0x80<br>
[  472.762645]  ? __warn+0x89/0x160<br>
[  472.762653]  ? __add_pages+0x121/0x140<br>
[  472.762659]  ? report_bug+0x17e/0x1b0<br>
[  472.762668]  ? handle_bug+0x51/0xa0<br>
[  472.762673]  ? exc_invalid_op+0x18/0x80<br>
[  472.762678]  ? asm_exc_invalid_op+0x1b/0x20<br>
[  472.762688]  ? __add_pages+0x121/0x140<br>
[  472.762696]  add_pages+0x17/0x70<br>
[  472.762702]  arch_add_memory+0x45/0x60<br>
[  472.762708]  pagemap_range+0x232/0x420<br>
[  472.762717]  memremap_pages+0x10e/0x2a0<br>
[  472.762722]  ? srso_alias_return_thunk+0x5/0xfbef5<br>
[  472.762730]  devm_memremap_pages+0x22/0x70<br>
[  472.762736]  pci_p2pdma_add_resource+0x1c7/0x560<br>
[  472.762744]  ? srso_alias_return_thunk+0x5/0xfbef5<br>
[  472.762750]  ? DRE_dmDevMemAlloc+0x44a/0x580 [dre_drv]<br>
[  472.762811]  DRE_drvProbe+0xc07/0xf30 [dre_drv]<br>
[  472.762852]  local_pci_probe+0x44/0xb0<br>
[  472.762859]  work_for_cpu_fn+0x17/0x30<br>
[  472.762864]  process_one_work+0x16c/0x350<br>
[  472.762872]  worker_thread+0x306/0x440<br>
[  472.762881]  ? __pfx_worker_thread+0x10/0x10<br>
[  472.762887]  kthread+0xef/0x120<br>
[  472.762893]  ? __pfx_kthread+0x10/0x10<br>
[  472.762899]  ret_from_fork+0x44/0x70<br>
[  472.762904]  ? __pfx_kthread+0x10/0x10<br>
[  472.762910]  ret_from_fork_asm+0x1b/0x30<br>
[  472.762921]  &lt;/TASK&gt;<br>
[  472.762924] ---[ end trace 0000000000000000 ]---<br>
[  472.774397] ------------[ cut here ]------------<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992365/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2018, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
