        <!DOCTYPE html>
        <html lang="en">
        <head><title>The Ceph filesystem [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/258516/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/257828/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/258516/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>The Ceph filesystem</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Please consider subscribing to LWN</b>
<p>
Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit
<a href="/Promo/nst-nag1/subscribe">this page</a> to join up and keep LWN on
the net.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>November 14, 2007</br>
           </div>
<p>
Ceph is a distributed filesystem that is described as scaling from gigabytes to
petabytes of data with excellent performance and reliability.  The project
is LGPL-licensed, with plans to move from a
FUSE-based client into the kernel.  This led Sage Weil to post a <a
href="http://lwn.net/Articles/258307/">message to linux-kernel</a>
describing the project and looking for filesystem developers who might be
willing to help.  There are quite a few interesting features in Ceph which
might make it a nice addition to Linux.
</p>

<p>
Weil outlines why he thinks Ceph might be of interest to kernel hackers:
<p>
<div class="BigQuote">
I 
periodically see frustration on this list with the lack of a scalable GPL 
distributed file system with sufficiently robust replication and failure 
recovery to run on commodity hardware, and would like to think that--with 
a little love--Ceph could fill that gap.</div>  
<p>
The filesystem is well
described in a <a
href="http://www.usenix.org/events/osdi06/tech/full_papers/weil/weil_html/index.html">paper</a>
from the 2006 USENIX Operating Systems Design and Implementation conference. 
The project's <a href="http://ceph.sourceforge.net/">homepage</a> has the
expected mailing list, wiki, and source code repository along with a detailed
overview of the feature set.
</p>

<p>
Ceph is designed to be extremely scalable, from both the storage and
retrieval perspectives.  One of its main innovations is splitting up
operations on metadata from those on file data.  With Ceph, there are two
kinds of storage nodes, metadata servers (MDSs) and object storage devices
(OSDs), with clients contacting the type appropriate for the kind of
operation they are performing.  The MDSs cache the metadata for files and
directories, journaling any changes, and periodically writing the metadata
as a data object
to the OSDs.
</p>

<p>
Data objects are distributed throughout the available OSDs using a
hash-like function that allows all entities (clients, MDSs, and OSDs) to
independently 
calculate the locations of an object.  Coupled with an infrequently 
changing OSD cluster map, all the participants can figure out where the
data is stored or where to store it.
</p>

<p>
Both the OSDs and MDSs rebalance themselves to accommodate changing
conditions and usage patterns.  The MDS cluster distributes the cached
metadata throughout, possibly replicating metadata of frequently used
subtrees of the filesystem in multiple nodes of the cluster.  This is done
to keep the workload evenly balanced throughout the MDS cluster.  For
similar reasons, the OSDs automatically migrate data objects onto storage devices that
have been newly added to the OSD cluster; thus distributing the workload
by not allowing new devices to sit idle.  
</p>

<p>
Ceph does N-way replication of its data, spread throughout the cluster.
When an OSD fails, the data is automatically re-replicated throughout the
remaining OSDs.  Recovery of the replicas can be parallelized because both
the source and destination are spread over multiple disks.  Unlike some other
cluster filesystems, Ceph starts from the assumption that disk failure will
be a regular occurrence.  It does not require OSDs to have RAID or other
reliable disk systems, which allows the use of commodity hardware for the
OSD nodes.
</p>

<p>
In his linux-kernel posting, Weil describes the
current status of Ceph:
<p>
<div class="BigQuote">I would describe the code base 
(weighing in at around 40,000 semicolon-lines) as early alpha quality: 
there is a healthy amount of debugging work to be done, but the basic 
features of the system are complete and can be tested and 
benchmarked.</div>
<p>
In addition to creating an in-kernel filesystem for
the clients (OSDs and MDSs run as userspace processes), there are several
other features &ndash; notably snapshots and security &ndash; listed as needing work. 
</p>

<p>
Originally the topic of Weil's PhD. thesis, 
Ceph is also something that he
hopes to eventually use at a web hosting company he helped start before
graduate school:
<div class="BigQuote">
We spend a lot of money on storage, and the proprietary products out there 
are both expensive and largely unsatisfying.  I think that any 
organization with a significant investment in storage in the data center 
should be interested [in Ceph].  There are few viable open source options once you 
scale beyond a few terabytes, unless you want to spend all your time 
moving data around between servers as volume sizes grow/contract over 
time.
</div>
</p>

<p> Unlike other projects, especially those springing from academic
backgrounds, Ceph has some financial backing that could help it get to a
polished state more quickly.  Weil is looking to hire kernel and filesystem
hackers to get Ceph to a point where it can be used reliably in production
systems.  Currently, he is sponsoring the work through his web hosting
company, though an independent foundation or other organization to foster
Ceph is a possibility down the road.  </p>

<p>
Other filesystems with similar feature sets are available for Linux, but
Ceph takes a fundamentally different approach to most of them.  For those
interested in filesystem hacking or just looking for a reliable solution
scalable to multiple petabytes, Ceph is worth a look.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems-Ceph">Filesystems/Ceph</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/258516/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor258672"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 15:05 UTC (Thu)
                               by <b>dion</b> (guest, #2764)
                              [<a href="/Articles/258672/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
This sounds a lot like Google FS, to the point where I'm sort of missing a comparison.

Could it be that the Ceph author read the GFS paper &lt;<a href="http://labs.google.com/papers/gfs.html">http://labs.google.com/papers/gfs.html</a>&gt;
and reimplemented the ideas?

... not that is a bad thing, many great pieces of code have been written as  implementations
of other peoples ideas, some times better than the original.


</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258672/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor258697"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 17:44 UTC (Thu)
                               by <b>i3839</b> (guest, #31386)
                              [<a href="/Articles/258697/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
Probably not. I was thinking about a distributed filesystem too, and what I came up with
resembles Ceph quite a lot. That was a few years ago, never had the time or urge to implement
it. Point being, most design decisions are rather obvious when you set out the requirements.

</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258697/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor258735"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 18:12 UTC (Thu)
                               by <b>dion</b> (guest, #2764)
                              [<a href="/Articles/258735/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
I wasn't trying to dismiss Ceph as being uncreative in any way, I was just missing the
comparison with google fs.


</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258735/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor258746"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 18:34 UTC (Thu)
                               by <b>sayler</b> (guest, #3164)
                              [<a href="/Articles/258746/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
After reading the OSDI paper
(<a href="http://www.usenix.org/events/osdi06/tech/full_papers/weil/weil_html/index.html">http://www.usenix.org/events/osdi06/tech/full_papers/weil...</a>), Ceph tries
to solve a more general problem than GFS.  

Note that this neither makes GFS nor Ceph better than the other.  

Like many solutions to problems in distributed computing these days, GFS optimizes for
specific  workloads (the Ceph authors claim "Similarly, the Google File System is optimized
for very large files and a workload consisting largely of reads and file appends." -- check
out section 8 of the OSDI paper for more comparisons).  

In general, Ceph is a much conservative approach, wrt the file system interface.  The claim is
that the FS exported by Ceph is general purpose, exposes POSIX file semantics, and performs
well across a wider variety of workloads.  It will be hard to say whether this is *true*
(whatever that means) until it is used much more widely..

I made some comments on the Kernel Trap discussion of Ceph, but I'll repeat the high point
here:  it's cool to see research software GPL'd and targeted toward a general audience.

There are quite a few interesting local and distributed filesystem projects going on right now
(meaning, ones that are attempting to be more than research vehicles).  I look forward to
seeing btrfs, Hadoop's cluster FS, Ceph, and other projects which I have no doubt forgotten
about.. :)
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258746/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor258764"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 20:02 UTC (Thu)
                               by <b>zooko</b> (guest, #2589)
                              [<a href="/Articles/258764/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
There's my project -- <a href="http://allmydata.org">http://allmydata.org</a> Tahoe .  It is an open source distributed
filesystem with very nice security properties -- everything is encrypted and integrity-checked
and you can share or withhold access to any subtree of the filesystem with anyone.  It uses
erasure coding so that you can choose any M between 1 and 256 and any K between 1 and M such
that the file gets spread out onto M servers, where the availability of any K of the servers
is sufficient to make the file available to you.

Regards,

Zooko

</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258764/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor258794"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 15, 2007 23:31 UTC (Thu)
                               by <b>sayler</b> (guest, #3164)
                              [<a href="/Articles/258794/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
yay! erasure coding!
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258794/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor258810"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Google fs?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 16, 2007 3:40 UTC (Fri)
                               by <b>zooko</b> (guest, #2589)
                              [<a href="/Articles/258810/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
Yeah!  Also we've separately published the erasure coding library, which is derivative of
Luigi Rizzo's old library, but to which we added a Python interface, a command-line interface,
performance optimization, and other stuff.

<a href="http://pypi.python.org/pypi/zfec">http://pypi.python.org/pypi/zfec</a>



</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/258810/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
<a name="CommAnchor259612"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Cluster file systems</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Nov 22, 2007 11:34 UTC (Thu)
                               by <b>ringerc</b> (subscriber, #3071)
                              [<a href="/Articles/259612/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
I've long been frustrated by the lack of a solid, general purpose distributed cluster file
system for Linux. There's a real need even if you *don't* have terabytes of data and hundreds
of servers.

Right now, there's no good way to distribute things like home directories on the network, even
just between a few Linux servers. You either land up using a central server or set of servers
(failure prone, harder to expand, a pain to maintain, etc) or using a cluster FS that relies
on an iSCSI/AoE/FC shared storage backend (expensive, complex). You'll be lucky to find a
network file system that'll give you reliable home directories, either, with correct handling
of lockfiles, various app-specific databases, etc.

In short, even for a common and simple problem like making sure that users have the same home
directory across several machines, all the existing options seem to stink.

For that matter, even traditional options like using NFS to export homedirs from a central
server have, in my experience, been less than reliable. I'm unimpressed with Linux's NFS
support on both the server &amp; client side; I've seen too many unkillable processes,
unlinked-but-perpetually-undeleted files, etc, and I've had to *REBOOT* too many servers to
fix NFS issues. And that's without going into the issues with NFS's not-quite-POSIX FS
semantics.

A similar issue applies for virtualized servers. They need storage somewhere, and unless you
can afford a SAN that storage is going to be server based (whether internal or external, it
doesn't matter). There's always one box you can't bring down for maintainance without bringing
down some/all of your VMs. Being able to provide distributed storage for VMs would be quite
wonderful.

If I was really dreaming I'd want a native client for Mac OS X and for Windows, so that there
was no need to re-share the cluster FS though a gateway server. Experience suggests that such
native file system implementations are rarely solid enough to work live over the network with,
though, and are usually only good for copying files back and forth.

Even without that, just being able to collect the storage across the servers at my company
into a shared, internally redundant pool that would remain accessible if any one server went
down would be ... wonderful.
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/259612/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor379574"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DRBD?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 19, 2010 18:20 UTC (Fri)
                               by <b>niner</b> (subscriber, #26151)
                              [<a href="/Articles/379574/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You might want to have a look at DRBD (<a href="http://www.drbd.org">http://www.drbd.org</a>). It's a<br>
distributed, redundand block device which you can use instead of the<br>
expensive iSCSI/AoE/FC shared storage backend to base your cluster FS<br>
upon. It works quite well with good performance and excellent coverage of<br>
possible error conditions.<br>
<p>
Even without a cluster filesystem (and it's complexities), you can at<br>
least build a good shared nothing failover cluster for your central<br>
storage server.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/379574/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2007, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
