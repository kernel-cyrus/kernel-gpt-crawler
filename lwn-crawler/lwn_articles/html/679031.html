        <!DOCTYPE html>
        <html lang="en">
        <head><title>In-band deduplication for Btrfs [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/679031/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/678567/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/679031/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>In-band deduplication for Btrfs</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>We're bad at marketing</b>
<p>
We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please <a href="/Promo/nsn-bad/subscribe">subscribe today</a> to help us keep doing that, and so
we don’t have to get good at marketing.
</blockquote>
<div class="GAByline">
           <p>March 9, 2016</p>
           <p>This article was contributed by Neil&nbsp;Brown</p>
           </div>
<p>
  "In-band deduplication" is the process of detecting and unifying
  duplicate data blocks as files are being written, rather than at some
  later time.  Btrfs support for this feature has been under
  development since at least <a href="/Articles/550545/">early
2013</a>.  Quite recently 
it reached the point where developer Qu Wenruo thought it was
sufficiently ready to send Btrfs maintainer Chris Mason a  <a
href="http://thread.gmane.org/1455774178-3595-1-git-send-email-quwenruo@cn.fujitsu.com">pull
request</a> (as yet
unanswered)
hoping that it might be added to the kernel during the 4.6 merge
window.  While this is far from a magic bullet that will suddenly
remove all the waste in your filesystem that is caused by duplicate
data, there are use cases where it could bring real benefits.</p>

<h4>Offline and in-band</h4>

<p>It has been possible to unify duplicated blocks in Btrfs since Linux
3.12 when the <tt>BTRFS_IOC_FILE_EXTENT_SAME</tt> <tt>ioctl()</tt> command was <a href="http://git.kernel.org/torvalds/c/416161db9b63">added</a> (it has
since been <a href="http://git.kernel.org/torvalds/c/54dbc1517237">renamed
<tt>FIDEDUPERANGE</tt></a> when the basic <tt>ioctl()</tt> handling was moved
  to the VFS, so it could someday be used by other filesystems).  This
<tt>ioctl()</tt> is given a range of bytes in the file (which must be aligned to
the filesystem block size) and a list of offsets in other files (or
possibly the same file).  This identifies two or more ranges of the
same size that are claimed to be identical.  Btrfs reads all the
blocks, checks that they are in fact identical, and then changes its
internal bookkeeping so that all files point to one set of blocks
on the storage medium.  The space used by the other copies of the data
will typically become free space available for reallocation.</p>

<p>
  To understand how this works it is necessary to understand how Btrfs
  makes use of "<a
  href="https://en.wikipedia.org/wiki/Btrfs#Extents">extents</a>".  An
  extent is a contiguous set of blocks on 
  the storage device and also a contiguous set of blocks in a file (at
  least initially).  It has a "logical address" that is mapped to a
  physical location on one, or perhaps more, of the underlying devices.
  An extent can become part of several different files, either through
  snapshots, reflinks, or deduplication.  To allow this storage space
  to ultimately be reused, Btrfs maintains a reference count for each
  extent and will not re-use any of the space until that count becomes
  zero.

<p>
  It is not generally possible to split an extent.  If a single block
  is written into the middle of a large extent, the "copy on write" design
  of Btrfs requires that either the whole extent be copied or that

the file's indexing
  information be changed to point to the first half of the extent,
  then the new block, then the remainder of the extent.  Btrfs takes the
  latter approach.

<p>
When
  <tt>FIDEDUPERANGE</tt> is used, the file indexes will be updated to point
  to the 
  relevant extents or partial extents of the source file, and the
  reference counts on the various extents will be increased or decreased
  as appropriate.  This may not release quite as much space as
  expected since there may have been extents that were only partially
  identical between two files.  The identical part in one extent may
  have no files including it any more, but the space will not be freed
  until the whole extent is no longer referenced.


<p><a href="https://github.com/markfasheh/duperemove"><code>duperemove</code></a> is a tool that can be used to examine a set of files,
look for duplicated regions, and call the ioctl to remove that
duplication from the underlying storage.  This can be useful anywhere
that files are likely to be completely or largely the same, but where
they need to be kept separate for administrative or other practical reasons.  A common use case is filesystem images used for
virtualization or the multiple similar-but-not-identical file sets 
used by containers.  Running <code>duperemove</code> from time to time
could save 
a lot of disk space.</p>

<p>This functionality was referred to in the initial commit as &quot;offline
deduplication&quot;, which is a little confusing since &quot;offline&quot; in the
context of filesystems usually implies that the filesystem isn't
mounted, and that is not the case here.  It is more like
&quot;on-demand&quot; deduplication and is <a href="https://btrfs.wiki.kernel.org/index.php/Deduplication">sometimes</a> referred to as &quot;batch&quot;
deduplication.  This contrasts with the new work that could be called
&quot;time-of-write&quot; or transparent deduplication, but is called &quot;in-band&quot;
deduplication in this patch set.</p>

<p>When <code>duperemove</code> runs, it computes a hash value for every
block in 
every file and then compares these, looking for ranges of matching
blocks.  When it finds suitable ranges, it requests the deduplication.
In-band deduplication uses the same idea of hashing blocks of data,
but in many other respects it is quite different.</p>

<p>
    In-band duplication works in units of extents of a specific size — a
    deduplication block size of up to 8MB can be configured; the default is
    128KB. When data is written out to storage, blocks are gathered
    into extents of this size whenever possible.  If there are only
    sufficient blocks being written for a smaller extent, that extent will
    be written without deduplication.  For
    every extent of the target size, the hash (currently SHA-256, though
    possibly configurable in the future) is calculated. If another
    extent can be found with the same hash, then a new reference is
    added to that extent and the new data is not written. If no match is
    found, the data is written as normal and the hash is stored together
    with the logical address of the extent so that it is available for
    future matching.
</p>

<p>
   This process can be enabled for a whole Btrfs filesystem using the
   "<tt>btrfs&nbsp;dedup&nbsp;enable</tt>" command and then disabled for
   individual files 
   or directories by using the "<tt>btrfs&nbsp;property&nbsp;set</tt>"
   command to set the 
   "<tt>dedup</tt>" property to "<tt>disable</tt>".  Setting this on a
   directory causes 
   all files or directories created within that directory to inherit the
   setting.  This means that enabling deduplication on only a 
   subset of a filesystem is possible, but a little bit clumsy.

<h4>Two back-ends</h4>

<p>There are two separate back-ends for storing the mapping between
hashes and extents; the fact that the implementation of these
back-ends is kept cleanly separate from their use is rightfully
highlighted as a strength of this patch set.</p>

<p>One of the two back-ends is an in-memory mapping.  Two <a
href="/Articles/184495/">red-black trees</a> are
created when the filesystem is mounted;  whenever new data is
written to a file on which deduplication is enabled, new entries are added to
the trees.  One tree maps from hash to logical address and is used to
see if a suitable extent exists that already stores the required data.
The second provides a reverse mapping that allows hash entries to be
deleted when an extent is freed after its reference count reaches
zero.  The size of these trees is limited by a configurable entry
count that defaults to 32768.  It would not be surprising to see that
replaced or augmented with a &quot;<a
href="/Articles/550463/">shrinker</a>&quot; callback from the memory
management subsystem, so that it could shrink only when memory is tight.</p>

<p>The second mechanism stores these mappings in the filesystem.  Btrfs
has flexible data structures for storing all sorts of metadata
and data on disk using a number of distinct B-trees, one for each
<a
href="/Articles/579009/">subvolume</a>, one for the extent reference counts,
one for storing checksums, 
etc.  The in-band deduplication patchset adds another B-tree that has
two types of keys: one for lookup by hash and another for lookup by extent
address.</p>

<p>Looking up the hash in the B-tree is not quite so straightforward as
one might hope, since Btrfs has a fixed format for the lookup key: a 64-bit
object-ID, an eight-bit type, and a 64-bit offset. 
It is not possible to store
the full 256-bit hash in the key and even storing 128 bits in the two
64-bit fields would be problematic. Btrfs requires that all keys be
unique, so that approach would effectively limit the hash size to
128 bits.</p>

<p>The approach chosen is to store 64 bits of the hash in the object-ID,
and the logical address of the extent in the offset field. Each key is
accompanied by a variable-length data field and this is used to store
the full hash. If the hashes of two extents collide in those 64 bits,
the offsets will still be different, so the keys will be unique.

<p>
    To handle these collisions a lookup first performs a regular B-tree
    search for a key with the appropriate hash bits in the object-ID and
    <tt>U64_MAX</tt> in the offset field.  This will provide the last possible
    location where the target hash could be stored.  A linear search is
    then performed searching backward and comparing the full hash until
    a match is found or there are no more keys with the required hash
    fragment.

<p>Unlike with in-memory lookup there is no mechanism to limit the number
of hash entries stored, beyond normal filesystem-full checks that
might prevent a new extent from being written.</p>

<p>One difference between this in-band deduplication and the on-demand
approach that is worth highlighting is the dependence placed on the
hash.  <code>duperemove</code> uses a hash only to guide the search for duplicate
blocks — the hash is not authoritative and Btrfs will not allow the
deduplication to happen if the identified regions are not
byte-for-byte identical.  In-band duplication as currently implemented
does not perform that comparison. If the hash matches, then the
extents are assumed to match.
This means the correctness of the
   deduplication is completely dependent on the uniqueness of the hash,
   so the extra effort to make use of all 256 bits is easy to justify.
   Whether that complete dependence is itself justified is not an easy
   question to answer.  It is certainly <i>extremely</i> unlikely for two
   distinct blocks to have the same hash, but it is also certainly quite
   possible.  It would only need to cause corruption once to be
   extremely embarrassing.  Adding byte-for-byte comparison is <a
   href="http://mid.gmane.org/56D8E42C.6030106@cn.fujitsu.com">on the 
   planned feature list</a> but, according to Wenruo, "not anytime soon".
 
<p>In-band deduplication brings the benefit of being automatic, but has a cost
that it will probably miss duplication that <code>duperemove</code> could find.
Different alignment of extents between files would completely defeat
the duplicate detection, as would creating files before deduplication
was enabled.  The former could be improved to some degree with a
smaller extent size, though that would brings costs of its own.  As is so
often the case, finding the most effective solution — which could include a
mix of offline and in-band deduplication — will be highly
dependent on each particular use case.</p>

<h4>Test, test, and test</h4>

<p>Wenruo assures us that this patch set has seen quite a lot of testing
and that it has been some time since the last of the bugs found
by that testing was fixed, which is encouraging.  Some <a href="http://thread.gmane.org/1456301196-15874-1-git-send-email-quwenruo@cn.fujitsu.com">new tests</a> have
been submitted for the <a href="http://oss.sgi.com/cgi-bin/gitweb.cgi?p=xfs/cmds/xfstests.git;a=summary">xfstests</a> test suite specifically to exercise the
deduplication and to check for some of the bugs that have been found
and fixed.</p>

<p>One aspect of testing that seemed strangely absent from the pull request
was any hint of how in-band deduplication affects performance.  It is to be expected
that this sort of functionality would slow writes down, particularly
when the table of hashes is not kept in memory.  It could also speed
up some writes if lots of duplicate extents are found.  Some
indication of the sort of performance change experienced would
certainly help to complete the picture.</p>

<p>But maybe the hope is to crowdsource that testing.  There are so many
different possible hardware configurations and usage scenarios to test that
it is hard for one developer to even begin to give meaningful
results.  A large community, on the other hand, can try lots of things
in parallel.  As Wenruo noted in his pull request, there is still work to
be done, but it should be quite ready for people to test.</p>

<p>Trying it out requires some <a
href="http://thread.gmane.org/1455776300-11234-1-git-send-email-quwenruo@cn.fujitsu.com">patches</a>
to the <code>btrfs-progs</code> along with the Git
tree from the pull request, but that should be no challenge for those
who enjoy compiling their own kernels.   I'm sure additional results would be
most welcome.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Btrfs">Btrfs</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems-Btrfs">Filesystems/Btrfs</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Brown_Neil">Brown, Neil</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/679031/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor679545"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 10, 2016 14:00 UTC (Thu)
                               by <b>martin.langhoff</b> (subscriber, #61417)
                              [<a href="/Articles/679545/">Link</a>] (17 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
  At first blush the performance cost and collision risk of the technique described make me uneasy...<br>
<p>
There's a known pattern of calculating a fast cheap hash in-band during writes, which is later used by an offline process to assess dedupe candidates.<br>
<p>
The fast hash will have some false positives, that's OK. It's merely a low cost guide for the batch or idle process to pick candidates. The office process performs a full comparison, and might have additional rules, such as skipping files with high rates of change.<br>
<p>
Perhaps that's a better approach? <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679545/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679552"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 10, 2016 14:57 UTC (Thu)
                               by <b>jtaylor</b> (subscriber, #91739)
                              [<a href="/Articles/679552/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I guess the in-band deduplication has the goal of avoiding a duplicated writes completely before they even go to disk.<br>
<p>
Your suggested approach is basically batch duplication, btrfs already stores cheap hashes with its data, a userspace program could use these too relatively quickly find candidates for the dedupe ioctl to deduplicate safely.<br>
duperemove basically does this, except it cannot yet do incremental deduplication.<br>
It could probably be done by using btrfs subvolume find-new command that can list what has changed since a certain filesystem generation<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679552/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor679634"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 10, 2016 18:41 UTC (Thu)
                               by <b>roblucid</b> (guest, #48964)
                              [<a href="/Articles/679634/">Link</a>] (15 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Presumably it's expected that the HW acceleration will be available for the common SHA-256 algorithmn, which is in kernel, so the costs of hashing an extent may be less than feared on many core CPUs<br>
<p>
I would guess avoiding the bit for bit comparison is done for performance reasons, on the pragmatic basis that collisions, even if there's only really 64bits of entropy are going to be very, very rare :)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679634/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679641"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 10, 2016 19:01 UTC (Thu)
                               by <b>martin.langhoff</b> (subscriber, #61417)
                              [<a href="/Articles/679641/">Link</a>] (14 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Very rare is a statement of probability. You can't play probability games with filesystems because you roll the dice so many times that you will hit the jackpot.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679641/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679678"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 10, 2016 23:23 UTC (Thu)
                               by <b>nybble41</b> (subscriber, #55106)
                              [<a href="/Articles/679678/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Even if you generated a new 256-bit hash every picosecond (10^12 hashes per second), it would be 10^19 years before the probability of a collision reached 50%, taking into account the Birthday Paradox. That is over 700 million times the current age of the universe, with a 50% probability of *one* collision. The probability of finding any collisions is still less than 10^-9 after 500 trillion (5*10^14) years.<br>
<p>
Even filesystems don't "roll the dice" often enough to make 256-bit hash collisions a serious consideration.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679678/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679723"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 14:05 UTC (Fri)
                               by <b>bgoglin</b> (subscriber, #7800)
                              [<a href="/Articles/679723/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Unlikely doesn't mean it won't ever happen.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679723/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679728"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 15:24 UTC (Fri)
                               by <b>nybble41</b> (subscriber, #55106)
                              [<a href="/Articles/679728/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
For all practical purposes, in this case it does mean exactly that. It is far, far more likely that the filesystem will suffer catastrophic failure from some other cause (for example, a freak surge in cosmic gamma radiation sufficient to wipe out human civilization) than that you will ever see a SHA-256 hash collision occur by random chance in the context of a single filesystem. It is so far down the list of things to worry about that developers would be more productively employed working on almost anything else compared to implementing bit-for-bit block comparison to supplement the SHA-256 match.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679728/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679762"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 15:30 UTC (Fri)
                               by <b>micka</b> (subscriber, #38720)
                              [<a href="/Articles/679762/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
So, nobody does cryptanalysis on SHA-256 and try to create attacks on it ?<br>
OK, that's not by random chance anymore, but...<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679762/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679769"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 16:25 UTC (Fri)
                               by <b>nybble41</b> (subscriber, #55106)
                              [<a href="/Articles/679769/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Sure, cryptoanalysis uncovering weaknesses in the SHA-256 algorithm is a possibility. I was replying only to the "it's not mathematically impossible, ergo it must be treated as a realistic possibility" argument. However, if attackers can arrange for a SHA-256 hash collision on demand then I think we'll have bigger problems to worry about than some corrupted filesystem data. There are also various well-known methods to thwart attacks dependent on predictable hash results, like seeding the hash function with a hidden per-filesystem salt, and in the event that such an attack is discovered the workaround is simple: just disable online deduplication until the hash function can be updated.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679769/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor680180"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 15, 2016 16:10 UTC (Tue)
                               by <b>intgr</b> (subscriber, #39733)
                              [<a href="/Articles/680180/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; However, if attackers can arrange for a SHA-256 hash collision on demand then I think we'll have bigger problems to worry about</font><br>
<p>
I disagree, that actually is a very scary failure mode for a file system. If an attacker is allowed to influence what gets stored in a file system, then a preimage attack or possibly a clever application of a collision attack would allow poisoning the file system.<br>
<p>
For instance, an attacker knows that some user wants to store document A on the system. The attacker can prepare a colliding document B and upload it before the user gets the chance to upload A. When document A is written later, the file system will throw away A and keep the tampered document B instead.<br>
<p>
Consider that document A can be, for example, a system package update that the system administrator installs. Lulz ensues.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/680180/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor680201"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 15, 2016 17:26 UTC (Tue)
                               by <b>nybble41</b> (subscriber, #55106)
                              [<a href="/Articles/680201/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; &gt; However, if attackers can arrange for a SHA-256 hash collision on demand then I think we'll have bigger problems to worry about</font><br>
<font class="QuotedText">&gt; I disagree, that actually is a very scary failure mode for a file system.</font><br>
<p>
I wasn't trying to downplay the problems it would create for a filesystem, and I agree with everything else you said. However, SHA-256 hashes are used for more than just identifying blocks within filesystems for deduplication. The ability to create SHA-256 hash collisions would undermine the entire digital signature system, for example. Implementing a workaround is also easier in the context of deduplication—in the short term you can just turn it off while you reindex the drive with a different hash function. Unless the content of your filesystem is *really* important, the odds of anyone wasting a SHA-256 collision 0-day attack on it are vanishingly small, and even a major issue with the algorithm which cut the effective bit length in half would not represent an immediate practical threat.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/680201/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor681275"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 24, 2016 16:18 UTC (Thu)
                               by <b>nye</b> (subscriber, #51576)
                              [<a href="/Articles/681275/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;It is far, far more likely that the filesystem will suffer catastrophic failure from some other cause (for example, a freak surge in cosmic gamma radiation sufficient to wipe out human civilization) than that you will ever see a SHA-256 hash collision occur by random chance in the context of a single filesystem</font><br>
<p>
Somewhat more to the point, if you have a function which checks for duplicate data - whether by comparing the hashes, or comparing the data itself - the chance of a hash collision is less than the chance that random memory errors will happen to cause that function to return the wrong result. That is to say, the reliability of the byte-for-byte function is no greater than the compare-the-hashes function, at least when it comes to order of magnitude.<br>
<p>
In practical terms, if you have a pipeline which relies on every step operating correctly, there's not much point in paying an ongoing cost to improve the reliability of one given component if there are others that are orders of magnitude more likely to fail. Sure, technically it makes a difference, but only in the sense that you can make the ocean bigger by tipping a bucket of water into it.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/681275/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor679777"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 17:56 UTC (Fri)
                               by <b>ikm</b> (guest, #493)
                              [<a href="/Articles/679777/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; You can't play probability games with filesystems</font><br>
<p>
You always play probability games, whether you like it or not. Any digital circuit is a system with SNRs sufficiently high to quantize levels reliably, but this can't ever be 100% reliable. Data redundancy can be put on top of it to make it more reliable - however, there's still always a probability of state and data corruption in such a system. Once you realize that, the fears of having a hash collision reduce to the question of being able to calculate the respective probabilities and compare them with the other probabilities involved.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679777/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor679790"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 11, 2016 21:56 UTC (Fri)
                               by <b>oever</b> (guest, #987)
                              [<a href="/Articles/679790/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
git uses the SHA-1 hash of a file as the identifier. If there is a collision, the git repository breaks. Other threats to data integrity are much higher than the chance of an accidental collision. Git it also a deduplicated file system of sorts, so the same considerations apply.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679790/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679899"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 13, 2016 19:21 UTC (Sun)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/679899/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
And because those considerations apply to git, they apply to bup as well. Bup actually provides a tool to check the probability of such trouble. Let's run it over my backups for the last few years:<br>
<p>
56 matching prefix bits<br>
2.02 bits per doubling<br>
104 bits (51.38 doublings) remaining<br>
2.93716e+15 times larger is possible<br>
<p>
Everyone on earth could have 438382 data sets like yours, all in one<br>
repository, and we would expect 1 object collision.<br>
<p>
I am not scared of a collision.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679899/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor679980"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 14, 2016 15:29 UTC (Mon)
                               by <b>mathstuf</b> (subscriber, #69389)
                              [<a href="/Articles/679980/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I thought I read somewhere that someone had an object of 40 zeros and had some collision locally. Or was that just a potentiality?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/679980/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor680200"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 15, 2016 17:08 UTC (Tue)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/680200/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I've only heard of it as a possibility. Nobody has ever mentioned encountering a real collision, and frankly I'm not worried about one turning up in the foreseeable future.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/680200/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor696406"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 4, 2016 15:24 UTC (Thu)
                               by <b>JoeyUnknown</b> (guest, #110181)
                              [<a href="/Articles/696406/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It's unlikely, but it should still not be that difficult to read the data and compare both. For those that don't need the performance hit, that would be turned off and you would remove a dimension from your data structure (btree[hashkey]-&gt;bucket-&gt;blocks to btree[hashkey]-&gt;block).<br>
<p>
It should be a performance option. I can think of plenty of cases where for me a hash is fine. In some cases however, I don't really want to play dice with my data. Secondary to that, while right now the possibility of a collision is low, in future things can happen that might change that.<br>
<p>
In some cases, depending on scenario, I would rather a system that performs worse than a possibility of a bizarre hidden integrity failure which can make a heck of a mess. If there ever was a hash collision, chances are it wouldn't be detected. The data would just have to be rebuilt and repaired or something. It's just one less vetor to worry about when it comes to big data where integrity is sensitive.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/696406/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor680221"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 15, 2016 21:11 UTC (Tue)
                               by <b>robbe</b> (guest, #16131)
                              [<a href="/Articles/680221/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I, for one, welcome this SHA2@Home project trying to find a collision, however unlikely.<br>
<p>
But, the willingness to dedicate CPU cycles to pure science is waning – maybe we should promise the person who finds the first collision some riches. Let’s call it btrcoin.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/680221/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor681521"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 28, 2016 1:56 UTC (Mon)
                               by <b>quwenruo</b> (guest, #107900)
                              [<a href="/Articles/681521/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thanks for bringing this feature to the spotlight.<br>
<p>
I found that most comment is concerning on the collision of SHA256.<br>
That's normal, as when it's possible to have collision, it will happen one day.<br>
Personally I'm quite confident about current "strong" hash algorithm, but since users have the conern, then we need to address them, and just like ZFS, to provide a byte-by-byte comparison option for dedupe.<br>
<p>
Currently what we can do is, to add SHA512 to reduce the collision. Although SHA512 is originally planned to improve the hash speed on modern 64bit machines.<br>
<p>
But for now, we're focusing on polishing the ioctl interface and on-disk format with maintainers(Chris and David), hoping the in-memory backend can be merged in 4.7 first.<br>
<p>
<p>
For byte-by-byte comparison, it maybe be scheduled after all current backend merged.<br>
But since so many user worry about the SHA256 collision, we will keep it in mind and investigate from now on.<br>
<p>
Thanks,<br>
Qu<br>
<p>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/681521/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor696361"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">In-band deduplication for Btrfs</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 4, 2016 15:08 UTC (Thu)
                               by <b>JoeyUnknown</b> (guest, #110181)
                              [<a href="/Articles/696361/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
SHA is more relevant if you have a major security concern or something and if you're using an implementation that isn't collision safe (as SHA tends to avoid it more than more basic hash algorithms).<br>
<p>
I think that this should be made this to work safely first. It should not be an incomplete optimised solution first.<br>
<p>
A basic system doesn't need to use cryptographically secure but it does need to not have a risk of collision no matter how unlikely.<br>
<p>
Your hash can be really anything from CRC to the most significant bits of the block. Something that avoids excessive bucketing however would be ideal (IE all bits contribute more or less equally). That might be similar to what you get with cryptographic functions, but it doesn't mean you need a cryptographic function. It doesn't really matter for just making it work.<br>
<p>
Then when you have a match or matches, all you can do is a full content compare to confirm and use the match. You will need to be able to bucket because one key could match multiple blocks.<br>
<p>
Once you have something that works and that is safe, then you can start to look at optimising it as much as possible while still keeping it entirely reliable. Once you've reached the point you can go no longer with that, you should then add options for performance at the cost of risk and options for greater security.<br>
<p>
Options might be skipping the read check, using a more secure or large hash, writing blocks but then deferring deduplication as a background task, <br>
<p>
Partial oportunistic dedupelication is another option as well. For example, there's a very good chance when a block is written that is that same as another, well, if A[n] == B[n] the probability of A[n +- 1] == B[n +- 1] is very high. In this case, you would deduplicate where cheap and easy but let a scheduled task clean up the rest later. Another option might be hinting, that is, during a big operation to turn on dedupe in some manner. You could also prioritse the most common data. A zeroed out block is one of the obvious cases.<br>
<p>
You can also do some direct content indexing with various tree structure (IE, each byte or word) but that does not work well at all for random data. Those kind of trees more or less represent something akin to compression trees (and also similar to deduplication). Random data does not compress well at all so that would be an attack vector or risk of relying on a less lossy key compression based system.<br>
<p>
This isn't really a simple thing but I can say this, I would not feel comfortable using a solution vulnerable to hash collision no matter how unlikely it is, especially one that can't detect it. I might be sensitive but I would prefer a flawless system.<br>
<p>
There may be other similar systems out there you can look at that basically do the same but outside of the realm of file systems. Hash tables have collisions and need to detect them so that might be a place to start looking at.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/696361/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2016, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
