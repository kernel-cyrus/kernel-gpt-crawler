        <!DOCTYPE html>
        <html lang="en">
        <head><title>The RCU API, 2024 edition [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/988638/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/989983/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/988638/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>The RCU API, 2024 edition</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Please consider subscribing to LWN</b>
<p>
Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit
<a href="/Promo/nst-nag1/subscribe">this page</a> to join up and keep LWN on
the net.
</blockquote>
<div class="GAByline">
           <p>September 13, 2024</p>
           <p>This article was contributed by Paul McKenney</p>
           </div>
<p>Read-copy-update (RCU) is a synchronization mechanism that was added to
the Linux kernel in October 2002.
RCU is most frequently used as a
<a href="https://linuxfoundation.org/webinars/unraveling-rcu-usage-mysteries/">replacement for reader-writer locking</a>,
but is also used in
<a href="https://linuxfoundation.org/webinars/unraveling-rcu-usage-mysteries-additional-use-cases/">a number of other ways</a>.
This mechanism is notable in that RCU readers do not directly synchronize with
RCU updaters,
which makes RCU read paths extremely fast and also
permits RCU readers to accomplish useful work even
when running concurrently with RCU updaters.
Those wishing an in-depth introduction to RCU are invited to consult
the LWN series
<a href="http://lwn.net/Articles/262464/">here</a>,
<a href="http://lwn.net/Articles/263130/">here</a>,
and
<a href="http://lwn.net/Articles/264090/">here</a>.

<P>
This article covers recent changes to the RCU API; it was contributed by
Paul McKenney, Boqun Feng, Frederic Weisbecker, Joel Fernandes, Neeraj
Upadhyay, and Uladzislau Rezki. 


<p>Although the basic idea behind RCU has not changed during the
three decades following its introduction into DYNIX/ptx, the RCU API
has evolved significantly since the
<a href="http://lwn.net/Articles/418853/">2010</a>,
<a href="https://lwn.net/Articles/609904/">2014</a>,
and
<a href="https://lwn.net/Articles/777036/">2019</a>
editions of the Linux-kernel RCU API.
The most recent five years of this evolution is documented by the
following sections.

<ol>
<li>	<a href="#Summary of RCU API Changes">
	Summary of RCU API changes</a>
</li><li>	<a href="#How Did Those 2019 Predictions Turn Out?">
	How did those 2019 predictions turn out?</a>
</li><li>	<a href="#What Next for the RCU API?">
	What next for the RCU API?</a>
</li></ol>
<p>
If that is not enough RCU for you, there are a lot more details to be found
in the associated <a href="/Articles/988641/">background-material
article</a>.


<p>But first, a few announcements:

<ol class="spacylist">
<li>	People familiar with the 2019 edition of the RCU API will note
	several additional names on the byline.
	These people have taken up the challenge of learning the various
	Linux-kernel RCU implementations, each investing significant
	time over a period of years.
	Not that Paul intends to go anywhere anytime soon, his father
	having taken early retirement only recently, but Mother Nature
	might force him into her retirement program at any time.
	Paul therefore asks you to please take them seriously,
	because if something happens to him, they are your Linux-kernel
	RCU maintainers.
</li><li>	It is wise to CC <tt>rcu@vger.kernel.org</tt>
	on any RCU-related email that might otherwise have been sent
	privately to Paul, who is likely to become less aggressive about
	checking email during off-hours.
</li><li>	In a not-unrelated change, the source of truth
	for RCU commits has moved from Paul's venerable
	<a href="https://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu.git">-rcu tree</a>
	to a
	<a href="https://git.kernel.org/pub/scm/linux/kernel/git/rcu/linux.git">shared RCU tree</a>.
	His -rcu tree will continue to exist, but will be one of several
	feeding into the shared RCU tree and elsewhere.
</li></ol>

<h3><a name="Summary of RCU API Changes"></a>
Summary of RCU API changes</h3>

<p>These sections summarize some of the most visible changes to RCU:

<ol>
<li>	<a href="#Lazy RCU Asynchronous Grace Periods">
	Lazy RCU grace asynchronous periods</a>
</li><li>	<a href="#Reworking of kfree_rcu()">
	Reworking of <tt>kfree_rcu()</tt></a>
</li><li>	<a href="#Polled RCU Grace Periods">
	Polled RCU grace periods</a>
</li><li>	<a href="#Tasks Rude RCU and Tasks Trace RCU">
	Tasks Rude RCU and Tasks Trace RCU</a>
</li><li>	<a href="#New SRCU Read-Side Critical Sections">
	New SRCU read-side critical sections</a>
</li><li>	<a href="#Read-Side Guards">
	Read-side guards</a>
</li><li>	<a href="#RCU Callback Dynamic (De-)Offloading">
	RCU callback dynamic (de-)offloading</a>
</li><li>	<a href="#Miscellaneous">
	Miscellaneous</a>
</li></ol>

<h4><a name="Lazy RCU Asynchronous Grace Periods"></a>
Lazy RCU grace asynchronous periods</h4>

<p>The addition of lazy RCU grace periods was prompted by energy-efficiency
concerns on battery-powered platforms, most notably Android and ChromeOS.
This laziness affects callbacks queued via <tt>call_rcu()</tt>, and
is not to be confused with the lazy processing of RCU callbacks from
<tt>kfree_rcu()</tt>, which is covered in the next section.

<p>New-age lazy <tt>call_rcu()</tt> grace periods are controlled
at build time by a new <tt>CONFIG_RCU_LAZY</tt> kernel configuration
option and, in kernels built with <tt>CONFIG_RCU_LAZY=y</tt>, at run
time by a new <tt>rcutree.enable_rcu_lazy</tt> kernel boot parameter.
There is an additional <tt>CONFIG_RCU_LAZY_DEFAULT_OFF</tt> configuration
option that changes the default setting of this new kernel boot parameter
to be disabled.  In other words, when the kernel boot parameter is not
specified, kernels built with <tt>CONFIG_RCU_LAZY=y</tt> but without
specifying <tt>CONFIG_RCU_LAZY_DEFAULT_OFF</tt> (or, equivalently,
built with <tt>CONFIG_RCU_LAZY_DEFAULT_OFF=n</tt>) will have lazy
<tt>call_rcu()</tt> callbacks, while kernels built with
<tt>CONFIG_RCU_LAZY=y</tt> and
<tt>CONFIG_RCU_LAZY_DEFAULT_OFF=y</tt> will have non-lazy (hurried)
<tt>call_rcu()</tt> callbacks.
<p>
The kernel boot parameter overrides the configuration options, so that (for
example) booting a <tt>CONFIG_RCU_LAZY=y</tt> kernel with
<tt>rcutree.enable_rcu_lazy=1</tt> 
will result in lazy <tt>call_rcu()</tt> callbacks.
Lazy callbacks will wait up to ten seconds before starting a grace
period, thus greatly reducing the number of grace periods (and their
associated wakeups of idle CPUs) on an almost-idle device.

However, <tt>call_rcu()</tt> callbacks will only ever be lazy on
<tt>rcu_nocbs</tt> CPUs. The reason for this is that systems with
<tt>non-rcu_nocbs</tt> CPUs consume more energy than their
<tt>rcu_nocbs</tt> counterparts, so those concerned with battery lifetime
would be well-advised to first enable <tt>rcu_nocbs</tt>. This can be done by
building the kernel with <tt>CONFIG_RCU_NOCB_CPU=y</tt> and booting it with
<tt>rcu_nocbs=all</tt>.

<p>Testing of lazy <tt>call_rcu()</tt> grace periods showed well
in excess of 10% improvements in energy efficiency.

<p>Some uses of <tt>call_rcu()</tt> cannot tolerate laziness, for
example, when the callback function does a wakeup.
These uses should instead invoke the new <tt>call_rcu_hurry()</tt>
function.
Note that invoking <tt>call_rcu_hurry()</tt> on a given CPU
will hurry along any earlier lazy callbacks that were previously
queued using <tt>call_rcu()</tt>.

<p>When suspending or hibernating, it is also important for all callbacks
to hurry.
(But don't take our word for it, ask a user about adding a ten-second
delay to suspend!)
RCU therefore has an <tt>rcu_pm_notify()</tt> function that hurries
callbacks at the start of a suspend or hibernation operation.
This same function re-enables laziness when this operation completes.

<h4><a name="Reworking of kfree_rcu()"></a>
Reworking of <tt>kfree_rcu()</tt></h4>

<p>From the viewpoint of pre-existing, working code,
<tt>kfree_rcu()</tt> works just like it always did.
However, under the covers, performance has increased significantly
through use of pages of pointers to track the memory and through use
of <tt>kfree_bulk()</tt>.
Both of these changes greatly improve cache locality, which provides up
to a 12% increase in performance.
Performance was increased still further for battery-powered devices via
carefully developed heuristics that govern how long a partially populated
page of pointers waits before being submitted to RCU.

<p>In addition, <tt>kfree_rcu()</tt> can now handle memory
from <tt>kmem_cache_alloc()</tt>, not due to any changes
to <tt>kfree_rcu()</tt>, but rather due to changes to
<tt>kfree()</tt>.
However, please note that <tt>rcu_barrier()</tt> does not wait
for memory being freed via <tt>kfree_rcu()</tt>, so that there
currently is no way to safely invoke <tt>kmem_cache_destroy()</tt>
on module exit if that module ever used <tt>kfree_rcu()</tt> on
memory from the corresponding <tt>kmem_cache</tt> structure.
A <a href="https://docs.google.com/document/d/1v0rcZLvvjVGejT3523W0rDy_sLFu2LWc_NR3fQItZaA/edit?usp=sharing">fix</a> is in the works.
In the meantime, such modules should continue using <tt>call_rcu()</tt>
for this use case.

<p>One disadvantage of <tt>kfree_rcu()</tt> is that an
<tt>rcu_head</tt> structure must be embedded within the structure
to be freed, costing 16 additional bytes on 64-bit systems.
For example, a structure referenced by <tt>p</tt> where
the <tt>rcu_head</tt> structure is in a field named <tt>rh</tt>
can be RCU-deferred-freed using:
<p>
<pre>
    kfree_rcu(&amp;p-&gt;rh, rh)
</pre>
<p>
This issue is addressed by a new <tt>kfree_rcu_mightsleep()</tt>
function, which takes a single pointer to the beginning of the
object to be freed, as in <tt>kfree_rcu_mightsleep(p)</tt>,
thus avoiding the need for that <tt>rcu_head</tt> structure and
its 16&nbsp;bytes of memory.

<p>Of course, there is always a catch, and in this case the catch is that,
as the name suggests, <tt>kfree_rcu_mightsleep()</tt> might sleep.
In fact, if it is unable to allocate the memory needed to
track the object being deferred-freed, it will simply invoke
<tt>synchronize_rcu()</tt>, which blocks for some tens of
milliseconds, and then directly invokes <tt>kfree()</tt>.
In contrast, <tt>kfree_rcu()</tt> reacts to low memory by invoking
<tt>call_rcu()</tt>, thus giving up cache locality but not
caller-visible latency.
As always, choose wisely!

<p>There are versions of the kernel that have a single-argument
variant of <tt>kfree_rcu()</tt> instead of
<tt>kfree_rcu_mightsleep()</tt>.
This single-argument approach proved to be a serious mistake
that led to subtle bugs in which people forgot to specify
the second argument, which fails (but only sometimes) from
atomic contexts such as interrupt handlers.
Therefore, recent kernels provide only the double-argument
variant of <tt>kfree_rcu()</tt>.
Where the old single-argument version would have been used,
use <tt>kfree_rcu_mightsleep()</tt> instead.

<p>There are also <tt>kvfree_rcu()</tt> and
<tt>kvfree_rcu_mightsleep()</tt> functions
that operate on <tt>vmalloc()</tt> memory.

<p>In short, if you have RCU callbacks that do nothing
but <tt>kfree()</tt>, <tt>vfree()</tt>, or <tt>kmem_cache_free()</tt> to
immortal <tt>kmem_cache</tt> structures, these functions might save you
both CPU time and a few lines of code.  Many of these have already been
converted, courtesy of a <a
href="https://en.wikipedia.org/wiki/Coccinelle_(software)">Coccinelle</a>
script and patches from Julia Lawall, but new code is written all the time.

<h4><a name="Polled RCU Grace Periods"></a>
Polled RCU grace periods</h4>

<p>The historic RCU API does quite a bit of work for the user,
who can simply invoke <tt>synchronize_rcu()</tt> and, upon its return,
know that all pre-existing readers are done.
Or, if the <tt>synchronize_rcu()</tt> function's blocking
is problematic, pass a pointer and a callback function to
<tt>call_rcu()</tt> and upon invocation of that callback function,
know that all pre-existing readers are done.
Better yet, if that callback function is doing nothing but invoking
<tt>kfree()</tt> or <tt>kmem_cache_free()</tt>, just pass
a pointer and <tt>rcu_head</tt> offset to <tt>kfree_rcu()</tt>,
and rely on RCU to do everything, including the freeing.
Or even better still, if within a latency-tolerant, non-atomic context,
dispense with the <tt>rcu_head</tt> and pass just the pointer itself
to <tt>kfree_rcu_mightsleep()</tt>.

<p>However, as reported
<a href="https://lwn.net/Articles/974487/">here</a>,
there are situations in which these conveniences become counterproductive.
For example, in caching situations, it might be that an object
that has been queued for deferred free is once again needed.
In such cases, it might be helpful to cancel a
<tt>synchronize_rcu()</tt>, prevent a <tt>call_rcu()</tt>
callback from being invoked, or to prevent an object passed to
<tt>kfree_rcu()</tt> or <tt>kfree_rcu_mightsleep()</tt> from
being freed.
For another example, in situations that invoke many instances of
<tt>call_rcu()</tt> in short time periods, RCU's choice of
software-interrupt context for callback invocation might be suboptimal.
Unfortunately, providing for these situations would further complicate
the RCU API and implementation, and would also result in performance
degradation.

<div class="tlr">
<b>Quick Quiz 1</b>:
Why would <tt>call_rcu()</tt> be helpful in scheduling polling
for a particular grace period?
<p>
<details><summary><b>(Click for answer)</b></summary>
Because the callback passed to <tt>call_rcu()</tt> tends to be
invoked shortly after a grace period has completed, which is an
excellent time to do polling for the end of a grace period.
Alternatively, if the RCU-callback softirq context is inconvenient, you
can instead use <tt>queue_rcu_work()</tt> to schedule a workqueue
handler to execute shortly after a grace period completes.
</details>

</div>

<p>In recent Linux kernels, RCU instead provides a complete polling API
for managing RCU grace periods.
This permits RCU users to take full control over all aspects of
waiting for grace periods and responding to the end of a particular
grace period.
For example, the <tt>get_state_synchronize_rcu()</tt> function
returns a cookie that can be passed to
<tt>poll_state_synchronize_rcu()</tt>.
This latter function will return <tt>true</tt> if a grace period
has elapsed in the meantime.
The user may choose any convenient method to schedule the polling for
the end of the grace period, from <tt>mod_timer()</tt> up to and
including use of <tt>call_rcu()</tt> itself.
Once the grace period in question has elapsed, the user may choose any
convenient context from which to free memory, or to undertake whatever
other processing is required.


<p>For a bit more background on RCU's polled grace-period API, please see
<a href="https://paulmck.livejournal.com/65800.html">Stupid RCU Tricks: Waiting for Grace Periods From NMI Handlers</a>
or slides&nbsp;34-on in the
<a href="https://drive.google.com/file/d/1piN3sUrYJd9CwkY9jXGxPTvbGXwT-qvG/view?usp=sharing">Reclamation Interactions with RCU</a>
LSFMM+BPF 2024 presentation.

<h4><a name="Tasks Rude RCU and Tasks Trace RCU"></a>
Tasks Rude RCU and Tasks Trace RCU</h4>

<p>The 2019 edition of the RCU API described the addition of Tasks RCU
for use by ftrace and kprobes.
These facilities use trampolines containing tracepoint
code, and Tasks RCU is used to synchronize removal of a trampoline
with tasks that might still be executing within it.
Because ftrace and kprobes trampolines never do context
switches, nor do they invoke functions that do context switches,
a voluntary context switch suffices as a Tasks RCU quiescent
state.
This edition describes Tasks Rude RCU, which was consolidated from an
open-coded implementation provided by Steve Rostedt, and also Tasks Trace
RCU, which was added for use by
<a href="https://lwn.net/Articles/825415/">sleepable BPF programs</a>
that might block.

<p>Tasks Rude RCU augments Tasks RCU by handling the idle tasks that
Tasks RCU ignores, thus permitting trampolines to be installed in
the idle loop.
One could instead prohibit tracepoints and kprobes in the idle loop,
but the increasing quantity of power-management code living there
makes such a prohibition unpalatable.

<p>Therefore, true to its name, Tasks Rude RCU uses the
<tt>schedule_on_each_cpu()</tt> function to force a
context switch on each CPU, and thus force each CPU out of
the idle loop.
Those using battery-powered systems might well consider the
resulting wakeups of deep-idle-state CPUs to be quite rude,
hence the name.

<p>Like Tasks RCU, Tasks Rude RCU has no read-side markers.
It has <tt>synchronize_rcu_tasks_rude()</tt> and
<tt>call_rcu_tasks_rude()</tt> functions
to wait for a grace period, synchronously and asynchronously,
respectively.
The <tt>rcu_barrier_tasks_rude()</tt> function waits for
the invocation of all callbacks queued by previous invocations
of <tt>call_rcu_tasks_rude()</tt>, which is needed when
unloading modules such as <tt>rcutorture</tt> that
invoke <tt>call_rcu_tasks_rude()</tt>.
However, neither <tt>call_rcu_tasks_rude()</tt> nor
<tt>rcu_barrier_tasks_rude()</tt> is used in current mainline,
which is likely to lead to their removal sooner rather than later.

<p>Peter Zijlstra and Thomas Gleixner reworked the x86 entry/exit code,
using <a
href="https://docs.kernel.org/core-api/entry.html#non-instrumentable-code-noinstr"><tt>noinstr</tt></a>
and inlining, so that any function that RCU is not watching cannot be
traced.  On any architecture where this is completed, where the CPU-hotplug
architecture-specific offline code path has been addressed, and where the
maintainers feel confident that it will stay completed,
<tt>synchronize_rcu_tasks_rude()</tt> can become a no-op and
<tt>call_rcu_tasks_rude()</tt> can invoke its callback immediately from a
clean context.

<p>BPF programs use a combination of RCU, Tasks RCU, and
Tasks Rude RCU for various purposes, including synchronizing
uses of and removals of trampolines.
RCU is used to protect entire BPF programs, which works well,
but which prohibits BPF programs from blocking.
This prohibition in turn prevents BPF programs from unconditionally
loading the contents of user-space memory because of the fact that
user-space accesses might result in page faults.  That, in turn, might
result in blocking, waiting for that user-space data to be paged back in.
Therefore, BPF programs have been given conditional access to user-space
memory, which either completes the access or indicates failure.

<p>These failure indications can be quite inconvenient, so a new
special-purpose Tasks Trace RCU flavor has been created that allows
limited blocking within its read-side critical sections.
As such, Tasks Trace RCU can be thought of as variant of sleepable RCU (SRCU)
with low-overhead read-side markers.
Although, like SRCU, the implementation can tolerate arbitrary blocking,
by convention Tasks Trace RCU readers are only permitted to block for
long enough to handle a major page fault.

<p>The Tasks Trace RCU read-side markers are
<tt>rcu_read_lock_trace()</tt> and
<tt>rcu_read_unlock_trace()</tt>, and there is a lockdep-enabled
<tt>rcu_read_lock_trace_held()</tt> function that indicates
whether or not it is within an Tasks Trace RCU read-side critical section.
(When lockdep is disabled, this function always returns the value one.)
Synchronous grace-period waits are provided by
<tt>synchronize_rcu_tasks_trace()</tt> and
asynchronous grace-period waits by <tt>call_rcu_tasks_trace()</tt>.
The <tt>rcu_barrier_tasks_trace()</tt> function waits for
the invocation of all callbacks queued by previous invocations
of <tt>call_rcu_tasks_trace()</tt>.
It is not unusual for BPF code to need to wait for both an
RCU and an Tasks Trace RCU grace period, and a current accident of
implementation means that any Tasks Trace RCU grace period is also
a plain RCU grace period.
This could of course change at any time, so there is a
<tt>rcu_trace_implies_rcu_gp()</tt> function (which
currently unconditionally returns <tt>true</tt>) that
specifies whether or not this happy accident is still in effect.

<p>Again, Tasks Trace RCU is quite specialized, so those wishing to
use it should consult not only with its maintainers, but also with its
current users.

<h4><a name="New SRCU Read-Side Critical Sections"></a>
New SRCU read-side critical sections</h4>

<p>SRCU read-side critical sections use <tt>this_cpu_inc()</tt>,
which excludes interrupt and software-interrupt handlers, but is not guaranteed
to exclude non-maskable interrupt (NMI) handlers.
Therefore, SRCU read-side critical sections may not be used in
NMI handlers, at least not in portable code.
This restriction became problematic for <tt>printk()</tt>, which is
frequently called upon to do stack backtraces from NMI handlers.
This situation motivated adding <tt>srcu_read_lock_nmisafe()</tt> and
<tt>srcu_read_unlock_nmisafe()</tt>.
These new API members instead use <tt>atomic_long_inc()</tt>,
which can be more expensive than <tt>this_cpu_inc()</tt>, but
which does exclude NMI handlers.

<p>However, SRCU will complain if you use both the traditional
and the NMI-safe API members on the same <tt>srcu_struct</tt> structure.
In theory, it is possible to mix and match but, in practice, the rules for
safely doing so are not consistent with good software-engineering practice.
So if you need any of a given <tt>srcu_struct</tt> structure's read-side
critical sections to appear in an NMI handler, use
<tt>srcu_read_lock_nmisafe()</tt> and <tt>srcu_read_unlock_nmisafe()</tt>
to mark all of that <tt>srcu_struct</tt> structure's read-side critical
sections.  When lockdep is enabled, the kernel will complain bitterly if you
attempt to mix and match NMI-safe and non-NMI-safe SRCU readers on the same
<tt>srcu_struct</tt> structure.

<p>An SRCU read-side critical section must be wholly contained within
a given task.
Discussions led to the belief that this restriction was too
severe, resulting in the new <tt>srcu_down_read()</tt>
and <tt>srcu_up_read()</tt> API members, by analogy to
<tt>down_read()</tt> and <tt>up_read()</tt>.
However, these APIs have not yet seen any use.
If they continue to be unused, they will be removed.

<p>In addition, the <tt>list_for_each_entry_srcu()</tt> and
<tt>hlist_for_each_entry_srcu()</tt> iterators were added, and are
actually in use.

<p>Finally, the <tt>cleanup_srcu_struct_quiesced()</tt> was removed
because the deadlock issue that led to its creation was resolved by
adding <tt>WQ_MEM_RECLAIM</tt> workqueues.
Therefore, any code that would previously have used
<tt>cleanup_srcu_struct_quiesced()</tt> can now use
<tt>cleanup_srcu_struct()</tt> instead.

<h4><a name="Read-Side Guards"></a>
Read-side guards</h4>

<p>Zijlstra introduced read-side guards for RCU and SRCU and
Johannes Berg made the RCU read-side guards safe for the <tt>sparse</tt>
static-analysis tool.
These guards use the <tt>__cleanup__</tt> attribute to cause a
read-side critical section to be exited as soon as the scope ends.
This enables <a
href="https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization">the
RAII (resource allocation is initialization) pattern</a> 
for RCU and SRCU, for example.  From <tt>fs/libfs.c</tt>:

<blockquote><pre>
 1 static inline struct dentry *get_stashed_dentry(struct dentry *stashed)
 2 {
 3   struct dentry *dentry;
 4
 5   guard(rcu)();
 6   dentry = READ_ONCE(stashed);
 7   if (!dentry)
 8     return NULL;
 9   if (!lockref_get_not_dead(&amp;dentry-&gt;d_lockref))
10     return NULL;
11   return dentry;
12 }
</pre></blockquote>

<div class="tlr">
<b>Quick Quiz 2</b>:
What is with the extra pair of parentheses on line&nbsp;5?
<p>
<details><summary>Click for answer</summary>
For lock-based guards, these would specify which lock to acquire.
But RCU is global in nature, so does not need anything between
the second pair of parentheses.

<p>Those willing to look more deeply under the covers will see that
the <tt>(rcu)</tt> is the argument to the <tt>guard()</tt>
macro and the <tt>()</tt> is an argument to the constructor
function that enters the RCU read-side critical section.
</details>
</div>
<p>Line&nbsp;5 creates an RCU read-side critical section that
extends to the end of the enclosing scope, that is, to the
end of the function body.

<p>An SRCU read-side guard must specify which <tt>srcu_struct</tt>
to use, for example, as follows:
<br clear="all">

<blockquote><pre>
 1 static void gpiochip_setup_devs(void)
 2 {
 3   struct gpio_device *gdev;
 4   int ret;
 5
 6   guard(srcu)(&amp;gpio_devices_srcu);
 7
 8   list_for_each_entry_srcu(gdev, &amp;gpio_devices, list,
 9          srcu_read_lock_held(&amp;gpio_devices_srcu)) {
10     ret = gpiochip_setup_dev(gdev);
11     if (ret)
12       dev_err(&amp;gdev-&gt;dev,
13         "Failed to initialize gpio device (%d)\n", ret);
14   }
15 }
</pre></blockquote>

<p>Here, line&nbsp;6 enters an SRCU read-side critical section using
the <tt>srcu_struct</tt> structure named <tt>gpio_devices_srcu</tt>,
and this critical section extends to the end of the enclosing scope.

<p>But sometimes it is necessary to exit the critical section prior
to the end of the enclosing scope.
For this purpose, <tt>scoped_guard()</tt> creates an SRCU
read-side critical section that covers only the following statement,
which will often be a compound statement, for example, as follows:

<blockquote><pre>
 1 struct gpio_desc *gpio_to_desc(unsigned gpio)
 2 {
 3   struct gpio_device *gdev;
 4
 5   scoped_guard(srcu, &amp;gpio_devices_srcu) {
 6     list_for_each_entry_srcu(gdev, &amp;gpio_devices, list,
 7         srcu_read_lock_held(&amp;gpio_devices_srcu)) {
 8       if (gdev-&gt;base &lt;= gpio &amp;&amp;
 9           gdev-&gt;base + gdev-&gt;ngpio &gt; gpio)
10         return &amp;gdev-&gt;descs[gpio - gdev-&gt;base];
11     }
12   }
13
14   if (!gpio_is_valid(gpio))
15     pr_warn("invalid GPIO %d\n", gpio);
16
17   return NULL;
18 }
</pre></blockquote>

<div class="tlr">
<b>Quick Quiz 3</b>:
So why aren't there read-side guards for the three Tasks RCU variants?
<p>
<details><summary>Click for answer</summary>
For Tasks RCU and Tasks Rude RCU, the readers are implicit, which means
that there is no useful way to create a guard.
For Tasks Trace RCU, read-side guards are easy to implement, and will
be implemented should a use case arise.
</details>
</div>


<p>Here, line&nbsp;5 enters an SRCU read-side critical section that
extends from line&nbsp;6 through line&nbsp;12.

<p>In all cases, use of read-side guards avoids bugs in which
code enters a critical section but then fails to exit it.


<h4><a name="RCU Callback Dynamic (De-)Offloading"></a>
RCU callback dynamic (de-)offloading</h4>

<p>RCU callbacks may be offloaded, which means that, instead of being
invoked in software-interrupt context (usually on the CPU that queued
them), they are instead processed (shepherded through the end of the grace
period) in the context of <tt>rcuog</tt> kthreads, then invoked in the
context of per-CPU <tt>rcuoc</tt> kthreads.  Callback offloading can
provide substantial improvements for both HPC and real-time workloads by
removing the "noise" of callback invocation from the CPUs doing
time-critical work, or, failing that, by letting the scheduler decide when
and where the RCU callbacks should be invoked.  These kthreads may be
controlled by the system administrator using the usual set of scheduling
facilities, ranging from <tt>taskset</tt> to control groups.

<div class="tlr">
<b>Quick Quiz 4</b>:
Why not always offload RCU callback invocation?
<p>
<details><summary>Click for answer</summary>
	Because offloaded callbacks require that <tt>call_rcu()</tt>
	explicitly synchronize with whatever CPU the corresponding
	<tt>rcuog</tt> kthread might be running on.
	This synchronization is not free, and poses the usual choice between high
	throughput (callbacks not offloaded) and real-time response (callbacks
	offloaded).
	And because RCU has no way of knowing which approach is best for a given
	workload, it must therefore defer to the better judgment of the system
	administrator.
</details>
</div>

<p>By default, CPUs are never offloaded.
Setting the <tt>RCU_NOCB_CPU_DEFAULT_ALL</tt> kernel configuration option causes
all CPUs to be offloaded.
Either build-time option may be overridden by the <tt>nohz_full</tt>
or the <tt>rcu_nocbs</tt> kernel boot parameter, or both, if you
feel that your future self will need the additional confusion.

<p>As the number of CPUs on the typical computer system has increased,
so has the number of workloads running on such a system, and in turn,
the need to dynamically adjust those workloads.
There is thus now an in-kernel facility to offload and de-offload
RCU callbacks on specific CPUs.
However, this facility has not yet been made available to user space
due to persistent issues, including race conditions, deadlocks, and
other hangs.

<p>The current direction is to allow run-time offloading and de-offloading,
but only for offline CPUs.
This is expected to significantly simplify the code while supporting
all known use cases.
This facility will likely be made available to user space with the keenly
anticipated run-time adjustment of the <tt>nohz_full</tt> kernel
boot parameter.

<h4><a name="Miscellaneous"></a>
Miscellaneous</h4>

<p>The <tt>CONFIG_RCU_FAST_NO_HZ</tt> kernel configuration option was intended
to improve energy efficiency, but a survey in 2021 showed that the only
users of this option also offloaded RCU callbacks.  In that
case, all <tt>CONFIG_RCU_FAST_NO_HZ</tt> does is to provide a slight
slowdown for transitions to and from idle.  This configuration option was
therefore removed.

<p>The data-access APIs added <tt>rcu_dereference_raw_check()</tt>,
<tt>rcu_replace_pointer()</tt>, and <tt>unrcu_pointer()</tt>,
but also removed <tt>rcu_dereference_raw_notrace()</tt>
and <tt>rcu_swap_protected()</tt>.

<p>The updater-validation APIs removed
<tt>RCU_NONIDLE()</tt> because Zijlstra's and Gleixner's
idle-loop rework removed the need for it.

<p>The RCU list APIs added
<tt>list_tail_rcu()</tt>,
<tt>hlists_swap_heads_rcu()</tt>,
<tt>hlist_nulls_add_tail_rcu()</tt>, and
<tt>hlist_nulls_add_fake()</tt>, but removed
<tt>hlist_bl_del_init_rcu()</tt>.


<h3><a name="How Did Those 2019 Predictions Turn Out?"></a>
How did those 2019 predictions turn out?</h3>
<p>
The 2019 article included <a
href="https://lwn.net/Articles/777036/#What%20Next%20for%20the%20RCU%20API?">a
set of predictions</a> about the future of the RCU API.  Five years later,
a look at how they turned out seems warranted.

<ol class="spacylist">
<li>	A <tt>kmem_struct</tt> counterpart to <tt>kfree_rcu()</tt>
	will likely be required.

	<p>This had been predicted in 2014 as well but, yet again, this did
	not happen. 
	But something even better happened, namely that <tt>kfree()</tt>
	now handles memory obtained from <tt>kmem_struct_alloc()</tt>.
	This means that there is no longer a need for something like a
	<tt>kmem_struct_free_rcu()</tt> because <tt>kfree_rcu()</tt>
	now just handles this case.

	<p>Almost.
	But that is the subject of a new prediction.

<li>
<div class="tlr">
<b>Quick Quiz 32</b>:
Inlining <tt>rcu_read_lock()</tt> sounds quite valuable.
So why aren't Jiangshan's patches upstream?
<p>
<details><summary>Click for answer</summary>
They do remove function-call and task-structure-access overhead from
<tt>rcu_read_lock()</tt>, but at the cost of additional code on the
context-switch fast path.
This might well be a good tradeoff, but actual performance results
are required.
Please feel free to give
<a href="https://lore.kernel.org/all/20191102124559.1135-1-laijs@linux.alibaba.com/">this patch series</a>
a spin on your favorite workload!
(And of course to post the resulting performance results.)
</details>
</div>

Inlining of <tt>TREE_PREEMPT_RCU</tt>'s
	<tt>rcu_read_lock()</tt> primitive.
	
	<p>And yet again, this did not happen.

	<p>But not for lack of effort on the part of Lai Jiangshan,
	who provided not one but two patch series along these lines
	(<a href="https://lore.kernel.org/all/20240328075318.83039-1-jiangshanlai@gmail.com/">2024</a>,
	<a href="https://lore.kernel.org/all/20191102124559.1135-1-laijs@linux.alibaba.com/">2019</a>).


<li>	Additional forward-progress work, both in rcutorture and in
	RCU proper.

	<p>There was significant work in this area, along with upgrades to
	the callback-flooding testing in rcutorture to help ensure that the
	improvements stay improved.

<li>	Better handling of
	<a href="https://www.usenix.org/conference/atc17/technical-sessions/presentation/prasad">vCPU preemption within RCU readers</a>.

	<p>There have been some interesting
	<a href="https://www.youtube.com/watch?v=47YzSIWUXeI">experiments</a>
	in this area, but no commits have yet made it to mainline.

<li>	Adding rcutorture to kselftests, that is, adding a
	<tt>Makefile</tt> to
	<tt>tools/testing/selftests/rcutorture</tt>
	that carries out a quick rcutorture-based
	smoke test of RCU.

	<p>This has been done.
	Even better, there are now more people who make frequent
	use of rcutorture.

<li>	Disentangling <tt>rcu_barrier()</tt> from CPU hotplug operations,
	which could permit this function to be invoked from CPU-hotplug
	notifiers.

	<p>This has been completed.
</ol>

<div class="tlr">
<b>Quick Quiz 33</b>: what happened to quick quizzes 5-31?
<p>
<details><summary>Click for answer</summary>
They can be found in <a href="/Articles/988641/">the
background-material</a> supplemental article, for everybody who wants more
RCU.
</details>
</div>


<p>Three and two halves out of six, which is not as good as one might
hope, but better than they usually are.

<p>It is also illuminating to list the unexpected changes.
Some of these are hinted at above, but bear repeating:

<ol class="spacylist">
<li>	There is now a Tasks Rude RCU and Tasks Trace RCU.
<li>	There are a number of energy-efficiency improvements,
	including lazy RCU callbacks and lazy <tt>kfree_rcu()</tt>
	processing.
<li>	There is now a (more) complete set of polled RCU grace-period APIs.
<li>	SRCU read-side critical sections may now be in NMI handlers
	(using the new <tt>srcu_read_lock_nmisafe()</tt> and
	<tt>srcu_read_unlock_nmisafe()</tt> functions)
	and may also span tasks (using the new <tt>srcu_down_read()</tt>
	and <tt>srcu_up_read()</tt> functions).
<li> RAII guards are now available for RCU and SRCU, courtesy of Zijlstra
	and Berg.
<li>	There is now a kernel configuration option to reduce
	<tt>synchronize_rcu()</tt> latency during heavy RCU-callback
	loads.
<li>	Expedited RCU uses kthread workers instead of workqueues, which
	enables priority boosting to also boost expedited grace-period
	processing.
<li>	The expedited RCU CPU stall-warning timeout can now be set in
	milliseconds, and some users set it to 20&nbsp;milliseconds.
	And so it is that Linux's stall warning timeout finally beats
	the 1990s DYNIX/ptx timeout of 1.5&nbsp;seconds.
	For expedited grace periods, anyway.
<li>	It is now OK to disable interrupts across
	<tt>rcu_read_unlock()</tt> even if the corresponding RCU
	read-side critical section might have been preempted.
	This is a welcome side-effect of Jiangshan's first attempt
	to inline <tt>rcu_read_lock()</tt> and
	<tt>rcu_read_unlock()</tt>.
<li>	RCU code now takes greater advantage of the Kernel
	Concurrency Sanitizer (KCSAN).
<li>	CPUs may have their callbacks offloaded and de-offloaded at
	runtime, though this capability has not yet been made available
	to user space.
<li>	Debug-objects testing for double <tt>call_rcu()</tt> bugs
	can now print out more information on the memory passed in.
<li>	The rcutorture tests of RCU priority boosting are now much
	more stringent and resistant to false positives.
<li> There is now a <tt>kvm-remote.sh</tt> rcutorture test facility that
	spreads rcutorture tests over many remote systems.
<li>	There is now a <tt>torture.sh</tt> test facility that does an overnight
	test of various torture tests.
<li>	There is now a trivial textbook RCU implementation in
	rcutorture, just to keep the slides and textbooks honest.
<li>	The lockdep facility now checks for SRCU-based deadlocks.
<li>	The <a href="/Articles/718628/">Linux-kernel memory model's
	(LKMM's)</a> model of SRCU is now much more realistic.
<li>	A great many much-appreciated features and fixes from more than
	100 Linux-kernel developers.
</ol>

<h3><a name="What Next for the RCU API?"></a>
What next for the RCU API?</h3>

<p>As always, the most honest answer is that I do not know.
That said, here are a few things that might happen:

<ol class="spacylist">
<li>	The slab allocator will defer acting on
	<tt>kmem_struct_destroy()</tt> until after memory sent
	to <tt>kfree_rcu()</tt> has been freed.
	This would permit a module to use <tt>kfree_rcu()</tt>
	on memory obtained from <tt>kmem_struct</tt> allocators
	that this module passes to <tt>kmem_struct_destroy()</tt>
	at module-unload time.

<li>	<tt>TREE_PREEMPT_RCU</tt>'s
	<tt>rcu_read_lock()</tt> primitive will be inlined.
	After all, why not triple down?

<li>	Hazard pointers will be added to the Linux kernel's
	deferred-free toolbox.

<li>	RCU callbacks will benefit from concurrent expedited
	grace periods.

<li>	Although RCU is now capable of changing the callback-offloaded
	status of a given CPU at runtime, this has not been made available
	to user space.
	It seems likely that user space will gain this capability sooner
	rather than later, albeit with some restrictions.

<li>	Further upgrades to RCU's energy-efficiency, latency, and
	simplicity.

<li>	Someone will notice that <tt>rcu_barrier()</tt> is no
	longer guaranteed to wait for the grace periods corresponding
	to prior calls to <tt>synchronize_rcu()</tt>.
	(Late-breaking news: Vlastimil Babka has already noticed.)
</ol>

<p>But now as always, new use cases and workloads will place unanticipated
demands on RCU.

<h4>Acknowledgments</h4>

<p>We are all indebted to a huge number of people who have used,
abused, poked at, and otherwise helped to improve the RCU API.
Paul is grateful to Dan Kelley for his support of this effort.

<p>This work represents the view of the authors and does not necessarily
represent the view of the authors' respective employers.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Read-copy-update">Read-copy-update</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#E._McKenney_Paul">E. McKenney, Paul</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/988638/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor990278"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 13, 2024 19:59 UTC (Fri)
                               by <b>pbonzini</b> (subscriber, #60935)
                              [<a href="/Articles/990278/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
What is the intended use case for hazard pointers? Do you envision them used by "normal" code just like RCU, or only under special circumstances?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/990278/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor990331"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Sep 14, 2024 13:28 UTC (Sat)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/990331/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The thing that made it clear that hazard pointers are needed was the recent reworking of reference-count acquisition to better handle high contention for a case in which a reference needed to be acquired within an RCU read-side critical section.<br>
<p>
Of course, we have other ways of dealing with this, for example per-CPU refcounts.  Which work very well and thus see a lot of use.  But in this case, their per-object memory overhead was too much for traffic to bear.  A similar effect may often be achieved using SRCU, but only when it is OK for long readers to block updates.  All in all, we have taken RCU quite a bit farther than I would have guessed possible 25 years ago, but we are starting to find areas where more is needed.<br>
<p>
Hazard pointers is a good match for this situation.  A reference may be acquired without contention, and no per-object storage is required.<br>
<p>
To be sure, hazard pointers is limited in its own way: (1) Heavy barriers are needed for acquiring a reference, or, alternatively, IPIs or RCU readers.  (2) Unlike RCU, hazard pointers can say "no" to a reference acquisition.  But that was already the case in the aforementioned situation.  (3) Like RCU, hazard pointers involves deferred reclamation, but is often more able to do emergency reclamations.  (4) Hazard pointers is new code and will require some work to stabilize, just like RCU did, and sometimes still does.<br>
<p>
So, I expect hazard pointers to initially be used only under special circumstances.  Longer term, who knows?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/990331/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor992529"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 1, 2024 19:48 UTC (Tue)
                               by <b>jseigh</b> (guest, #173778)
                              [<a href="/Articles/992529/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The implementation of an asymmetric memory barrier for hazard pointers, as well as other things, is going to have a vcpu preemption problem also.<br>
<p>
Asymmetric memory barriers, that are used to eliminate the need for the expensive memory barrier in a hazard pointer read, look for events on the other processors which would constitute a memory barrier on the other processors.  Examples would be context switches, cpu in a wait state, and ipi interrupt handling.  The former 2 being used as quiescent states by RCU (if I'm guessing correctly), hence the use of RCU in an asymmetric memory barrier implementation.  A vcpu wait state would also constitute a memory barrier, so if they were observable, they to could be used to avoid vcpu preemption issues in a asymmetric memory barrier implementation.<br>
<p>
vcpu wait state detection should also be used for IPI implementations since the IPI's are also affected by vcpu preemption.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992529/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor992661"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers continued and vcpu preemption</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 2, 2024 14:27 UTC (Wed)
                               by <b>jseigh</b> (guest, #173778)
                              [<a href="/Articles/992661/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
A few additional comments.<br>
<p>
The hazard pointer implementation in the kernel.  Is there an API for that?  Also if it's new are they looking for rust or c?<br>
<p>
For the vm ballooning problem,  some vm's are using pause loop detection for spin locks (although they apparently are not that good at figuring out the right target vcpu to switch to.   If it was a global spin lock eventually most of the CPUs would trigger it so the vm would find the target by process of elimination.  For RCU if a processor looked unresponsive you could do a spin loop with pauses on the vcpu's quiescent state until it changed or just long enough to trigger the PLE event.   You might have to do this for all vcpus via IPI to get the vm to find the right target vcpu.<br>
<p>
 <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992661/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor992700"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 2, 2024 19:43 UTC (Wed)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/992700/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Here is the most recent patch series: <a href="https://lore.kernel.org/all/20241002010205.1341915-1-mathieu.desnoyers@efficios.com/">https://lore.kernel.org/all/20241002010205.1341915-1-math...</a><br>
<p>
There are earlier ones from Boqun Feng and Neeraj Upadhyay.<br>
<p>
Mathieu is also working on a userspace hazard-pointer implementation, which I would expect to use sys_membarrier().<br>
<p>
If I understand correctly, for hazard pointers and vCPU preemption, this is a performance problem rather than a correctness problem.  If a vCPU is preempted, this will delay the RCU grace period, the IPI response, or the sys_membarrier() response, as the case may be.  There have been a number of prototyped solutions to vCPU preemption, for example, based on priority boosting.  So, should vCPU preemption become a problem in practice, there are a number of starting points for a solution.<br>
<p>
But what is your preferred starting point?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992700/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor992704"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 2, 2024 22:39 UTC (Wed)
                               by <b>jseigh</b> (guest, #173778)
                              [<a href="/Articles/992704/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Re hazard pointer API.  I was mainly curious since the c++26 hazard pointer API seems to be more complex than I would expect.  If I did anything it would be in user space although I have a  hazard pointer based proxy collector implementation that suites most of my needs.  I don't use deferred reclamation with data structures requiring high rates of modifications,  e.g. a linked list lock-free queue because<br>
1) deferred reclamation would kill throughput<br>
2) I already have a lock-free queue which is ABA proof, i.e. doesn't require deferred reclamation to work correctly.<br>
<p>
Re vcpu preemption.  I don't really have any preferred starting point.  Just curious about that also.  I did propose a new machine instruction for restartable sequences about 20 years ago.   If we had something like that any deferred reclamation schemes based on that, it would have automatically worked in vm's much the same way LL/SC works in vm's without any special handling.<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992704/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor992766"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 3, 2024 12:01 UTC (Thu)
                               by <b>PaulMcKenney</b> (<b>&#x272D; supporter &#x272D;</b>, #9624)
                              [<a href="/Articles/992766/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The C++ Hazard Pointers API was informed by some years of production experience with the implementation in the Folly library.  As you might guess, Maged Michael was the force behind this implementation, the experience with it, and of course with the adjustments undertaken in response to that experience.  On the other hand, and to your point, it is not unusual for such experience to result in increased complexity.  Linux-kernel RCU is most certainly another example of this tendency, after all.  ;-)<br>
<p>
That said, I completely agree that if you can achieve your goals simply, then embrace simplicity.  In particular, if a simple lock-free queue does the job for you, then by all means use that queue!<br>
<p>
At one point, I had hoped that the various hardware transactional memory mechanisms would be useful for many things, including vCPU preemption.  And perhaps someday they will.  But at present, they seem to have fallen victim to the same "complexity surprise" called out above.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/992766/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor994564"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Hazard pointers</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 17, 2024 12:32 UTC (Thu)
                               by <b>jseigh</b> (guest, #173778)
                              [<a href="/Articles/994564/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I gave up on the idea of a hazard pointer implementation as soon as I realized it would involve writing a tracing garbage collector of sorts as part of that.<br>
<p>
I did remove the hazard pointer logic from smrproxy.  Absent the load of the address of the reader lock object, a lock action is 2 machine instructions, a load register followed by a store register.  No loop so it is formally wait-free.  Unlock is same as before, a store of 0 to memory.   I'm starting on porting it to c++ which also involves figuring out what a c++ deferred reclamation API looks like.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/994564/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2024, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
