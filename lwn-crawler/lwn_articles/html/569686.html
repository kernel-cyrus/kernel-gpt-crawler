        <!DOCTYPE html>
        <html lang="en">
        <head><title>Optimizing CPU hotplug locking [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/569686/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/569267/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/569686/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Optimizing CPU hotplug locking</h1>
</div>
<div class="ArticleText">
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>October 9, 2013</br>
           </div>
The 3.12 development cycle has seen an increased level of activity around
scalability and, in particular, the reduction of locking overhead.  Traffic
on the linux-kernel mailing list suggests that this work will extend into
3.13, if not beyond.  One of several patch sets currently under development
relates to CPU hotplugging — the process of adding CPUs to (or
removing them from) a running system.  
<p>
CPU hotplugging adds complications to a number of kernel subsystems; the
fact that processors can come and go at arbitrary times must always be
taken into account.  Needless to say, hotplug operations must be restricted
to times when the kernel is prepared for them; to that end, the kernel
provides a reference count mechanism to allow any thread to block CPU
hotplugging.  The reference count is raised with <tt>get_online_cpus()</tt>
to indicate that the set of online CPUs should not be changed; the
reference count is decremented with <tt>put_online_cpus()</tt>.
<p>
The implementation of <tt>get_online_cpus()</tt> in current kernels is
relatively straightforward:
<p>
<pre>
    mutex_lock(&amp;cpu_hotplug.lock);
    cpu_hotplug.refcount++;
    mutex_unlock(&amp;cpu_hotplug.lock);
</pre>
<p><blockquote class="ad">
<b><tt>$ sudo subscribe today</tt></b>
<p>
Subscribe today and elevate your LWN privileges. You’ll have
access to all of LWN’s high-quality articles as soon as they’re
published, and help support LWN in the process.  <a href="https://lwn.net/Promo/nst-sudo/claim">Act now</a> and you can start with a free trial subscription.
</blockquote>
<p>
Code that is managing an actual hotplug operation will acquire
<tt>cpu_hotplug.lock</tt> (after waiting for the reference count to drop to
zero if need be) and hold it for the duration of the operation.  This
mechanism ensures that no thread will see a change in the set of active
CPUs while it holds a reference, but there is a bit of a problem: each
reference-count change causes the cache line containing the lock and the
count to bounce around the system.  Since calls to
<tt>get_online_cpus()</tt> and <tt>put_online_cpus()</tt> can happen
frequently in the core kernel, this bouncing can be hard on performance.
<p>
The really sad fact in this case, though, is that CPU hotplug events are
exceedingly rare; chances are that, in most running systems, there will
never be a hotplug event until the system shuts down.  This kind of pattern
argues for a different approach to locking, where the common case is as
fast as it can be made to be.  That is exactly what <a
href="/Articles/569770/">Peter Zijlstra's CPU hotplug locking patch set</a>
sets out to do.  To reach that goal, Peter has had to create a custom locking
mechanism — a practice which is frowned upon whenever it can be avoided —
and incorporate a new RCU-based synchronization mechanism as well.  The
patch series shows the evolution of this approach; this article will follow
in the same path.
<p>
<h4>The new locking scheme</h4>
<p>
Peter's patch adds a couple of new variables related to CPU hotplugging:
<p>
<ul>
<li> <tt>__cpuhp_refcount</tt> is the new form of the reference count 
     controlling hotplug operations.  Unlike its predecessor, though, it is
     a per-CPU variable, so each CPU can tweak its own count without
     causing cache-line contention.
<p>
<li> <tt>__cpuhp_state</tt> is an enum with three values:
     <tt>readers_fast</tt>, <tt>readers_slow</tt>, and
     <tt>readers_block</tt>.
</ul>
<p>
"Readers," in the context of this locking mechanism, are threads that call
<tt>get_online_cpus()</tt>; they need the set of online CPUs to stay stable
but make no changes to it.  A "writer," instead, is a thread executing an
actual CPU hotplug operation.
<p>
The state starts out as <tt>readers_fast</tt>, an indication that no CPU
hotplugging activity is going on and that, thus, readers can take the fast
path through the locking code.  With that in mind, here is a simplified 
form of the core of the new <tt>get_online_cpus()</tt>:
<p>
<pre>
    if (likely(__cpuhp_state == readers_fast))
	__this_cpu_inc(__cpuhp_refcount);
    else
	__get_online_cpus();
</pre>
<p>
So, when things are in the <tt>readers_fast</tt> state,
<tt>get_online_cpus()</tt> turns into a simple, per-CPU increment
operation, with no cache-line contention.  Otherwise the slow-path code
(found in <tt>__get_online_cpus()</tt>) must be run.  The
<tt>put_online_cpus()</tt> code looks similar; when no CPUs are coming or
going, all that is needed is a per-CPU decrement operation.
<p>
When it is time to add or remove a CPU, the hotplug code will make a call
to <tt>cpu_hotplug_begin()</tt>.  This function begins with these three
lines of code:
<p>
<pre>
    __cpuhp_state = readers_slow;
    synchronize_sched();
    __cpuhp_state = readers_block;
</pre>
<p>
The assignment to <tt>__cpuhp_state</tt> puts an end to the fast-path
reference count operations.  A call to <tt>synchronize_sched()</tt>
(a read-copy-update primitive that waits for each CPU to schedule at least
once) is necessary to ensure that no thread is still running in the hot-path
code in either <tt>get_online_cpus()</tt> or <tt>put_online_cpus()</tt>.
Once that condition is assured, the state is changed again to
<tt>readers_block</tt>.  That will cause new readers to block (as described
below), but there may still be old readers running, so the
<tt>cpu_hotplug_begin()</tt> call will block until all of the per-CPU
reference counts fall to zero. 
<p>
At this point, it is worth looking at what happens in the
<tt>__get_online_cpus()</tt> slow path.  If that code sees
<tt>__cpuhp_state</tt> as <tt>readers_slow</tt>, it will simply increment
the per-CPU reference count and return in the usual manner; it is still
possible to obtain a reference in this state.  If, instead, it sees
<tt>readers_block</tt>, it will increment an (atomic) count of waiting
threads, then block on a wait queue without raising the reference count.
The <tt>__put_online_cpus()</tt> slow path is simpler: it decrements the
reference count as usual, then calls <tt>wake_up()</tt> to wake any thread
that might be waiting in <tt>cpu_hotplug_begin()</tt>.
<p>
Returning to that function: <tt>cpu_hotplug_begin()</tt> will return to its
caller once all references have been returned (all of the per-CPU reference
counts have dropped to zero).  At that point, it is safe to carry out the
CPU hotplug event, changing the set of online CPUs; afterward, a call is
made to <tt>cpu_hotplug_done()</tt>.  That function reverses what was done
in <tt>cpu_hotplug_begin()</tt> in the following way:
<p>
<pre>
    __cpuhp_state = readers_slow;
    wake_up_all(&amp;cpuhp_readers);
    synchronize_sched();
    __cpuhp_state = readers_fast;
</pre>
<p>
It will then wait until the count of waiting readers drops to zero before
returning.  This wait (like the entire hotplug operation) is done holding
the global hotplug mutex, so, while that wait is happening, no other CPU
hotplug operations can begin.
<p>
This code raises some interesting questions, starting with: why does
<tt>cpu_hotplug_done()</tt> have to set the state to <tt>readers_slow</tt>,
rather than re-enabling the fast paths immediately?  The purpose here is to
ensure that any new readers that come along will see all of the changes
made by the writer while readers were blocked.  The extra memory barriers
in the slow path will ensure that all CPUs see the new state of the world
correctly.  The <tt>synchronize_sched()</tt> call is needed to ensure that
any thread that might try to block will have done so; that means, among
other things, that the count of waiting readers will be complete.
<p>
Why does <tt>cpu_hotplug_begin()</tt> explicitly block all readers?  This
behavior turns the CPU hotplug locking mechanism into one that is biased toward
writers; the moment a writer comes along, new readers are blocked almost
immediately. Things are done this way because there could be a <i>lot</i> of
readers in a large and busy system; if they cannot be blocked, writers
could be starved indefinitely.  Given that CPU hotplug operations are so
rare, there should be no real performance issues resulting from blocking
readers and allowing hotplug operations to proceed as soon as possible.
<p>
What is the purpose of the count of waiting readers?  A single writer can
put readers on hold, but those readers should be allowed to proceed before
a second hotplug operation can be carried out.  By waiting for the count to
drop to zero, <tt>cpu_hotplug_done()</tt> ensures that every reader that
was blocked will be able to proceed before the next writer clogs up the
works again.
<p>
The end result of all this work is that, most of the time, the locking
overhead associated with <tt>get_online_cpus()</tt> will be replaced by a
fast, per-CPU increment operation.  There is a cost paid in the form of
more complex locking code and, perhaps, more expensive hotplug operations,
but a CPU hotplug event is not something that needs to be optimized for.
So it seems like a net win.
<p>
<h4>rcu_sync</h4>
<p>
Interestingly enough, though, Peter's patch still wasn't fast enough for
some people.  In particular, the <tt>synchronize_sched()</tt> calls were
seen as being too expensive.  To address this problem, Oleg Nesterov put
together <a href="/Articles/569779/">a patch</a> adding a new "rcu_sync"
mechanism.  In brief, the API looks like:
<p>
<pre>
	struct rcu_sync_struct;

	void rcu_sync_enter(struct rcu_sync_struct *rss);
	void rcu_sync_exit(struct rcu_sync_struct *rss);

	bool rcu_sync_is_idle(struct rcu_sync_struct *rss);
</pre>
<p>
An rcu_sync structure starts out in the "idle" state; it can be moved out
of that state with one or more <tt>rcu_sync_enter()</tt> calls.  When an
equal number of <tt>rcu_sync_exit()</tt> calls have been made, the
structure will test as idle again.  The state changes are made using RCU so
that, in particular, <tt>rcu_sync_exit()</tt> works via an ordinary RCU
callback rather than calling <tt>synchronize_sched()</tt>.
<p>
To use this infrastructure with CPU hotplugging, Peter defined the "idle"
state as meaning that no hotplug operations are underway; then, calls to
<tt>rcu_sync_is_idle()</tt> can replace tests against the
<tt>readers_fast</tt> state described above — and the
<tt>synchronize_sched()</tt> calls as well.  That should make things faster
— though the extent of the speedup is not entirely clear.
<p>
After all this work is done, a simple mutex-protected reference count has
been replaced by a few hundred lines of complex, one-off locking code.  In
the process, the kernel has gotten a little bit harder to understand.  This
new complexity is unfortunate, but it seems to be an unavoidable by-product
of the push for increased scalability.  Getting the best performance out of
a highly concurrent system can only be made so simple.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Hotplug">Hotplug</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Read-copy-update-rcu_sync">Read-copy-update/rcu_sync</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Scalability">Scalability</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/569686/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor569886"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 19:18 UTC (Wed)
                               by <b>mikemol</b> (guest, #83507)
                              [<a href="/Articles/569886/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I would love to see CPU hotplugging as part of being a virtualization guest. If I could take a KVM guest and permit the hot-add of CPU cores during load spikes (call it an ephemeral core policy, if you like), or shift core allotments around as load patterns change around the clock, well, that'd be pretty nice.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569886/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569890"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:09 UTC (Wed)
                               by <b>busterb</b> (subscriber, #560)
                              [<a href="/Articles/569890/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Can't KVM and other virtualization layers emulate CPUs in the first place? If so, would the problem be simplified by simply giving the VM, say, 32 virtual CPUs, backed by one real CPU. Then when load increases, the number of real CPUs is increased. The virtualized OS would not need to know about hotplug at all, since it always thinks it has 32 CPUs.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569890/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569896"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:56 UTC (Wed)
                               by <b>mikemol</b> (guest, #83507)
                              [<a href="/Articles/569896/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That results in increased inefficiency in the form of context switching on the VM host, exactly as if you were to run thirty-two CPU-intensive threads on a fewer-core machine.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569896/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor569899"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 21:30 UTC (Wed)
                               by <b>pbonzini</b> (subscriber, #60935)
                              [<a href="/Articles/569899/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That's available in KVM indeed, starting with QEMU 1.5 (it's all in userspace so any kernel version will do).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569899/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569901"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 21:35 UTC (Wed)
                               by <b>mikemol</b> (guest, #83507)
                              [<a href="/Articles/569901/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Any kernel version for host, I presume. How does it register with the guest? (I'll admit to not knowing how CPU hotplug events logically propagate through x86 systems...)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569901/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569931"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 10, 2013 7:49 UTC (Thu)
                               by <b>pbonzini</b> (subscriber, #60935)
                              [<a href="/Articles/569931/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It's all ACPI magic.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569931/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
<a name="CommAnchor569889"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 19:58 UTC (Wed)
                               by <b>peterzzz</b> (guest, #70147)
                              [<a href="/Articles/569889/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You forgot to point out that the fast-path test of __cpuhp_state and the inc/dec of __cpuhp_refcount needs to be done with preemption disabled; otherwise the sync_sched()s on the write side could be satisfied by scheduling in between those two statements and things would still go haywire.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569889/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor569891"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:20 UTC (Wed)
                               by <b>peterzzz</b> (guest, #70147)
                              [<a href="/Articles/569891/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
So the point of the rcu_sync primitive is to optimise the write side of things; in particular it can do away with one or both. In the case where there's a single usage of rcu_sync_begin()...rcu_sync_exit() the user on the exit side will not have to wait for sync_sched() to complete.<br>
<p>
What happens is that we use call_rcu_sched() to schedule a callback after the appropriate GP which sets gp_state == GP_IDLE; ie. enables the fast path again.<br>
<p>
When a new rcu_sync_begin() happens before that callback fires, we can avoid the sync_sched() because we never enabled the fast path, so we don't have to wait for all those to have happened either; in which case we've avoided waiting on both sync_sched() calls.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569891/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569893"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:22 UTC (Wed)
                               by <b>peterzzz</b> (guest, #70147)
                              [<a href="/Articles/569893/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Obviously I meant to say: one or both .. sync_sched() calls.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569893/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569894"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">rcu_sync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:31 UTC (Wed)
                               by <b>corbet</b> (editor, #1)
                              [<a href="/Articles/569894/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      The article <i>did</i> say "In particular, the synchronize_sched() calls were seen as being too expensive."  Was there something that I said wrong there?
      
          <div class="CommentReplyButton">
            <form action="/Articles/569894/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor569895"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">rcu_sync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 9, 2013 20:45 UTC (Wed)
                               by <b>peterzzz</b> (guest, #70147)
                              [<a href="/Articles/569895/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I mostly replied to: "That should make things faster — though the extent of the speedup is not entirely clear." and tried to explain.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/569895/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor570175"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">rcu_sync</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 11, 2013 21:02 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/570175/">Link</a>] 
      </p>
      
      </div>
      </summary>
      Since the premise of the design, according to the article, is that CPU hotplug events are exceedingly rare, how could anyone see the synchronize_sched() calls as being too expensive?  They happen twice per CPU hotplug event, right?  How is anyone supposed to notice the improvement made by using rcu_sync?

      
          <div class="CommentReplyButton">
            <form action="/Articles/570175/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
<a name="CommAnchor570032"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 10, 2013 18:03 UTC (Thu)
                               by <b>pwsan</b> (subscriber, #56604)
                              [<a href="/Articles/570032/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Minor nit.  The article states:<br>
<p>
<font class="QuotedText">&gt; CPU hotplug events are exceedingly rare; chances are that, in most running systems, there will never be a hotplug event until the system shuts down.</font><br>
<p>
Considering that the majority of Linux devices are ARM-based Android systems, and that many of these use CPU hotplug to bring CPU cores up and down while the system is running, the above statement is probably inaccurate.<br>
<p>
However it's probably true to write that calls to {get,put}_online_cpus() are frequent with respect to actual hotplug operations, so even for most Linux systems in use, the patch set certainly seems to be an improvement.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/570032/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor570143"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 11, 2013 16:05 UTC (Fri)
                               by <b>hamjudo</b> (guest, #363)
                              [<a href="/Articles/570143/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      Performance on bringing up a CPU matters a lot, because something is waiting to use that CPU. Likewise, the rest of the system is probably doing important stuff, so the locking interaction should minimize performance hit on other processes. <p> Conversely, when shutting down a CPU because it isn't needed, the rest of the system is probably relatively idle. All that matters is how many milliwatt hours get used.<p>
It's more complex than that, because once a processor shutdown is started, you have to wait until it's entirely shutdown, before you can bring it back up. If shutdowns take so long that it becomes common to choose to bring processors back up then shutdown time becomes a fraction of start up time. Also, processors may get shutdown for thermal or power management reasons, in which case performance matters.<p>From a practical point of view, this code will be running on my cellphone years before it gets to my employer's data center.
      
          <div class="CommentReplyButton">
            <form action="/Articles/570143/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor570156"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Optimizing CPU hotplug locking</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 11, 2013 16:40 UTC (Fri)
                               by <b>pwsan</b> (subscriber, #56604)
                              [<a href="/Articles/570156/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
What you write is true.  It's worth noting though that the hotplug lock overhead of CPU hot adds or removes is a tiny fraction of the total duration of the operation.  It's dwarfed by the time spent migrating tasks and timers, calling the notifier chains, etc.<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/570156/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2013, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
