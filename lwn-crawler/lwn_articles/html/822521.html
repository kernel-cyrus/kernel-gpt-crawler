        <!DOCTYPE html>
        <html lang="en">
        <head><title>DMA-BUF cache handling: Off the DMA API map (part 2) [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/822521/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/822870/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/822521/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>DMA-BUF cache handling: Off the DMA API map (part 2)</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</blockquote>
<div class="GAByline">
           <p>June 11, 2020</p>
           <p>This article was contributed by John Stultz</p>
           </div>
<a href="/Articles/822052/">Part 1 of this series</a>, covered some
background on ION, DMA-BUF heaps, the DMA API, and the concept of
"ownership" when it comes to handling CPU-cache maintenance, finally ending
on a conventional DMA API view of how DMA-BUF cache handling should be
done.  The article concluded with a discussion of why the traditional DMA
APIs can perform poorly on contemporary systems.  This article completes
the series with an exploration of
some of the approaches that DMA-BUF exporters can use to avoid
unnecessary cache operations along with some rough proposals for how we
might improve things.
<p>

The first part left off with the DMA API's viewpoint that the ownership of
a DMA-BUF is transferred to the device domain with a call to
<tt>dma_buf_map_attachment()</tt> and transfers back to the CPU with
<tt>dma_buf_unmap_attachment()</tt>; cache-maintenance operations are done
on each call.  This sequence provides correctness with regard to CPU-cache
handling but, for buffer pipelines that involve many devices where the CPU
doesn't actually touch the buffer, these cache operations on each map and
unmap call add up and can cause dramatic performance problems.

<p>
<h4>Who owns the buffer?</h4>
<p>
To avoid these extraneous cache operations, the DMA-BUF interface allows
one aspect of the DMA-API rules to be turned upside down.  The DMA API,
remember, assumes that the CPU is the natural owner of all memory and that
it is only during DMA transactions — where buffer ownership has been clearly
transferred to the device — that careful rules are needed.  The DMA-BUF
interfaces, instead, require the CPU to call <a
href="https://elixir.bootlin.com/linux/v5.7.1/source/drivers/dma-buf/dma-buf.c#L1064"><tt>dma_buf_begin_cpu_access()</tt></a>
before accessing a DMA-BUF.  After it is done, the CPU calls <a
href="https://elixir.bootlin.com/linux/v5.7.1/source/drivers/dma-buf/dma-buf.c#L1100"><tt>dma_buf_end_cpu_access()</tt></a>.
If the buffer is accessed from user space, these transitions are effected
using the <tt>DMA_BUF_IOCTL_SYNC</tt> <tt>ioctl()</tt> command instead.

<p>
Specifically:
<p>
<blockquote>
<dl>
<dt><tt>dma_buf_begin_cpu_access()</tt></dt>
<dd>
	Allows the exporter to ensure that the memory is actually available
	for CPU access; the exporter might need to allocate or swap-in and
	pin the backing storage. The exporter also needs to ensure that CPU
	access is coherent for the requested access direction.
</dd>	
<p>
<dt><tt>dma_buf_end_cpu_access()</tt></dt>
<dd>
	Called when the importer is done accessing the CPU. The exporter
	can use this to flush caches and unpin any resources pinned in
	<tt>dma_buf_begin_cpu_access()</tt>.
</dd>
</dl>
</blockquote>
<p>

This approach allows one to look at the DMA-BUF memory as not being owned
by the CPU by default; it can be considered to be in the device domain by
default instead.  CPU-cache handling can thus be done primarily in those
calls to ensure the CPU gets a consistent view of the DMA-BUF and may allow
for the avoidance of costly cache operations in cases where we
pass, map, and access the DMA-BUF only between multiple devices.

<p>

However, this inconsistency with the DMA API causes confusion, and not all
DMA-BUF exporters take the same approach. Some exporters try to
abide by the DMA API, flushing and invalidating the CPU cache on every map and
unmap operation, some may leave those cache operations to the
exporter's begin and end hooks, and others do both.

<p>

While DMA-BUFs were designed to share buffers between user space and
multiple devices, the first DMA-BUF exporters were tied to specific
drivers, often superseding custom, driver-specific buffer-allocation code.
Consider, for example, a GPU driver that allocates a buffer, performs some
rendering into it, and provides a handle back to user space. The user-space
process could then pass that buffer, and others along with it, back to the
GPU to compose the web-browser window with the other windows that were open in a
desktop environment. Switching to a DMA-BUF provided a more generic handle
type, so it made sense to use even if the buffers weren't really being
shared between multiple devices.
<p>
But, knowing that the buffer was shared between just the CPU and the
device, the DMA-BUF exporter could optimize some of the cache
operations. For instance, some DMA-BUF exporters cache the
scatter/gather table resulting from the first DMA mapping operation and, as
long as the <tt>dma_buf_map_attachment()</tt> calls are done in the same
direction, reuse that table.  In this way, they can avoid expensive cache
operations on each <tt>dma_buf_map_attachment()</tt> and
<tt>dma_buf_unmap_attachment()</tt> call, finally releasing the mapping in
<tt>dma_buf_detach()</tt>. These optimizations work because the
exporters are tied to the device, so the buffers aren't really being shared, or
the devices the buffers are shared with are cache coherent, so the cache
maintenance is unnecessary.

  <p>

This scheme is efficient, but it has resulted in the dozen or so DMA-BUF
exporters upstream having different cache-handling and usage semantics. So
when we start to look at how to implement generic DMA-BUF exporters to
support multiple-device pipelines in some sort of performant way, the
rules of the road are not clearly established.
<p>

<h4>Handling ownership with multiple mappings</h4>
<p>

While the DMA API provides good guidance for specifying ownership using the
map and unmap calls, getting good performance on mobile devices often
requires that multiple devices and the CPU all have active mappings to a
buffer at the same time.  That makes the concept of buffer ownership quite
a bit more subtle. For instance, it's common in graphics to have a buffer
mapped both by the GPU and a display at the same time. To do this, the
system has to be able to share a frame buffer between multiple devices
before the frame is finished drawing. This allows the GPU to write to the
buffer directly and then signal the display driver, which then can
immediately show the buffer.
<p>

 For this specific use case, <a
href="https://www.kernel.org/doc/html/v5.6/driver-api/dma-buf.html#dma-fences">DMA-BUF
fences</a> built on the <a href="/Articles/702339/">explicit fence
infrastructure</a> were added, providing a mechanism for a driver (or user
space) to wait on a fence that is specific to a buffer.  Another driver
will eventually signal that fence, initiating the transfer of ownership.


However, supporting these parallel mappings requires careful
cache handling. Usually this is left to the drivers to do explicitly using
the DMA-API synchronization calls.  When one is
working on a single integrated device with a custom vendor kernel, it is
possible to know which driver passes buffers to which and, thus, to be able
to add the optimal and correct cache handling. But outside of that controlled
environment, it's more complicated.
<p>

So we are already seeing two different styles of ownership tracking being
handled here.  <b>Implicit handling</b> means that the ownership of a
DMA-BUF is transferred when the buffer is mapped to (or unmapped from) a device.
<b>Explicit handling</b>, instead, is where buffers are already mapped on
two (or more devices) devices and ownership is effectively transferred by
DMA-BUF fences.

<p>

DMA-BUF exporters are normally responsible for handling
the cache operations for buffers as the ownership of the buffer is passed
around.  They can do so properly in the implicit context of
<tt>dma_buf_map_attachment()</tt> and <tt>dma_buf_unmap_attachment()</tt>
calls or, alternatively, 
in the <tt>dma_buf_begin_cpu_access()</tt> and
<tt>dma_buf_end_cpu_access()</tt> calls. 


However, in the explicit case, the DMA-BUF exporter has no hooks for
DMA-BUF fence signals, so the exporter cannot do any cache management in
response to the transfer of ownership. This creates a difficult
situation, where the responsibility for the buffer cache management is
split between the DMA-BUF exporter and the drivers using the buffer.  To do
it correctly, each driver must each understand its place in a buffer
pipeline to know the coherency of the device that comes after it.
<p>

More problematically, even if the DMA-BUF exporter did have a hook for the
DMA fence signals, it has no way of knowing which of the multiple styles of
ownership tracking is being used. Do we perform cache operations on map and
unmap calls, assuming explicit usage with default CPU ownership? Or on
<tt>dma_buf_begin_cpu_access()</tt> and <tt>dma_buf_end_cpu_access()</tt>,
assuming implicit usage with default device ownership? Or do we avoid that
extra overhead and assume drivers will do explicit fence signaling to
transfer ownership?  These choices may leave us with an implementation that
is either too slow to use, or potentially incompatible with some
drivers. This is quite contrary to the goal of DMA-BUFs functioning as a
generic interchange mechanism.
<p>

So for someone trying to write a DMA-BUF exporter, this all starts to feel
like a level of&nbsp;10 ("<q>Read the documentation and you'll get it wrong</q>")
or&nbsp;11 ("<q>Follow common convention and you'll get it wrong</q>") on Rusty
Russell's <a
href="https://ozlabs.org/~rusty/ols-2003-keynote/img56.html">classic API
scale</a>, especially if you
care about performance. This poses a large problem for the goal of having
generic DMA-BUF heaps that can be shared between vendors.
<p>

<h4>Potential solutions</h4>

I feel like this situation could be improved, and have a few ideas we
might consider.
Since the DMA-BUF interface already strays from the DMA API, I think we
should establish some explicit conventions for how DMA-BUFs
should be used. Better documentation could help, so that both DMA-BUF
exporter authors and DMA-BUF users have a solid sense of the model to
follow.  In particular, we should try to:
<p>
<ul class="spacylist">
<li> Create a formal sense of ownership for DMA-BUF objects outside of
    the implicit map/unmap method that the DMA API specifies.
    
<li> Work to provide some mechanism to formally track that
    ownership. These hooks could be added to the <tt>dma_buf_ops</tt>
    structure so the
    exporter can be informed of these ownership changes.
    
<li> Deprecate implicit handling and move drivers to use this mechanism
    to mark the explicit ownership transfers.
    
<li> Add some state tracking for DMA-BUFs so that we know their cache
    state and we can correctly perform cache operations only on transitions
    of ownership that make them necessary.
</ul>    
<p>

It may be that most of the above can be achieved with documentation and
consolidating the current DMA-BUF exporter implementation semantics. The
<tt>dma_buf_begin_cpu_access()</tt> and <tt>dma_buf_end_cpu_access()</tt>
calls are sufficient to handle device-to-CPU and CPU-to-device
transitions. But their proper use needs to be explicitly defined as such
and consistently implemented by DMA-BUF exporters, formalizing the concept
that buffers are device-owned by default. This would allow for safely
implementing pre-flushed buffers and skipping unnecessary synchronization
operations. 
<p>

However, a drawback with this approach alone is that, for multiple uses by
the CPU in a series, there would be unnecessary flushes on each
call. Additionally, there is some concern that, with a mix of CPU-coherent
and non-coherent devices, we may need to do CPU-cache handling when
transferring ownership between such devices. Both of these situations might
make it useful to have some device-usage bracketing calls along with some
state tracking so that ownership transfer (and not just usage) could be
determined.

<p>

This concept of ownership will also need to consider future work to support
partial cache flushes, allowing both the CPU and device to be carefully
working on the same buffer at the same time. Thus, ownership (and the
respective cache operations) would be managed on the granularity of a
single cache line, rather than the entire buffer, possibly looking more
like advisory range locks on a file.

<p>

The DMA-BUF heaps interface (along with ION that came before it) concedes
that, in some cases, user space knows more about how a buffer will be used
than the kernel does.  Thus, it can be optimal to let user space choose
the allocation type for a given pipeline. The DMA-BUF design, which allows
the rules and policy for a buffer to be left to the exporter
implementations, provides a lot of useful flexibility, which I don't want
to eliminate. However, I do think that, as vendors start their ION
migration efforts, having clear, established conventions that don't have
large pits to fall into will be important to avoid a collection of
unnecessarily incompatible heaps and users.  Hopefully this stirs some
further discussion.

<p>

<h4>Thanks</h4>
<p>
Thanks so much to Rob Clark, Robert Foss, Sumit Semwal, Azam Sadiq Pasha
Kapatrala Syed, Daniel Vetter, and Linus Walleij for their early reviews and
feedback on this series.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Device_drivers-Support_APIs">Device drivers/Support APIs</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Direct_memory_access">Direct memory access</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Stultz_John">Stultz, John</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/822521/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor822913"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMA-BUF cache handling: Off the DMA API map (part 2)</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 12, 2020 6:27 UTC (Fri)
                               by <b>blackwood</b> (guest, #44174)
                              [<a href="/Articles/822913/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Maybe some lingo clarification. dma-fence wasn't created for explicit fencing, it predates that by quite a while, being used for implicit fencing. dma-fence is simply the kernel-internal construct for pipelined (see below) device synchronization, and explicit vs implicit fencing are two flavours of how this looks like to userspace and how userspace controls access and ordering when passing a dma-buf around.<br>
<p>
Other bit that's new or untraditional lingo in the article, at least for gpu heads: "explicit handling" using dma-fence is usually called "pipelined access", since it's used to queue up an entire processing pipeline across multiple components or engines, and then let it all asynchronously run to completion, with dma-fence providing hand-off and ordering. "implicit handling" is usually called synchronous handling/access, since the cpu synchronously hands the buffer to each device, and waits for that device to finish before going to the next one. Generally code in drivers/gpu is using the pipelined model, and code everywhere else (mostly drivers/media) is using the synchronous model for dma-buf access.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/822913/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor822919"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMA-BUF cache handling: Off the DMA API map (part 2)</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 12, 2020 9:58 UTC (Fri)
                               by <b>Karellen</b> (subscriber, #67644)
                              [<a href="/Articles/822919/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>Really interesting article, but thanks so much for the "Classic API Scale" link. I'd not seen that before.

<p>Was even better to go back to <a href="https://ozlabs.org/~rusty/ols-2003-keynote/img39.html">the first slide</a> in that run and play them all forwards.
      
          <div class="CommentReplyButton">
            <form action="/Articles/822919/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor823031"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMA-BUF cache handling: Off the DMA API map (part 2)</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 15, 2020 4:20 UTC (Mon)
                               by <b>alison</b> (subscriber, #63752)
                              [<a href="/Articles/823031/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Isn't the main cause of difficulty that the CPU and the device need so much detailed information about each other?   Don't most subsystems solve this problem with an intermediary "host" library that abstracts these details away?  Perhaps the required speed for cache operations won't allow the luxury of an additional abstraction layer?<br>
<p>
Great article, by the way.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/823031/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor824005"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">DMA-BUF cache handling: Off the DMA API map (part 2)</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 21, 2020 13:44 UTC (Sun)
                               by <b>hexiaolong2008</b> (guest, #108652)
                              [<a href="/Articles/824005/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hi John,<br>
<p>
Thanks for these two articles! They are really great! And I've translated them into Chinese on my blog (all rights reserved):<br>
<p>
Part 1: <a href="https://blog.csdn.net/hexiaolong2009/article/details/106745686">https://blog.csdn.net/hexiaolong2009/article/details/1067...</a><br>
Part 2: <a href="https://blog.csdn.net/hexiaolong2009/article/details/106843185">https://blog.csdn.net/hexiaolong2009/article/details/1068...</a><br>
<p>
But I still have some questions about the following paragraph:<br>
<p>
<font class="QuotedText">&gt; "But, knowing that the buffer was shared between just the CPU and the device, the DMA-BUF exporter could optimize some of the cache operations. For instance, some DMA-BUF exporters cache the scatter/gather table resulting from the first DMA mapping operation and, as long as the dma_buf_map_attachment() calls are done in the same direction, reuse that table. In this way, they can avoid expensive cache operations on each dma_buf_map_attachment() and dma_buf_unmap_attachment() call, finally releasing the mapping in dma_buf_detach()."</font><br>
<p>
I'm so confused about the first sentence. I think the optimization instance you give in this paragraph is just used for multi-device buffer sharing, which has no CPU access. So we can avoid the cache operations on each device. But how to understand this words "knowing that the buffer was shared between just the CPU and the device"? I can't find any relationship between this sentence and the instance along with it. Do you mean the cache operation is only necessary when the buffer was shared between just the CPU and the device, so we can optimize it for mult-devices buffer sharing which has no cpu access?<br>
<p>
<font class="QuotedText">&gt; "These optimizations work because the exporters are tied to the device, so the buffers aren't really being shared, or the devices the buffers are shared with are cache coherent, so the cache maintenance is unnecessary."</font><br>
<p>
How to understand the words "so the buffers aren't really being shared"? I think the buffer was still shared with other devices. Or, do you mean that the buffer which was shared just between CPU and device then can be called "shared"，if the buffer was shared between devices without CPU access then it can't be called "shared"？<br>
<p>
Maybe my English is so pool that I can't understand this paragraph. But I really want to know what these two sentences exactly mean. Waiting for your reply.  Thanks!<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/824005/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2020, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
