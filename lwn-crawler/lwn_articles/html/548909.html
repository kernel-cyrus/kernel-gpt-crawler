        <!DOCTYPE html>
        <html lang="en">
        <head><title>Wait/wound mutexes [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/548909/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/548477/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/548909/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Wait/wound mutexes</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>We're bad at marketing</b>
<p>
We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please <a href="/Promo/nsn-bad/subscribe">subscribe today</a> to help us keep doing that, and so
we donâ€™t have to get good at marketing.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 1, 2013</br>
           </div>
Developers wanting to add new locking primitives to the kernel tend to be
received with a certain amount of skepticism.  The kernel is already well
equipped with locking mechanisms, and experience shows that new mechanisms
tend to be both unnecessary and hard to get right.  The "<a
href="/Articles/548780/">wait/wound mutex mechanism</a>" proposed by
Maarten Lankhorst may well get that kind of response.  But it is an
interesting approach to a specific locking problem that merits a closer
look.
<p>
<h4>A conceptual overview</h4>
<p>
Situations where multiple locks must be held simultaneously pose a risk of
deadlocks: if the order in which those locks are acquired is not always the
same, there will eventually come a time when two threads find themselves
blocked, each waiting for the other to release a lock.  Kernel code tends
to be careful about lock ordering, and the "lockdep" checking tool
has gotten quite good about finding code that violates the rules.  So
deadlocks are quite rare, despite the huge number of locks used by the
kernel.
<p>
But what about situations where the ordering of lock acquisition cannot be
specified in advance, or, even worse, is controlled by user space?
Maarten's patch describes one such scenario: a chain of buffers used with
the system's graphical processing unit (GPU).  These buffers must, at
various times, be "owned" by the GPU itself, the GPU driver, user space, and,
possibly, another driver completely, such as for a video frame grabber.
User space can submit the buffers for processing in an arbitrary order, and
the GPU may complete them in a different order.  If locking is used to
control the ownership of the buffers, and if multiple buffers must be
manipulated at once, avoiding deadlocks could become difficult.
<p>
Imagine a simple situation where there are two buffers of interest:
<p>
<blockquote>
<img src="https://static.lwn.net/images/2013/wake-wound1.png" alt="[buffers]" width=442 height=72
border=0> 
</blockquote>
<p>
Imagine further that we have two threads (we'll call them T1 and T2) that
attempt to lock 
both buffers in the opposite order: T1 starts with Buffer&nbsp;A, while T2
starts with Buffer&nbsp;B.  As long as they do not both try to grab the
buffers at the same time, things will work.  But, someday, each will
succeed in locking one buffer and a situation like this will develop:
<p>
<blockquote>
<img src="https://static.lwn.net/images/2013/wake-wound2.png" alt="[locked buffers]" width=442 height=72
border=0> 
</blockquote>
<p>
The kernel's existing locking primitives have no answer to a situation like
this other than "don't do that."  The wait/wound mutex, instead, is
designed for just this case.  In general terms, what will happen in this
situation is:
<p>
<ul>
<li> The thread that "got there first" will simply sleep until the
     remaining buffer becomes available.  If T1 started the process of
     locking the buffers first, it will be the thread that waits.
<p>
<li> The other thread will be "wounded," meaning that it will be told it
     must release any locks it holds and start over from scratch.
</ul>
<p>
So if T2 is wounded, the deadlock will be resolved by telling T2 to release
Buffer&nbsp;B; it must then wait until that buffer becomes available again
and start over.  So the situation will look something like this: 
<p>
<blockquote>
<img src="https://static.lwn.net/images/2013/wake-wound3.png" alt="[locked buffers]" width=442 height=72
border=0> 
</blockquote>
<p>
Once T1 has released the buffers, T2 will be able to retry and, presumably,
make forward progress on its task.
<p>
<h4>The details</h4>
<p>
The first step toward using a set of locks within the wait/wound mechanism
is to define a "class"; this class is essentially a context within which
the locks are to be acquired.  When multiple threads contend for the same
locks, they must do so using the same class.  A wait/wound class is defined
with:
<p>
<pre>
    #include &lt;linux/mutex.h&gt;

    static DEFINE_WW_CLASS(my_class);
</pre>
<p>
As far as users of the system are concerned, the class needs to exist, but
it is otherwise opaque; there is no explicit initialization required.
Internally, the main purpose for the class's existence is to hold a
sequence number (an atomic
counter) used to answer the "who got there first" question; it also contains
some information used by lockdep to verify correct use of wait/wound locks.
<p>
The acquisition of a specific set of locks must be done within a "context"
that tracks the specific locks held.  Before acquiring the first lock, a
call should be made to:
<p>
<pre>
    void ww_acquire_init(struct ww_acquire_ctx *ctx, struct ww_class *ww_class);
</pre>
<p>
This call will assign a sequence number to the context and do a bit of record
keeping.  Once that has been done, it is possible to start acquiring locks:
<p>
<pre>
    int ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx);
</pre>
<p>
If the lock has been successfully acquired, the return value will be zero.
When all goes well, the thread will manage to acquire all of the locks it
needs.  Once  that process is complete, that fact should be
signaled with:
<p>
<pre>
    void ww_acquire_done(struct ww_acquire_ctx *ctx);
</pre>
<p>
This function is actually a no-op in the current implementation, but that
could change in the future.  After this call, the processing of the locked
data can proceed normally.  Once the job is done, it is time to release the
locks and clean up:
<p>
<pre>
    void ww_mutex_unlock(struct ww_mutex *lock);
    void ww_acquire_fini(struct ww_acquire_ctx *ctx);
</pre>
<p>
Each held lock should be released with <tt>ww_mutex_unlock()</tt>; once
<i>all</i> locks have been released, the context should be cleaned up with
<tt>ww_acquire_fini()</tt>.
<p>
The above description describes what happens when all goes well, but it has
left out an important case that all wait/wound mutex users must handle:
the detection of a potential deadlock.  That case comes about whenever an
attempt is made to lock a <tt>ww_mutex</tt> that is already locked; in this
case, there are three possible outcomes.  
<p>
The first of these comes about if the locking thread already holds that
<tt>ww_mutex</tt> and is attempting to lock it for a second time.  With
ordinary mutexes, this would be an error, but the wait/wound mechanism is
designed for this case.  Evidently, sometimes, the ordering of the locking
is <i>so</i> poorly defined that multiple locking attempts can happen.  In
such cases, <tt>ww_mutex_lock()</tt> will return <tt>-EALREADY</tt>.  The
locking thread, assuming it knows how to respond to <tt>-EALREADY</tt>, can
continue about its business.
<p>

The second possibility is that the sequence number in the context for the locking
process is higher than the number associated with thread already holding
the lock.  In this case, the new caller gets "wounded";
<tt>ww_mutex_lock()</tt> will return <tt>-EDEADLK</tt> to signal that fact.
The wounded thread 
is expected to clean up and get out of the way.  "Cleaning up" means
releasing all locks held under the relevant context with calls to
<tt>ww_mutex_unlock()</tt>.  Once all of the locks are free, the wounded
thread can try again, but only when the contended lock is released by the
victorious thread; waiting for that to happen is done with:
<p>
<pre>
    void ww_mutex_lock_slow(struct ww_mutex *lock, struct ww_acquire_ctx *ctx);
</pre>
<p>
This function will block the calling thread until <tt>lock</tt> becomes
free; once it returns, the thread can try again to acquire all of the other
locks it needs.  It is entirely possible that this thread could, once
again, fail to acquire all of the needed locks.  But, since the sequence number
increases monotonically, a once-wounded thread must eventually reach a
point where it has the highest priority and will win out.
<p>
The final case comes about when the new thread's sequence number is lower than
that of the thread currently holding the lock.  In this case, the
new thread will simply block in <tt>ww_mutex_lock()</tt> until the lock is
freed.  If the thread holding 
the contended lock attempts to acquire another lock that is already held by
the new thread, it will get the <tt>-EDEADLK</tt> status at that point; it
will then release the contended lock and let the new thread proceed.  Going
back to the example above:
<p>
<blockquote>
<img src="https://static.lwn.net/images/2013/wake-wound2.png" alt="[locked buffers]" width=442 height=72
border=0> 
</blockquote>
<p>
Thread T1, holding the lower sequence number, will
wait for Buffer&nbsp;B to be unlocked, while thread T2 will see
<tt>-EDEADLK</tt> when it attempts to lock Buffer&nbsp;A.
<p>
The documentation in the patch does not describe what happens if the
holding process never calls <tt>ww_mutex_lock()</tt> again.  In this case,
it will never know that it is supposed to back off.  But, in this case, the
holder must necessarily already have acquired all of the locks it needs, so
there should be no reason why it cannot simply finish its work and release
the locks normally.  So the end result will be the same.
<p>
<h4>Conclusion</h4>
<p>
Needless to say, there are plenty of details that have not been covered
here; see the <a href="/Articles/548921/">ww-mutex-design.txt document</a>
included with the patch set for more information.
<p>
In that document, there are code examples for three different ways of
working with wait/wound mutexes.  One need not read for long to conclude
that the API looks a bit complex and tricky to use; it will be far harder
to write correct locking code using this facility than it would be with normal
mutexes.  Perhaps that complexity is necessary, and it seems certain that
this mechanism will not be needed in many places in the kernel, so the
complexity should not spread too far.  But an API
like this can be expected to raise some eyebrows.

<p>
What is missing at this point is any real code that uses wait/wound
mutexes.  Kernel developers will certainly want to see some examples of
where this kind of locking mechanism is needed.  After all, the kernel has
made it through its first two decades without this kind of complex locking;
convincing the community that this feature is now necessary is going to
take a strong sales effort.  That is best done by showing how wait/wound
mutexes solve a problem that cannot be easily addressed otherwise.  Until
that is done, wait/wound mutexes are likely to remain an interesting bit of
code on the sidelines.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Locking_mechanisms-Mutexes">Locking mechanisms/Mutexes</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/548909/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor549040"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/wound mutexes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 7:54 UTC (Thu)
                               by <b>airlied</b> (subscriber, #9104)
                              [<a href="/Articles/549040/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Oh there is code to use it, we have code in the drm TTM layer doing ordered buffer reservations with backoff for deadlocks, and we need something at a higher level for dma-buf to use if we are sharing buffers between multiple GPU drivers or other misc drivers.<br>
<p>
So Maarten has code to port TTM over to this infrastructure already in a branch, and has posted it to dri-devel previously I think.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549040/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor549042"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/wound mutexes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 8:07 UTC (Thu)
                               by <b>mlankhorst</b> (subscriber, #52260)
                              [<a href="/Articles/549042/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yeah, the full branch is at <a href="http://cgit.freedesktop.org/~mlankhorst/linux/log/">http://cgit.freedesktop.org/~mlankhorst/linux/log/</a> , with TTM converted and a WIP to do the same on intel and sync shared dma-bufs between radeon/nouveau and intel. The actual sharing part is still a bit hacky, and less reviewed. This is because it involves synchronization between multiples gpu's, which is a step further.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549042/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor549047"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/wound mutexes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 9:03 UTC (Thu)
                               by <b>blackwood</b> (guest, #44174)
                              [<a href="/Articles/549047/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yeah, the big reason for pushing these ww mutexes into core code from ttm (where they are called reservations) is to enable cross-device synchronization of dma access to shared buffer objects (aka dma_bufs). Currently access is completely unsynchronized in the kernel, so if userspace doesn't block (which it really shouldn't), displaying a frame rendered on a discrete gpu on the integrated one will horribly tear.<br>
<p>
Rob Clark started a proposal for dma_fences (now in Maarten's branch), which are attached to each dma_buf taking part in any given gpu render batch (or any other dma op affecting a dma_buf).<br>
<p>
Now if you naively walk all the dma_bufs, lock each of them one-by-one and attach a new set of fences, races with other threads have a peculiar effect: If you're unlucky you can create a synchronization loop between a bunch of buffers and fences, and since these fences can be fully hw-based (i.e. never wake up the cpu to do the syncing) you'll end up with deadlocked gpus, each waiting on the other.<br>
<p>
Hw sync/wait deadlocks between different drivers is the kind of fun I don't want to deal with, so we need a multi-object locking scheme which works cross-devices.<br>
<p>
Note that i915 isn't currently based on ttm (and personally I'm not known as a big fan of ttm), but the proposed ww mutex stuff is massively improved:<br>
- not intermingled with all the gpu memroy management craziness ttm also does<br>
- sane, clear api (we grade on a curve in gpu-land ...) with decent documentation<br>
- _really_ good debug support - while writing the kerneldoc for Maarten's code I've checked whether any kind of interface abuse would be caught. Furthermore we have a slowpath injection debug option to exercise the contended case (-EDEADLK) with single-threaded tests.<br>
<p>
Now one critique I hear often is "why can't you guys use something simpler?".  Reasons against that:<br>
- current ttm based drivers (radeon, nouveau, ...) already deal with this complexity. Furthermore gpus tend to die randomly, so all the command submission ioctls for the big drivers (i915, radeon, nouveau) are already fully restartable. Ditching a tiny bit of code to handle the ww mutex slowpath won't sched complexity.<br>
- Current drivers rely on the -EALREADY semantics for correctnes. Crazy, I know, but like I've said: We grade on a scale ... Any simple scheme would so need to support this, too. Toghether with the first point you won't really have be able to achieve any reduction in interface complexity for drivers.<br>
- Thanks to a big discussion with Peter Zijlstra we have a rather solid plan forward for PI-boosting and bounded lock acquisition in linux-rt.<br>
<p>
Thus far all the proposed "simple" schemes fall short in one place or the other.<br>
<p>
Also, cross-device sync with dma_buf/fence isn't the only use-case I see rolling in:<br>
- i915 is in desperate need of a finer-grained locking scheme. We run into ugly OOM issues all over the place due to our current "one lock to rule them all" design. We duct-tape over the worst with horrible hacks, but spurious OOMs are still too easy to hit. Moving to a per-object lock scheme using ww mutexes will fix that. Plan B would be to use ttm, but that'd be _really_ disruptive, and I don't like ttm that much.<br>
- We're in the process of switching to per-object locking for drm modeset objects, but the complex (and dynamic) graph nature of all the connections poses some interesting issues. Ww mutexes would naturally solve this problem, too.<br>
-Daniel<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549047/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor549050"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/wound mutexes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 9:14 UTC (Thu)
                               by <b>blackwood</b> (guest, #44174)
                              [<a href="/Articles/549050/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I've forgotten to stress that in my big reply to Maarten's comment a bit, so let's reiterate: Current kernels already ship with these mad deadlock-avoiding mutexes, they're simply called reservations instead of w/w mutexes.<br>
<p>
So we have both code using w/w mutexes, and it's not really a new concept for drivers/gpu/drm. Last paragraphs needs to be updated a bit ;-)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549050/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor549136"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/Wake</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 21:07 UTC (Thu)
                               by <b>ncm</b> (guest, #165)
                              [<a href="/Articles/549136/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I see two usages intersprinkled, "wake/wound" and "wait/wound".  Is this what amounts to a benign disagreement over spelling (I note pronunciation is about the same), or did I miss something?  <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549136/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor549141"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Wait/Wake</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 2, 2013 21:16 UTC (Thu)
                               by <b>jake</b> (editor, #205)
                              [<a href="/Articles/549141/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; I see two usages intersprinkled, "wake/wound" and "wait/wound".</font><br>
<p>
Indeed.  Typo alert. "wait/wound" is correct, fixed now, thanks.<br>
<p>
jake<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549141/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor549898"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2013 21:48 UTC (Thu)
                               by <b>ksandstr</b> (guest, #60862)
                              [<a href="/Articles/549898/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There are at least two concrete reasons for using wait/wound mutexes over the more common "define a lock ordering, and (very carefully) violate it with try-lock primitives" scheme of fine-grained multiple locking.<br>
<p>
The first is that a failure to try-lock requires some sort of a fall-back procedure that either doesn't violate locking order, or does so with a different try-lock subject than in previous iterations. Coming up with that procedure is an enormous hassle, and cramps many a concurrent design. Wait/wound mutexes would seem to avoid this hazard by permitting the wounded thread to resume only after the contended mutex has been released at least once.<br>
<p>
The second is that (depending on the interface) wait/wound mutexes could reduce the "slumbering herd" effect that occurs when a thread holds a number of mutexes but then has to wait on one more, repeating down the tree. (this effect also tends to magnify non-mutex waits through the mutex scheme, making it especially pernicious.) This reduction would happen by having the wounded thread wait for the contended mutex only after releasing its own, thereby allowing sufficiently unrelated threads to proceed unimpeded. The net gain is lower aggregate latency under contention.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/549898/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550052"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2013 17:08 UTC (Fri)
                               by <b>heijo</b> (guest, #88363)
                              [<a href="/Articles/550052/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The procedure can simply be to remember the lock you wanted to take in all previous attempts and start the next attempt by sorting them by lock order and taking them.<br>
<p>
Eventually you'll succeed, although it may take time quadratic in the total number of locks that may be taken across attempts.<br>
<p>
Wait/wound on the other hand guarantees that the first-coming thread will progress in linear time in the number of locks, but it seems the other threads may burn unlimited CPU time attempting to take locks and retrying in pathological cases.<br>
<p>
This could be fixable by having the "later" thread wait directly for all earlier threads to be done, to save power at the expense of latency, although this is probably not an issue in practice.<br>
<p>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550052/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550092"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2013 19:17 UTC (Fri)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/550092/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; but it seems the other threads may burn unlimited CPU time attempting to take locks and retrying in pathological cases.</font><br>
<p>
undefined CPU time, but not unlimited.<br>
<p>
each thread gets a sequence number when they start trying to get a lock, and when two threads conflict, the one with the lower sequence number wins.<br>
<p>
As a result, every thread is guaranteed to be able to get the lock in preference to any threads that were first tried to get the lock after it did.<br>
<p>
This puts a outer bound on the amount of CPU it will waste in the meantime (admittedly, not a bound that you can readily calculate since you don't know how long the locks will be held by processes that have priority over you)<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550092/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550137"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2013 23:44 UTC (Fri)
                               by <b>brong</b> (guest, #87268)
                              [<a href="/Articles/550137/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hang on - are you saying that the sequence number you were allocated persists even after you back out and try again?<br>
<p>
My understanding was that once you're wounded, you have to restart from scratch.  If you're restarting with the same (low) sequence number rather than being allocated a brand new one, then I see your point.  Otherwise, I see starvation possibilities, the horror.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550137/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550147"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 11, 2013 2:21 UTC (Sat)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/550147/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
from the article<br>
<p>
<font class="QuotedText">&gt; But, since the sequence number increases monotonically, a once-wounded thread must eventually reach a point where it has the highest priority and will win out.</font><br>
<p>
They don't say explicitly that the sequence number is maintained, but I don't see what this would mean otherwise.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550147/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550677"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2013 13:45 UTC (Wed)
                               by <b>mlankhorst</b> (subscriber, #52260)
                              [<a href="/Articles/550677/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Correct. The current implementation preserves the sequence number. But it's an implementation detail, the user of the api will never have to work with the sequence numbers directly.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550677/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor550772"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2013 20:49 UTC (Wed)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/550772/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It may be "only an implementation detail" in that the user of the API never deals with the sequence number.<br>
<p>
But this detail is critical to avoiding the risk of permanent starvation of some thread.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550772/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
<a name="CommAnchor550449"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">They're mutexes, Jim, but not as we know them</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2013 17:40 UTC (Mon)
                               by <b>ksandstr</b> (guest, #60862)
                              [<a href="/Articles/550449/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The distinct ordering of locks is required to prevent the client from turning a set of N locks, out of which M are required, into a very expensive spinlock; the low-level idea being that the lock that violates ordering (and generates the EDEADLK status) is a different one each time, and that its being locked may end up not being due to the current client's other locks. WW mutexes prevent this by sleeping the backing-off client until the conflicting mutex has been released at least once, which is strictly required for its progress.<br>
<p>
But as you say, for algorithms where the needed set of locks cannot change between backout and retry, your solution is adequate. I've found that those algorithms generally aren't candidates for fine-grained locking, though that's neither here or there.<br>
<p>
Personally, if wait/wound mutexes remove these halfway esoteric concerns from mainstream kernel code (along with the entire idea of lock ordering), I'll be happy as a clam.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/550449/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor558742"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Nesting wait/wound mutexes?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 13, 2013 3:22 UTC (Sat)
                               by <b>vomlehn</b> (guest, #45588)
                              [<a href="/Articles/558742/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I can certainly see that notification that you have to unlock currently held mutexes to avoid a deadlock is a Good Thing and could simplify life. Still, given that you may have to undo, and then redo, work when you unlock an m/m mutex and then reacquire it, I am left with the question of knowing how far you *have to* back off in order to avoid a deadlock. It would be nice if you didn't have to release every single m/m mutex that you hold.<br>
<p>
It may be the case that, as a practical matter, this is not a performance issue. I can see, however, a maintenance issue. One subsystem may acquire a m/m mutex, then call another subsystem that acquired a different m/m mutex. This M/M mutex implementation implies that the called subsystem will have to know that the caller holds an m/m mutex and return to that subsystem (or use a callback to that subsystem) to release the first mutex. Ick.<br>
<p>
I can conceive of the m/m mutex code keeping track of the m/m mutexes held by a given task and providing feekback on whether any more mutexes need to be released, which simplifies the problem a little from the performance perspective.<br>
<p>
All-in-all, I'm not sure this is ready for prime time.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/558742/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2013, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
