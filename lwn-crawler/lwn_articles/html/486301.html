        <!DOCTYPE html>
        <html lang="en">
        <head><title>A deep dive into CMA [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/486301/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/485888/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/486301/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>A deep dive into CMA</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Ready to give LWN a try?</b>
<p>
With a subscription to LWN, you can stay current with what is happening in the Linux and free-software community and take advantage of subscriber-only site features.  We are pleased to offer you <b><a href="https://lwn.net/Promo/nst-trial/claim">a free trial subscription</a></b>, no credit card required, so that you can see for yourself.  Please, join us!
</blockquote>
<div class="GAByline">
           <p>March 14, 2012</p>
           <p>This article was contributed by Michal "mina86" Nazarewicz</p>
           </div>
<p>The <a href="/Articles/486305/">Contiguous
    Memory Allocator</a> (or CMA), which LWN <a
    href="http://lwn.net/Articles/447405/">looked at</a> back in June 2011,
    has been developed to allow allocation of big, physically-contiguous memory blocks.  Simple in principle, it has grown
    quite complicated, requiring cooperation between many
    subsystems.  Depending on one's perspective, there are different things to be
    done and watch out for with CMA.  In this article, I will
    describe 
    how to use CMA and how to integrate it with a given platform.</p>

    <p>From a device driver author's point of view, nothing should
    change.  CMA is integrated with the DMA subsystem, so the usual calls
    to the DMA API (such as <tt>dma_alloc_coherent()</tt>) should work
    as usual. In fact, device drivers should never need to call the CMA API
    directly, since instead of bus addresses and kernel mappings it
    operates on pages and page frame numbers (PFNs), and provides no
    mechanism for 
    maintaining cache coherency.</p>

    <p>For more information, looking at <a
    href="http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=blob;f=Documentation/DMA-API.txt;h=66bd97a95f10e18fc9b2a4bff7acb691957cc6a3;hb=805a6af8dba5dfdd35ec35dc52ec0122400b2610"><tt>Documentation/DMA-API.txt</tt></a>
    and <a
    href="http://git.kernel.org/?p=linux/kernel/git/torvalds/linux.git;a=blob;f=Documentation/DMA-API-HOWTO.txt;h=a0b6250add79c9c144fd1185fa91314febaa4241;hb=805a6af8dba5dfdd35ec35dc52ec0122400b2610"><tt>Documentation/DMA-API-HOWTO.txt</tt></a>
    will be useful.
    Those two documents describe the provided functions as well as giving
    usage examples.</p>

    <h4>Architecture integration</h4>

    <p>Of course, someone has to integrate CMA with the DMA subsystem of
    a given architecture.  This is performed in a few, fairly easy
    steps.</p>

    <p>CMA works by reserving memory early at boot time.  This memory,
    called a <dfn>CMA area</dfn> or a <dfn>CMA context</dfn>, is later
    returned to the buddy allocator so that it can be used by regular
    applications.  To do the reservation, one needs to
    call:</p>

<pre>
    void dma_contiguous_reserve(phys_addr_t limit);
</pre>

    <p>just after the low-level "memblock" allocator is initialized but
    prior to the buddy 
    allocator setup.  On ARM, for example, it is called in
    <tt>arm_memblock_init()</tt>, whereas on x86 it is just after memblock
    is set up in <tt>setup_arch()</tt>.</p>

    <p>The <tt>limit</tt> argument specifies physical address above
    which no memory will be prepared for CMA.  The intention is to
    limit CMA contexts to addresses that DMA can handle.  In the
    case of ARM, the limit is the minimum of <tt>arm_dma_limit</tt> and
    <tt>arm_lowmem_limit</tt>.  Passing zero will allow CMA to
    allocate its context as high as it wants.  The only constraint is
    that the reserved memory must belong to the same zone.</p>

    <p>The amount of reserved memory depends on a few Kconfig options
    and a <tt>cma</tt> kernel parameter.  I will describe them <a
    href="#size">further down</a> in the article.</p>

    <p>The <tt>dma_contiguous_reserve()</tt> function will reserve memory
    and prepare it to be used with CMA.  On some architectures (<a
    href="http://lwn.net/Articles/450286/">eg. ARM</a>) some
    architecture-specific work needs to be performed as well.  To
    allow that, CMA will call the following function:</p>

<pre>
    void dma_contiguous_early_fixup(phys_addr_t base, unsigned long size);
</pre>

    <p>It is the architecture's responsibility to provide it along with
    its declaration in the <tt>asm/dma-contiguous.h</tt> header file.  If
    a given architecture does not need any special handling, it's enough
    to provide an empty function definition.</p>

    <p>It will be called quite early, thus some subsystems
    (e.g. <tt>kmalloc()</tt>) will not be available.  Furthermore, it
    may be called several times (since, as <a
    href="#per_device">described</a> below, several
    CMA contexts may exist).</p>

    <p>The second thing to do is to change the architecture's DMA implementation to use
    the whole machinery.  To allocate CMA memory one uses:</p>

<pre>
    struct page *dma_alloc_from_contiguous(struct device *dev, int count, unsigned int align);
</pre>

    <p>Its first argument is a device that the allocation is performed on
    behalf of.  The second specifies the <em>number of pages</em> (not
    bytes or order) to allocate. The third argument is the alignment expressed as a page order.
    It enables allocation of buffers whose physical addresses are aligned
    to 
    2<sup><tt>align</tt></sup> pages.  To avoid fragmentation, if at
    all possible pass zero here.  It is worth noting that there is
    a Kconfig option (<tt>CONFIG_CMA_ALIGNMENT</tt>) which specifies
    maximum alignment accepted by the function.  Its default value is
    8 meaning 256-page alignment.</p>

    <p>The return value is the first of a sequence of <tt>count</tt>
    allocated pages.</p>

    <p>To free the allocated buffer, one needs to call:</p>

<pre>
    bool dma_release_from_contiguous(struct device *dev, struct page *pages, int count);
</pre>

    <p>The <tt>dev</tt> and <tt>count</tt> arguments are same as before,
    whereas <tt>pages</tt> is what
    <tt>dma_alloc_from_contiguous()</tt> returned. If the region passed to the function did not come from CMA, the
    function will return <tt>false</tt>.  Otherwise, it will return
    <tt>true</tt>. This removes the need for higher-level functions to track
	which allocations were made with CMA and which were made using some other
	method.</p>

    <p>Beware that <tt>dma_alloc_from_contiguous()</tt> may not be
    called from atomic context.  It performs some “heavy” operations
    such as page migration, direct reclaim, etc., which may take
    a while.  Because of that, to make
    <tt>dma_alloc_coherent()</tt> and friends work as advertised,
    the architecture needs to have a different method of allocating
    memory in atomic context.</p>

    <p>The simplest solution is to put aside a bit of memory at boot
    time and perform atomic allocations from that.  This is in fact what
    ARM is doing.  Existing architectures most likely already have a special
    path for atomic allocations.</p>

    <h4>Special memory requirements</h4>

    <p>At this point, most of the drivers should “just work”.  They
    use the DMA API, which calls CMA.  Life is beautiful.  Except some
    devices may have special memory requirements.  For instance,
    Samsung's S5P Multi-format codec requires buffers to be located in different
    memory banks (which allows reading them through two memory channels,
    thus increasing memory bandwidth).
    Furthermore, one may want to separate some devices' allocations
    from others to limit fragmentation within CMA areas.</p>

    <p>CMA operates on contexts.  Devices use one global area by
    default, but private contexts can be used as well.  There is
    a many-to-one mapping between <tt>struct device</tt>s and a
    <tt>struct cma</tt> (ie. CMA context).  This means that a single
    device driver needs to have separate <tt>struct device</tt>
    objects to use more than one CMA context, while at the same time
    several <tt>struct device</tt> objects may point to the same CMA
    context.</p>

    <a name="per_device"></a>
    <p>To assign a CMA context to a device, all one needs to do is
    call:</p>

<pre>
    int dma_declare_contiguous(struct device *dev, unsigned long size,
			       phys_addr_t base, phys_addr_t limit);
</pre>

    <p>As with
    <tt>dma_contiguous_reserve()</tt>, this needs to be called
    after memblock initializes but before too much memory gets grabbed
    from it.  For ARM platforms, a convenient place to put the call
    to this function is in the machine's <tt>reserve()</tt> callback.  This
    won't work for automatically probed devices or those loaded as
    modules, so some other mechanism will be needed if those kinds of
    devices require CMA contexts.</p>

    <p>The first argument of the function is the device that the new
    context is to be assigned to.  The second specifies the <em>size in
    bytes</em> (not in pages) to reserve for the areas.  The third is the physical address of
    the area or zero.  The last one has the same meaning as
    <tt>dma_contiguous_reserve()</tt>'s <tt>limit</tt> argument.  The
    return value is 
    either zero or a negative error code.</p>

    <p>There is a limit to how many “private” areas can be declared,
    namely <tt>CONFIG_CMA_AREAS</tt>.  Its default value is seven but
    it can be safely increased if the need arises.</p>

    <p>Things get a little bit more complicated if the same non-default CMA
    context needs to be used by two or more devices.  The 
    current API does not provide a trivial way to do that.  What can
    be done is to use <tt>dev_get_cma_area()</tt> to figure out the CMA area
    that one device is using, and <tt>dev_set_cma_area()</tt> to set the
    same context to another device.  This sequence must be called no
    sooner than in <tt>postcore_initcall()</tt>.  Here is how it might
    look:</p>

<pre>
    static int __init foo_set_up_cma_areas(void)
    {
	struct cma *cma;

	cma = dev_get_cma_area(device1);
	dev_set_cma_area(device2, cma);
	return 0;
    }    
    postcore_initcall(foo_set_up_cma_areas);
</pre>

    <p>As a matter of fact, there is nothing special about the
    default context that is created by
    <tt>dma_contiguous_reserve()</tt> function.  It is in no way
    required and the system will work without it. If there is no default
    context,  <tt>dma_alloc_from_contiguous()</tt> will return
    <tt>NULL</tt> for devices without assigned 
    areas.  <tt>dev_get_cma_area()</tt> can be used to
    distinguish between this situation and allocation failure.</p>

    <a name="size"></a>

    <p><tt>dma_contiguous_reserve()</tt> does not take a size as an
    argument, so how does it know how much
    memory should be reserved?  There are two sources of this information:

    <p>There is a set of Kconfig options, which specify the default
    size of the reservation.  All of those options are located under
    “Device Drivers” » “Generic Driver Options” » “Contiguous Memory
    Allocator” in the Kconfig menu.  They allow choosing from four
    possibilities: the size can be an absolute value in megabytes,
    a percentage of total memory, the smaller of the two, or the larger
    of the two.  The default is to allocate 16 MiBs.</p>

    <p>There is also a <tt>cma=</tt> kernel command line option.  It
    lets one specify the size of the area at boot time without the
    need to recompile the kernel.  This option specifies the size in
    bytes and accepts the usual suffixes.</p>

    <h4>So how does it work?</h4>

     <p>To understand how CMA works, one needs to know a little about
     migrate types and pageblocks.</p>

     <p>When requesting memory from the buddy allocator, one provides
     a <tt>gfp_mask</tt>.  Among other things, it specifies the
     "migrate type" of the requested page(s).  One of the migrate types
     is <tt>MIGRATE_MOVABLE</tt>.  The idea behind it is that data
     from a movable page can be migrated (or moved, hence the name),
     which works well for disk caches, process pages, etc.</p>

     <p>To keep pages with the same migrate type together, the buddy
     allocator groups
     pages into "pageblocks," each having a migrate type assigned to it.
     The allocator then tries to allocate pages from pageblocks with a type
     corresponding to the request.  If that's not possible, however, it
     <em>will</em> take pages from different pageblocks and may even
     change a pageblock's migrate type.
     This means that a non-movable page can be allocated from
     a <tt>MIGRATE_MOVABLE</tt> pageblock which can also result in that
     pageblock changing its migrate type.  This is undesirable for CMA,
     so it introduces a <tt>MIGRATE_CMA</tt> type which has one
     important property: only movable pages can be allocated from a
     <tt>MIGRATE_CMA</tt> pageblock.</p>

     <p>So, at boot time, when the <tt>dma_contiguous_reserve()</tt> and/or
     <tt>dma_declare_contiguous()</tt> functions are called, CMA talks
     to memblock to reserve a portion of RAM, just to give it back to the
     buddy system later on with the underlying pageblock's migrate type
     set to <tt>MIGRATE_CMA</tt>.  The end result is that all the
     reserved pages end up back in the buddy allocator, so they
     can be used to satisfy movable page allocations.</p>

     <p>During CMA allocation, <tt>dma_alloc_from_contiguous()</tt>
     chooses a page range and calls:</p>

<pre>
     int alloc_contig_range(unsigned long start, unsigned long end,
     	                    unsigned migratetype);
</pre>
     <p>The <tt>start</tt> and <tt>end</tt> arguments specify the page
     frame numbers (or the <dfn>PFN</dfn> range) of the target memory.  The
     last argument, 
     <tt>migratetype</tt>, indicates the migration type of the
     underlying pageblocks; in the case of CMA, this is
     <tt>MIGRATE_CMA</tt>.
     The first thing this function does is to mark the pageblocks
     contained within the <tt>[start,&nbsp;end)</tt> range as
     <tt>MIGRATE_ISOLATE</tt>.  The buddy allocator will never touch
     a pageblock with that migrate type.
     Changing the migrate type does not magically free pages, though; this is
     why <tt>__alloc_conting_migrate_range()</tt> is called next.  It
     scans the PFN range and looks for pages that can be migrated
     away.</p>

     <p>Migration is the process of copying a page to some other portion
     of system memory and updating any references to it.  The former is
     straightforward and the latter is handled by the memory management
     subsystem.  After its data has been migrated, the old page is freed by
     giving it back to the buddy allocator.  This is why the containing
     pageblocks had 
     to be marked as <tt>MIGRATE_ISOLATE</tt> beforehand.  Had they been given
     a different migrate type, the buddy allocator would not think twice
     about using them to fulfill other allocation requests.</p>

     <p>Now all of the pages that <tt>alloc_contig_range()</tt> cares
     about are (hopefully) free.  The function takes them away from buddy
     system, then changes pageblock's migrate type back to
     <tt>MIGRATE_CMA</tt>.  Those pages are then returned to the caller.

     <p>Freeing memory is much simpler process.
     <tt>dma_release_from_contiguous()</tt> delegates most of its work
     to:</p>

<pre>
     void free_contig_range(unsigned long pfn, unsigned nr_pages);
</pre>

     <p>which simply iterates over all the pages and puts them back to the
     buddy system.</p>



<h4>Epilogue</h4>

<p>The Contiguous Memory Allocator patch set has gone a long way from its <a
href="/Articles/396657/">first version</a> (and even longer from its  
predecessor – <a href="https://lkml.org/lkml/2009/5/13/100">Physical
Memory Management</a> posted almost three years ago).  On the way, it
lost some of its functionality but got better at what it does now.  On
complex platforms, it is likely that CMA won't be usable on its own,
but will be used in combination with <a href="/Articles/480055/">ION</a> and <a
href="/Articles/474819/">dmabuf</a>.</p>

<p>Even though it is at its 23rd version, CMA is still not
perfect and, as always, there's still a lot that can be done to
improve it.  Hopefully though, getting it finally merged into the -mm tree
will get more people working on it to create a solution that benefits
everyone.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Contiguous_memory_allocator">Contiguous memory allocator</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Memory_management-Large_allocations">Memory management/Large allocations</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Nazarewicz_Michal_mina86">Nazarewicz, Michal “mina86”</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/486301/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2012, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
