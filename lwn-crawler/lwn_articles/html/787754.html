        <!DOCTYPE html>
        <html lang="en">
        <head><title>Memory management for 400Gb/s interfaces [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/787754/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/787286/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/787754/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Memory management for 400Gb/s interfaces</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>This article brought to you by LWN subscribers</b>
<p>
Subscribers to LWN.net made this article &mdash; and everything that
       surrounds it &mdash; possible.  If you appreciate our content, please
       <a href="/Promo/nst-nag3/subscribe">buy a subscription</a> and make the next
       set of articles possible.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>May 8, 2019</br>
           <hr>
<a href="/Articles/lsfmm2019/">LSFMM</a>
</div>
Christoph Lameter has spent years improving Linux for high-performance
computing tasks.  During the memory-management track of the 2019 Linux
Storage, Filesystem, and Memory-Management Summit, he talked about the
problem of keeping up with a 400Gb/s network interface.  At that speed,
there simply is no time for the system to get its work done.  Some ways of
improving the situation are in sight, but it's a hard problem overall and,
despite some progress, the situation is getting worse.
<p>
The problem is that, at those data rates, the kernel's page cache is
overwhelmed and simply cannot keep up.  That is not entirely the kernel's
fault; there is an increasing mismatch between interface speeds and memory
speeds.  As a result, sites have stopped upgrading their Infiniband fabrics;
there is no point in making the fabric go any faster.  A PCIe&nbsp;3 bus
can manage 1GB/s in each lane; x86 systems have 44 lanes, all of which
must be used together to keep up with a 400Gb/s interface.  So extra
capacity on the fabric side is not useful.
<p>
PCIe 4 offers a bit of relief in the form of a doubled transfer rate but,
Lameter said, that effort is currently stalled.  Meanwhile latencies are
high.  The whole Intel computing paradigm is in trouble, he said; it is no

<a href="/Articles/787759/"><img
src="https://static.lwn.net/images/conf/2019/lsfmm/ChristophLameter-sm.jpg" alt="[Christoph
Lameter]" title="Christoph Lameter" class="rthumb"></a>

longer suitable for high-performance computing.  The <a
href="https://web.archive.org/web/20190620074439/https://opencapi.org/">OpenCAPI</a>  architecture
is somewhat faster than PCIe, but it is only available on POWER9 systems.
The fastest interlink available currently is NVIDIA's <a
href="https://www.nvidia.com/en-us/data-center/nvlink/">NVLink</a>, which
can attain 300GB/s; that too is only available on POWER9.
<p>
In the area of memory bandwidth, processor vendors are adding memory
channels; Intel has six of them now, AMD has eight.  But that adds more
pins and complicates routing.  These systems can move 20GB/s in each
channel, which puts an upper bound on what any individual thread can do; a
single thread cannot keep up with even a 100Gb/s network interface.  So
multiple cores are needed to get the job done.  There is some potential in
<a href="https://en.wikipedia.org/wiki/GDDR_SDRAM">GDDR</a> and <a
href="https://en.wikipedia.org/wiki/High_Bandwidth_Memory">HBM</a> memory;
those, combined with NVLink, show that it is possible 
to do better than current systems do.
<p>
Jesper Brouer has done <a href="/Articles/629155/">a lot of work</a>
improving the performance of the kernel's network stack; he was able to get
up to a rate of 10Gb/s.  But when the data rate is raised to 100Gb/s, there
are only 120ns available to process each packet; the system cannot take
even a single cache miss and keep up.  So that kind of network processing
must be done in hardware.  The development of the express data path (XDP)
mechanism is another sign that you just cannot use the network stack at
those rates. 
Moving some functions, such as checksums and timestamps, to the interfaces
can help somewhat.
<p>
Then, there are problems with direct I/O in the kernel; it works with
arrays of pointers to 4KB pages, meaning there is little opportunity for
batching.  1GB transfers are thus relatively slow.  The 5.1 kernel has
improved the situation by allowing for larger chunks of data to be managed;
that results in lower cache use, fewer memory allocations, and less
out-of-band data to communicate to devices — and, thus, higher
performance.  But this is a new feature that will not make its way into the
major distributions for some time.
<p>
The kernel's page cache, Lameter said, simply does not scale.  The fact
that it can't work with large pages makes things worse; users have to use
direct I/O or bypass the kernel entirely, which should not be necessary.
That said, there has been some progress.  The XArray data structure enables
handling multiple page sizes in the page cache.  The <a
href="/Articles/784964/">slab movable objects</a> work can help to address
fragmentation.  Work is being done to <a href="/Articles/730531/">avoid
acquiring the <tt>mmap_sem</tt> lock</a> while handling page faults, and
support for huge pages is being added to filesystems.  One option that has
not been pursued, he said, is to create a kernel that uses 2MB as its base
page size or increasing the base size to an intermediate value by grouping
4KB pages.
<p>
There is some value in persistent memory, which is attached to the memory
channels and is thus fast.  The DAX mechanism can be used to avoid the page
cache altogether.  This storage is currently limited in size, though, and
cannot be used with RDMA due to the <a
href="/Articles/787636/">well-discussed problems</a> with
<tt>get_user_pages()</tt>.
<p>
In the future, he said, kernel developers need to be thinking about
terabit streams.  There is 3D hologram streaming on the horizon, he said.
We increasingly need to move massive amounts of data, but everybody is busy
trying to avoid the kernel's limitations to get this work done.  Part of
the solution, eventually, will be new hardware architectures for
high-performance computing.
<p>
It would be nice, he concluded, if the
memory-management subsystem had a road map showing how it plans to meet
these challenges.

In the brief moment before the session ended, Matthew Wilcox said that not
having a road map is not necessarily a bad thing.  The development
community is indeed working on these problems; each developer has taken on
one piece of it.  Coordinating all of this work is what LSFMM is all about;
he now knows what others need from the subsystem.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Networking-Performance">Networking/Performance</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2019">Storage, Filesystem, and Memory-Management Summit/2019</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/787754/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor787892"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2019 4:54 UTC (Thu)
                               by <b>dmaas</b> (subscriber, #38073)
                              [<a href="/Articles/787892/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Hi Jonathan - Really appreciate your coverage of the Linux summit highlights. These talk summaries are one of the top reasons I subscribe to LWN. Keep up the good work!<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787892/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor787894"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">x86 systems have 44 lanes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2019 5:14 UTC (Thu)
                               by <b>zdzichu</b> (subscriber, #17118)
                              [<a href="/Articles/787894/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The comment about x86 maxing out at 44 lanes is misleading at best. There are CPU manufacturers with such limited processors. At the same time, server chips form AMD (EPYC) provide 128 PCIe lanes. Next generation of EPYC will move to PCIe 4.0  and could provide up to 162 lanes per processor.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787894/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor787904"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">x86 systems have 44 lanes</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2019 8:23 UTC (Thu)
                               by <b>gebi</b> (guest, #59940)
                              [<a href="/Articles/787904/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Afaik the 162 PCIe 4 Lanes is not entierly correct. AFAIK this number is for a dual epyc system which does not use the full 64 lanes for inter CPU communication (so it's not 162 lanes per CPU).<br>
<p>
From those 162 Lanes you mentioned you have to IMHO subtract 2 PCIe lanes which are reserved strictly for low speed devices (remote management),  thus leaving you with 160 Lanes for a dual CPU AMD epyc system, which is awesome!<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787904/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor787957"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2019 14:38 UTC (Thu)
                               by <b>rweikusat2</b> (subscriber, #117920)
                              [<a href="/Articles/787957/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
So, people can 'engineer' network interface whose capabilities cannot be used with run-of-the-mill x86 hardware?  Sounds like a problem for the sales dept :-&gt;.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787957/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788971"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 21, 2019 4:47 UTC (Tue)
                               by <b>gwolf</b> (subscriber, #14632)
                              [<a href="/Articles/788971/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
...Similar to the ARM SoCs that have Gigabit Ethernet... Connected via a USB2 hub :-Þ<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788971/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor787968"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 9, 2019 15:32 UTC (Thu)
                               by <b>nilsmeyer</b> (guest, #122604)
                              [<a href="/Articles/787968/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Intel has six of them now, AMD has eight.</font><br>
<p>
Technically, current AMD CPUs have 4 times 2 channels. <br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/787968/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788030"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">B and b</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 10, 2019 12:35 UTC (Fri)
                               by <b>epa</b> (subscriber, #39769)
                              [<a href="/Articles/788030/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Does the article use GB to mean gigabyte and Gb to mean gigabit?  Would be good to clarify, or better, use the same units throughout.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788030/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788194"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2019 2:55 UTC (Mon)
                               by <b>anatolik</b> (guest, #73797)
                              [<a href="/Articles/788194/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I wonder if unikernel frameworks like Unicycle [1] is a better option for ultra-low latency + high-throughput applications.<br>
<p>
[1] <a href="https://github.com/libunicycle/unicycle">https://github.com/libunicycle/unicycle</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788194/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788211"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2019 15:01 UTC (Mon)
                               by <b>rweikusat2</b> (subscriber, #117920)
                              [<a href="/Articles/788211/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I've had a (somewhat short) look at this. The text concludes with "230 microseconds mean response time is pretty impressive". That's 0.00023s. Response times in this order of magnitude (albeit for a simpler protocol) can be achieved with a perl process running atop a stock Linux kernel which has to contact a second process on the same computer for a database lookup, IOW, that's not impressive at all.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788211/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788280"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2019 16:38 UTC (Mon)
                               by <b>anatolik</b> (guest, #73797)
                              [<a href="/Articles/788280/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
You are really comparing apples to oranges. 2 hops network path adds its own latency to the response time. To compare Unicycle demo with your example fairly you need to make your perl script to parse HTTP request, generate a static HTTP response and then run it at another desktop-grade computer connected via Ethernet bridge.<br>
<p>
<font class="QuotedText">&gt; Response times in this order of magnitude</font><br>
It would be great to get numbers for similar workload (host-bridge-host with Apache) and see how much improvement unikernel solution provides.<br>
<p>
Some time ago I've been working at Google on low-level optimizations for their latency-critical datacenter services. I know that Google's engineers would die to improve its latency by 10%. If some solution like unikernel apps can reduce the latency by 30-50 percents then it is going to be a big deal.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788280/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788284"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 13, 2019 18:07 UTC (Mon)
                               by <b>rweikusat2</b> (subscriber, #117920)
                              [<a href="/Articles/788284/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
If the response time wasn't dominated by the processing time on the server side, this "optimization" would just end up being even more useless. Application protocol processing time is also not supposed to be improved by this, hence the protocol doesn't matter that much. It's still text based and there's even another intermediate application involved. The processing path is thus roughly "Perl program receives text-based request, makes a text based request to a program written in C which makes another request using some proprietary protocol to a 3rd application which then does a database lookup, sends the result back to program #2 which creates a text-based reply to send it to program #1 which then composes a text reply to the original requester." And the combined latency of this is comfortably in the 10^-4s range.<br>
<p>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788284/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor788392"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 15, 2019 16:38 UTC (Wed)
                               by <b>naptastic</b> (guest, #60139)
                              [<a href="/Articles/788392/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yeah, but it has to hit that latency target *every* *time* regardless of what else is happening on the system. Stock Linux suuuuuucks at reliable low-latency performance.<br>
<p>
&lt;rant&gt;<br>
<p>
My audio interface runs 32 channels at 24-bit 96KHz, which works out to a measly 10 MiB/s (rounding up.) Four orders of magnitude less data than these network adapters can move. In order to get glitch-free audio on a stock Linux kernel, I have to increase my acceptable latency into the tens of ms. Four orders of magnitude slower than what these network adapters need. Using those settings, fewer than 100 transfers per second happen between CPU and sound card. The puniest of bandwidth requirements, and extremely lenient latency requirements, and the kernel can just barely hang on.<br>
<p>
Using the MuQSS process scheduler, I can get 1.33ms latency with no dropouts at all; that amounts to ~750 transfers per second.  Performance only got really reliable once I ran root-over-NFS and disabled the on-board SATA controller; I'm not sure what to make of that. I haven't tried the -rt patchset in a few years, since tuning it is a lot of work, and the best I ever got still wasn't as reliable as MuQSS, last time I compared them.<br>
<p>
I'm always happy when the networking stack develops latency problems, because that's when they get fixed.<br>
<p>
&lt;/rant&gt;<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788392/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor788329"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2019 12:42 UTC (Tue)
                               by <b>da4089</b> (subscriber, #1195)
                              [<a href="/Articles/788329/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
400 Gbps ~= 50 GBps.  Let's say we had a 5GHz CPU, that's 10 bytes per clock cycle.<br>
<p>
Even if the CPU was able to actually read 10 bytes per cycle on a sustained basis, there's no time for any processing.<br>
<p>
At 10Gbps, with a lot of care, you can still do useful work with a general purpose CPU.  At 100Gbps and beyond?  Forget it.<br>
<p>
Unless there's a 100x performance boost for servers waiting around the corner, this ship has sailed.  There's not a lot of point trying to make the software cope with I/O which simply outstrips the processor.<br>
<p>
<p>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788329/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor788338"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 14, 2019 13:59 UTC (Tue)
                               by <b>excors</b> (subscriber, #95769)
                              [<a href="/Articles/788338/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I don't see why a CPU should have much trouble sustaining 10 bytes per cycle. Recent Intel desktop CPUs can read up to 64B/cycle and write 32B/cycle (per <a href="https://en.wikichip.org/wiki/intel/microarchitectures/skylake_%28client%29#Memory_subsystem">https://en.wikichip.org/wiki/intel/microarchitectures/sky...</a>), with server chips doubling that, on a single CPU core. They can also do 64B/cycle between L1 and L2, and DRAM going at 20GB/sec per channel with 6 channels is around 25B per CPU cycle at 5GHz, so it should be able to sustain well over 10B/cycle. And you shouldn't run out of time to process the data, since the processing can run concurrently with the memory accesses, and multiple cores with AVX etc can give enormous processing throughput.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788338/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor788790"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2019 14:57 UTC (Fri)
                               by <b>flussence</b> (guest, #85566)
                              [<a href="/Articles/788790/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That's why modern high end network cards and drivers have mechanisms for steering packets to specific cores/NUMAnodes. Assuming a more realistic 2.5GHz, one cycle per 20 bytes isn't useful, but 128 cores is. That's almost enough to build a router out of reasonably-priced x86 hardware (FSVO “reasonable” relative to Cisco).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788790/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor788820"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Memory management for 400Gb/s interfaces</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 19, 2019 5:33 UTC (Sun)
                               by <b>shentino</b> (guest, #76459)
                              [<a href="/Articles/788820/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
At least on architectures that support it, the kernel should provide a way for &gt;4KiB pages to be used, whether it's in the page cache, swap cache, disk I/O or anything else.<br>
<p>
There's no reason not to find a way to allow it to be exploited in some way.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/788820/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2019, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
