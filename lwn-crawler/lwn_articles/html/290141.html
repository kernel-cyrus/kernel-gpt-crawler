        <!DOCTYPE html>
        <html lang="en">
        <head><title>Block layer: integrity checking and lots of partitions [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/290141/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/289510/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/290141/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Block layer: integrity checking and lots of partitions</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Benefits for LWN subscribers</b>
<p>
The primary benefit from <a href="/Promo/nst-nag5/subscribe">subscribing to LWN</a>
       is helping to keep us publishing, but, beyond that, subscribers get
       immediate access to all site content and access to a number of extra
       site features.  Please sign up today!
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>July 15, 2008</br>
           </div>
One likes to think of disk drives as being a reliable store of data.  As
long as nothing goes so wrong as to let the smoke out of the device, blocks
written to the disk really should come back with the same bits set in the
same places.  The reality of the situation is a bit less encouraging,
especially when one is dealing with the sort of hardware which is available
at the local computer store.  Stories of blocks which have been corrupted,
or which have been written to a location other than the one which was
intended, are common.
<p>
For this reason, there is steady interest in filesystems which use
checksums on data stored 
to block devices.  Rather than take the device's word that it successfully
stored and retrieved a block, the filesystem can compare checksums and be sure.  A
certain amount of checksumming is also done by paranoid applications in
user space.  The checksums used by BitKeeper are said to have caught a
number of corruption problems; successor tools like git have checksums
wired deeply into their data structures.  If a disk drive corrupts a git
repository, users will know about it sooner rather than later.
<p>

Checksums are a useful tool, but they have one minor problem: checksum
failures tend to come when they are too late to be useful.  By the time a
filesystem or application notices that a disk block isn't quite what it
once was, the original data may be long-gone and unrecoverable.  But disk
block corruption often happens in the process of getting the data to the
disk; it would sure be nice if the disk itself could use a checksum to
ensure that (1)&nbsp;the data got to the disk intact, and (2)&nbsp;the disk
itself hasn't mangled it.
<p>

To that end, a few standards groups have put together schemes for the
incorporation of data integrity checking into the hardware itself.  These
mechanisms generally take the form of an additional eight-byte checksum
attached to each 512-byte block.  The host system generates the checksum
when it prepares a block for writing to the drive; that checksum will
follow the data through the series of host controllers, RAID
controllers, network fabrics, etc., with the hardware verifying the
checksum along each step of the way.  The checksum is stored with the data,
and, when the data is read in the future, the checksum travels back with
it, once again being verified at each step.  The end result should be that
data corruption problems are caught immediately, and in a way which
identifies which component of the system is at fault.
<p>

Needless to say, this integrity mechanism requires operating system
support.  As of the 2.6.27 kernel, Linux will have such support, at least
for SCSI and SATA drives, thanks to Martin Petersen.  The well-written <a
href="/Articles/290145/">documentation file</a> included with the data
integrity patches envisions three places where checksum generation and
verification can be performed: in the block layer, in the filesystem, and
in user space.  Truly end-to-end protection seems to need user-space
verification, but, for now, the emphasis is on doing this work in the block
layer or filesystem - though, as of this writing, no integrity-aware
filesystems exist in the mainline repository.
<p>
Drivers for block devices which can manage integrity data need to register
some information with the block layer.  This is done by filling in a
<tt>blk_integrity</tt> structure and passing it to
<tt>blk_integrity_register()</tt>.  See the document for the full details;
in short, this structure contains two function pointers.
<tt>generate_fn()</tt> generates a checksum for a block of data, and
<tt>verify_fn()</tt> will verify a checksum.  There are also functions for
attaching a tag to a block - a feature supported by some drives.  The data
stored in the tag can be used by filesystem-level code to, for example,
ensure that the block is really part of the file it is supposed to belong
to.
<p>

The block layer will, in the absence of an integrity-aware filesystem,
prepare and verify checksum data itself.  To that end, the <tt>bio</tt>
structure has been extended with a new <tt>bi_integrity</tt> field,
pointing to a <tt>bio_vec</tt> structure describing the checksum
information and some additional housekeeping.  Happily, the integrity
standards were written to allow the checksum information to be stored
separately from the actual data; the alternative would have been to modify
the entire Linux memory management system to accommodate that information.
The <tt>bi_integrity</tt> area is where that information goes;
scatter/gather DMA operations are used to transfer the checksum and data
to and from the drive together.
<p>

Integrity-aware filesystems, when they exist, will be able to take over the
generation and verification of checksum data from the block layer.  A call
to <tt>bio_integrity_prep()</tt> will prepare a given <tt>bio</tt>
structure for integrity verification; it's then up to the filesystem to
generate the checksum (for writes) or check it (for reads).  There's also a
set of functions for managing the tag data; again, see the document for the
details.
<p>

<h3>Extended partitions</h3>
<p>
One of the more annoying and long-lived annoyances in the Linux block layer
has been the limit on the number of partitions which can be created on any
one device.  IDE devices can handle up to 64 partitions, which is usually
enough, but SCSI devices can only manage 16 - including one reserved for
the full device.  As these devices get larger, and as applications which
benefit from filesystem isolation (virtualization, for example) become more
popular, this limit only becomes more irksome.
<p>
The interesting thing is that the work needed to circumvent this problem
was done some years ago when device numbers were extended to 32 bits.  Some
<a href="http://lwn.net/Articles/75928/">complicated schemes</a> were
proposed back in 2004 as a way of extending the number of partitions while
not changing any existing device numbers, but that approach was never
adopted.  In the mean time, increasing use of tools like <tt>udev</tt> has
pretty much eliminated the need for device number compatibility; on most
distributions, there are no persistent device files anymore.
<p>
So when Tejun Heo <a href="http://lwn.net/Articles/289927/">revisited the
partition limit problem</a>, he didn't bother with obscure bit-shuffling
schemes.  Instead, with his patch set, block devices simply move to a new
major device number and have all minor numbers dynamically assigned.  That
means that no block device has a stable (across boots) number; it also
means that the minor numbers for partitions on the same device are not
necessarily grouped together.  But, since nobody really ever sees the
device numbers on a contemporary distribution, none of this should matter.
<p>
Tejun's patch series is an interesting exercise in slowly evolving an
interface toward a final goal, with a number of intermediate states.  In
the end, the API as seen by block drivers changes very little.  There is a
new flag (<tt>GENHD_FL_EXT_DEVT</tt>) which allows the disk to use extended
partition numbers; once the number of minor numbers given to
<tt>alloc_disk()</tt> is exhausted, any additional partitions will be
numbered in the extended space.  The intended use, though, would appear to
be to allocate no traditional minor numbers at all - allocating disks with
<tt>alloc_disk(0)</tt> - and creating all partitions in that extended
space.  Tejun's patch causes both the IDE and sd drivers to allocate
<tt>gendisk</tt> structures in that way, moving all disks on most systems
into the (shared) extended number space.
<p>
Even though modern distributions are comfortable with dynamic device
numbers (and names, for that matter), it seems hard to imagine that a
change like this would be entirely free of systems management problems
across the full Linux user base.  Distributors may still be a little
nervous from the grief they took after the shift to the PATA drivers
changed drive names on installed systems.  So it's not really clear when
Tejun's patches might make it into the mainline, or when distributors would
make use of that functionality.  The pressure for more partitions is
unlikely to go away, though, so these patches may find their way in before
too long.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer">Block layer</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Partitions">Partitions</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/290141/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor290385"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 17, 2008 9:36 UTC (Thu)
                               by <b>smurf</b> (subscriber, #17840)
                              [<a href="/Articles/290385/">Link</a>] (9 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
There's LVM. it does exactly the same thing a partition does, only better (you can resize
arbitrary partitions, or even move them to another disk transparently).

Thus I kindof doubt that this is much of a problem.
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290385/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor290391"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 17, 2008 10:54 UTC (Thu)
                               by <b>ljt</b> (guest, #33337)
                              [<a href="/Articles/290391/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
I use LVM on top of RAID5, but it is _very_ convenient to be able to slice the enormous disks
in many parts. 
I sliced my 4 disks in 14 partitions each, making thus 14 RAID5 volumes. I can now assign
those PVs to whatever VGs I am using. It is extremely flexible.

the *only* problem I encounter is that 14 partitions is not enough: 400Go/14partitions*(4-1
RAID disks)=85 Go. I would rather have had a 20Go unit.

</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290391/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor290648"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 18, 2008 7:56 UTC (Fri)
                               by <b>dgm</b> (subscriber, #49227)
                              [<a href="/Articles/290648/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
Just to ensure I parsed it right:

Go = Gigaoctet = French abbreviation = GB (Gigabyte) ?
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290648/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor290767"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Go explained</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 18, 2008 20:57 UTC (Fri)
                               by <b>pr1268</b> (subscriber, #24648)
                              [<a href="/Articles/290767/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <p>My recently-purchased SATA disk says &quot;1 TB/To&quot; and &quot;32 MB Cache/Mo Cachette&quot; on the retail box, so my assumption is yes, this is French.  Plus, the line <i>Guarantie limit&eacute;e de 5 ans</i> would seem to confirm this.</p>

<p>I don't know French, but I can recognize it in written/printed text.  Yes, this is a late-model Seagate consumer drive. :-)</p>
      
          <div class="CommentReplyButton">
            <form action="/Articles/290767/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor290772"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Go explained</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 18, 2008 21:14 UTC (Fri)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/290772/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
It's French: 'octet'. (You often see this in older standards documents, 
too, which have to be clear about the number of bits in a byte.)

</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290772/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor290658"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 18, 2008 9:14 UTC (Fri)
                               by <b>smurf</b> (subscriber, #17840)
                              [<a href="/Articles/290658/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
Well, personally I don't see any reason for having multiple flexible-sized VGs on a single
RAID in the first place, much less ~60 of them, but maybe I'm just missing something.
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290658/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor290779"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 18, 2008 22:08 UTC (Fri)
                               by <b>giraffedata</b> (guest, #1954)
                              [<a href="/Articles/290779/">Link</a>] 
      </p>
      
      </div>
      </summary>
      The point is you can do that with LVM instead of partitions.  Slice each disk (which is a PV and VG) into 14 LVs, make 14 RAID5 volumes out of those, assign those PVs to VGs, ...
<p>
I've always hated partitions; even before LVM existed I knew a stacked device driver was a cleaner way than having partitioning intelligence in the lowest level of kernel disk management code and a weird minor number interpretation scheme.
<p>
Originally (pre-Linux), partitions were actually in a layer
<em>beneath</em> the kernel and that made sense for the problems that had to be solved at that time.  But inside Linux, LVM (or anything else layered on top of the physical device) is the cleaner way to go.

      
          <div class="CommentReplyButton">
            <form action="/Articles/290779/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor291417"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 24, 2008 9:01 UTC (Thu)
                               by <b>eduperez</b> (guest, #11232)
                              [<a href="/Articles/291417/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
<font class="QuotedText">&gt; I use LVM on top of RAID5, but it is _very_ convenient to be able to slice the enormous
disks in many parts.</font>
<font class="QuotedText">&gt; I sliced my 4 disks in 14 partitions each, making thus 14 RAID5 volumes. I can now assign
those PVs to whatever VGs I am using. It is extremely flexible</font>

Could you explain why do you need to do that, please?
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/291417/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor291442"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 24, 2008 13:07 UTC (Thu)
                               by <b>yhdezalvarez</b> (guest, #29255)
                              [<a href="/Articles/291442/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
Does LVM supports write barriers already? Last time I checked it didn't. So I'm using
partitions for now.
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/291442/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor303319"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Lots of partitions? what for?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Oct 16, 2008 5:29 UTC (Thu)
                               by <b>cortana</b> (subscriber, #24596)
                              [<a href="/Articles/303319/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I believe it does for 'linear' mappings. But that's just my recollection from a recent LWN article on the topic, so I may be wrong.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/303319/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor290443"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Block layer: integrity checking and lots of partitions</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 17, 2008 16:09 UTC (Thu)
                               by <b>mkp</b> (subscriber, #45897)
                              [<a href="/Articles/290443/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>
 Great writeup on the block integrity stuff.  Just a few comments:
</p>

<ul>
  <li>
   <p>
     The SCSI Data Integrity Field specification is fully baked and ratified and products are beginning to appear on the market.  The T13 committee that governs SATA is currently reviewing a proposal called "External Path Protection" that is essentially SCSI DIF adapted to the ATA protocol.  IOW, SATA support is work in progress but the block layer infrastructure has been designed to accomodate it.
   </p>
  </li>

  <li> 
   <p> 
    Short of using 520 byte sectors directly (and mangling the VM) there is no "standard" for DMA protection information to and from memory.  HBA interfaces are outside the scope of the T10 SCSI committee.
   </p>
   <p>
    When we started this project it became obvious that separate scatterlists for data and protection information were an absolute must.  Without them it would be far too intrusive to make Linux support end-to-end data integrity.  So we engaged with HBA vendors to make it so.  Docs available here: <a href="http://oss.oracle.com/projects/data-integrity/documentation/">http://oss.oracle.com/projects/data-integrity/documentation/</a>.
   </p>
  </li>

  <li>
   <p>
     With regards to making filesystems integrity-aware and passing protection information to and from userland: Yep, that's next on the list.  I'm hoping to be able to yak about this at the Plumbers Conference.
   </p>
  </li>
</ul>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290443/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor290834"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Block layer: integrity checking and lots of partitions</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jul 19, 2008 11:01 UTC (Sat)
                               by <b>garloff</b> (subscriber, #319)
                              [<a href="/Articles/290834/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment"><pre>
The current mapping of SCSI disks is documented in sd.c:

/*
 * Device no to disk mapping:
 * 
 *       major         disc2     disc  p1
 *   |............|.............|....|....| &lt;- dev_t
 *    31        20 19          8 7  4 3  0
 * 
 * Inside a major, we have 16k disks, however mapped non-
 * contiguously. The first 16 disks are for major0, the next
 * ones with major1, ... Disk 256 is for major0 again, disk 272 
 * for major1, ... 
 * As we stay compatible with our numbering scheme, we can reuse 
 * the well-know SCSI majors 8, 65--71, 136--143.
 */

With some more bit shuffling, support for 64 partitions would be
possible without breaking backwards compatibility. For this, the two
upper bits of disc2 could be taken, limiting us to 4k disks per major
(or 32k disks in total). That's fine, the naming scheme 
(sda-&gt;sdz,sdaa-&gt;sdaz,sdbX-&gt;sdzX,sdaaa-&gt;sdzzz) only
works for up to 18k disks only anyway.

Ugly of course ...
</pre></div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/290834/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2008, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
