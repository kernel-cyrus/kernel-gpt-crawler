        <!DOCTYPE html>
        <html lang="en">
        <head><title>Loopback NFS: theory and practice [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/595652/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/595318/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/595652/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Loopback NFS: theory and practice</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>We're bad at marketing</b>
<p>
We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please <a href="/Promo/nsn-bad/subscribe">subscribe today</a> to help us keep doing that, and so
we don’t have to get good at marketing.
</blockquote>
<div class="GAByline">
           <p>April 23, 2014</p>
           <p>This article was contributed by Neil&nbsp;Brown</p>
           </div>
<p>The Linux NFS developers have <a
href="http://comments.gmane.org/gmane.linux.nfs/8393">long known</a> that
mounting an NFS filesystem onto the same host that is exporting it
(sometimes referred to as a loopback or localhost NFS mount) can lead to
deadlocks. Beyond <a
href="https://git.kernel.org/cgit/linux/kernel/git/history/history.git/commit/?id=b91404d3496c7dd004f3e797b03fbf9795e1e7fa">one
patch</a> posted over ten years ago, little effort has been put into
resolving the situation as no convincing use case was ever presented.
Testing of the NFS implementation can certainly benefit from loopback
mounts;  this use probably triggered the mentioned patch. With that fix
in place, the remaining deadlocks do take some effort to trigger, so the
advice to testers was essentially &quot;be careful and you should be
safe&quot;.</p>

<p>For any other use case, it would seem that using a &quot;bind&quot; mount
would provide a result that is indistinguishable from a loopback NFS
mount. In short: if it hurts when you use a loopback NFS mount, then
simply don't do that.

However, a convincing use case recently came to light which
motivated more thought on the issue.  It led this author on an educational
tour of the interaction between filesystems and memory management, and
produced a  <a
href="/Articles/595743/">recently
posted patch set</a> (replacing an <a
href="http://comments.gmane.org/gmane.linux.kernel.mm/115743">earlier
attempt</a>) which removes most, and hopefully all, such deadlocks.</p>

<h4>A simple cluster filesystem</h4>

<p>That use case involves using NFS as the filesystem in a
high-availability cluster where all hosts have shared access to the
storage. For all nodes in the cluster to be able to access the storage
equally, you need some sort of cluster filesystem like OCFS2, Ceph,
or GlusterFS. If the cluster doesn't need particularly high levels
of throughput and if the system administrator prefers to stick with known
technology, NFS can provide a simple and tempting alternative.</p>

<p>To use NFS as a cluster filesystem, you mount the storage on an
arbitrary node using a local filesystem (ext4, XFS, Btrfs, etc),
export that filesystem using NFS, then mount the NFS filesystem on all other
nodes. The node exporting the filesystem can make it appear in the local
namespace in the desired location using bind mounts and no loopback NFS
is needed — at least initially.</p>

<p>As this is a high-availability cluster, it must be able to survive the
failure of any node, and particularly the failure of the node running the
NFS server. When this happens, the cluster-management software can mount the
filesystem somewhere else.  The new owner of the filesystem can  export it
via NFS and take over the IP address of the failed host; all nodes will
smoothly be able to access the 
shared storage again. All nodes, that is, except the node which has taken
over as the NFS server.</p>

<p>The new NFS-serving node will still have the shared filesystem mounted
via NFS
and will now be accessing it as a loopback NFS mount. As such, it will be
susceptible to all the deadlocks that have led us to recommend against
loopback NFS mounts in the past. In this case, it is not possible to
&quot;simply use bind mounts&quot; as the filesystem is already mounted,
applications are already using it and have files open, etc.  Unmounting
that filesystem would require stopping those applications — an action which
is clearly contrary to the high-availability goal.
</p>

<p>This scenario is clearly useful, and clearly doesn't work. So what was
previously a wishlist item, and quite far from the top of the list at
that, has now become a bug that needs fixing.</p>

<h4>Theory meets practice</h4>

<p>The deadlocks that this scenario trigger generally involve a sequence of
events like: (1)&nbsp;the NFS
server tries to allocate memory, (2)&nbsp;the memory allocator then tries
to free 
memory by writing some pages out to the filesystem via the NFS client, and
(3)&nbsp;the NFS client waits for the NFS server to make some progress. My
assumption 
had been that this deadlock was inevitable because the same memory manager
was trying to serve two separate but competing users: the NFS client and
the NFS server.</p>

<p>A possible fix might be to run the NFS server inside a virtual machine,
and to give this VM a fixed and locked allocation of memory so there would
not be any competition. This would work, but it is hardly the simple
solution that our administrator was after and would likely present challenges
in sizing the VM for optimal performance.</p>

<p>This is where I might have left things had not a colleague, Ulrich
Schairer, presented me with a system which was deadlocking exactly as
described and said, effectively, &quot;It's broken, please fix&quot;. I
reasoned that analyzing the deadlock would at least allow me to find a 
precise answer as to why it cannot work. I now know it led to more than
that.

After a sequence of patches and re-tests it turned out that there were
two classes of problem, each of which differed in important ways from the
problem which was 
addressed 10 years ago. Trying to understand these problems led to an
exploration of the nature and history of the various mechanisms already
present in Linux to avoid memory-allocation deadlocks as <a
href="http://lwn.net/Articles/594725/">reported on</a> last week.</p>

<p>With that context, it might seem that some manipulation of the
<code>__GFP_FS</code> and/or <code>PF_FSTRANS</code> flags should allow the
deadlock to be resolved. If we think of <code>nfsd</code> as simply being
the lower levels of the NFS filesystem, then the deadlock involves a lower
layer of a filesystem allocating memory and thus triggering writeout to
that same filesystem. This is exactly the deadlock that
<code>__GFP_FS</code> was designed to prevent, and, in fact, setting
<code>PF_FSTRANS</code> in all <code>nfsd</code> threads did fix the
deadlock that was the easiest to hit.</p>

<p>Further investigation revealed, as it often does, that reality is
sometimes more complex 
than theory might suggest. Using the <code>__GFP_FS</code> infrastructure,
either directly or through <code>PF_FSTRANS</code>, turns out to be neither
sufficient, nor desirable, as a solution to the problems with loopback NFS
mounts. The remainder of this article explores why it is not sufficient and
next week we will conclude with an explanation of why it isn't even
desirable.</p>

<h4>A pivotal patch</h4>

<p>Central to understanding both sides of this problem is a <a
href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=ee72886d8ed5d9de3fa0ed3b99a7ca7702576a96">change
that happened</a> in Linux 3.2. This change was authored by my colleague
Mel Gorman who fortunately sits just on the other side of the Internet from
me and has greatly helped my understanding of some of these issues (and
provided valuable review of early versions of this article).

This patch series changed the interplay between memory reclaim and
filesystem writeout in a way that, while not actually invalidating
<code>__GFP_FS</code>, changed its importance.</p>

<p>Prior to 3.2, one of the several strategies that memory reclaim would
follow was to initiate writeout of any dirty filesystem pages that it
found. Writing a dirty page's contents to persistent storage is an
obvious requirement before the page itself can be freed,
so it would seem to make sense to do it while looking for pages to
free. Unfortunately, it had some serious negative side effects.</p>

<p>One side effect was the amount of kernel stack space that could be
used. The <code>writepage()</code> function in some filesystems can be quite
complex and, as a result, can quite reasonably use a lot of stack space. If
a memory 
allocation request was made in some unrelated code that also used a lot of
stack space, then the fact that memory allocation led directly to memory
reclaim and, from there, to filesystem writeout, meant that two heavy users of
stack space could be joined together, significantly increasing the total
amount of stack space that could be required.  In some cases, the amount of
space needed could exceed the size of the kernel stack.</p>

<p>Another side effect is that pages could be written out in an
unpredictable order. Filesystems tend to be optimized to write pages out in
the order they appear in the file, first page first. This allows space on
the storage device to be allocated optimally and allows multiple pages to be
easily grouped into fewer, larger writes. When multiple processes are each
trying to reclaim memory, and each is writing out any dirty pages it finds,
the result is somewhat less orderly than we might like.</p>

<p>Hence the change in Linux 3.2 removed writeout from direct reclaim,
leaving it to be done by <code>kswapd</code> or the various filesystem
writeback threads. In such a complex system as Linux memory management, a
little change like that should be expected to have significant follow-on
effects, and the patch mentioned above was really just the first of a short
series which made the main change and then made some adjustments to restore
balance. The particular adjustment which interests us was to <a
href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=92df3a723f84cdf8133560bbff950a7a99e92bc9">add
a small delay</a> during reclaim.</p>

<h4>Waiting for writeout</h4>

<p>The writeout code that was removed would normally avoid submitting a
write if doing so might block. This can happen if the block I/O request
queue is full 
and the submission needs to wait for a free slot; it can be avoided by
checking if the backing device is &quot;congested&quot;. However, if the
process that is allocating memory is in the middle of writing to a file on
a particular device, and the memory reclaim code finds a dirty page that
can be written to that same device, then it skips the congestion test and,
thus, it
may well block. This has the effect of slowing down any process writing to
a device to match the speed of the device itself and is important for
keeping balance in memory management.</p>

<p>With the change so that direct reclaim would no longer write out dirty
file pages, this delay no longer happened (though the
<code>backing_device_info</code> field of the task structure which enabled
the delay is still present with no useful purpose).

In its place, we get an explicit small delay if all the dirty pages
looked at are waiting for a congested backing device. This delay causes
problems for loopback NFS mounts. In contrast to the implicit delay
present before Linux 3.2, this delay is not avoided by clearing
<code>__GFP_FS</code>. This is why using <code>__GFP_FS</code> or
<code>PF_FSTRANS</code> is not sufficient.</p>

<p>
Understanding this problem requires an understanding of the 
 &quot;<a
href="http://lxr.free-electrons.com/source/include/linux/backing-dev.h?v=3.14#L64">backing
device</a>&quot; object,  an abstraction within
Linux that holds some important information about the storage device
underlying a
filesystem. This information includes
the recommended read-ahead size and the request queue length — 
and also  whether the device is congested or not. For local
filesystems <tt>struct backing_dev_info</tt> maps directly to the
underlying block  
device (though, for Btrfs, which can have multiple block devices,
there are extra challenges). For NFS, the queue in this structure is
a list of requests to
be sent to the NFS server rather than to a physical device. When this queue
reaches a predefined size, the 
backing device for the NFS filesystem will be designated as
&quot;congested&quot;.</p>

<p>If the backing device for a loopback-mounted NFS filesystem ever gets
congested while memory is tight, we have a problem. As
<code>nfsd</code> tries to allocate pages to execute write requests, it will
periodically enter reclaim and, as the NFS backing device is congested, it
will be forced to sleep for 100ms. This delay will slow NFS throughput down to
several kilobytes per second and so will ensure that the NFS backing device
remains 
congested. This does not actually result in a deadlock as forward progress
is achieved, but it is a livelock resulting in severely reduced throughput,
which is nearly as bad.</p>

<p>This situation is very specific to our NFS scenario, as the problem is
caused by a backing device writing into the page cache. It is not really a
general filesystem recursion issue, so it is not the same sort of problem
that might be addressed with suitable use of <code>__GFP_FS</code>.</p>

<h4>Learning from history</h4>

<p>This issue is, however, similar to the problem from ten years ago that was
fixed by the patch mentioned in the introduction. In that case, the problem
was that a process which was dirtying pages would be slowed down until a
matching number of dirty pages had been written out. When this happened,
<code>nfsd</code> could end up being blocked until <code>nfsd</code> had
written out some pages, thus producing a deadlock. In our present case, the
delay happens when reclaiming memory rather than when dirtying memory, and
the delay has an upper limit of 100ms, but otherwise it is a similar
problem.</p>

<p>The solution back then was to add a per-process flag called
<code>PF_LESS_THROTTLE</code>, which was set only for <code>nfsd</code>
threads. This flag increased the threshold at which the process would be
slowed down (or &quot;throttled&quot;) and so broke the deadlock.
There are two important ideas to be seen in that patch:  use a
per-process flag, and do not remove the throttling completely but
relax it just enough to avoid the deadlock. If <code>nfsd</code> were
not throttled at all when dirtying pages, that would just cause other
problems.</p>

<p>With our 100ms delay, it is easy to add a test for the same per-process
flag, but the sense in which the delay should only be partially removed is
somewhat less obvious.</p>

<p>The problem occurs when <code>nfsd</code> is writing to a local
filesystem, but the NFS queue is congested. <code>nfsd</code> should
probably still be throttled when that local filesystem is congested, but
not when the NFS queue is congested. If other queues are congested, it
probably doesn't matter very much whether <code>nfsd</code> is throttled or
not, though there is probably a small preference in favor of
throttling.</p>

<p>As the <code>backing_dev_info</code> field of the task structure was
(fortuitously) not removed when direct-reclaim writeback was removed in
3.2, we can easily use <code>PF_LESS_THROTTLE</code> 
to avoid the delay in cases where <code>current-&gt;backing_dev_info</code>
(i.e. the backing device that <code>nfsd</code> is writing to) is not
congested. This may not be completely ideal, but it is simple and meets the
key requirements, so should be safe ... providing it doesn't upset other
users of <code>PF_LESS_THROTTLE</code>.</p>

<p>Though <code>PF_LESS_THROTTLE</code> has only ever been used in
<code>nfsd</code>, there have been various patches proposed between 
<a
href="http://mirror.vtx.ch/patches/downloads/linux/linux-2.6.11.7-loop_AES-3.0c.patch">2005</a> 
and <a
href="https://lkml.org/lkml/2013/10/16/348">2013</a>  adding the flag
to the writeback process used by the <code>loop</code> block device, which
makes a regular file behave like a block device. This process is in exactly
the same situation as <code>nfsd</code>: it implements a backing device by
writing into the page cache. As such, it can be expected to face exactly the
same problems as described above and would equally benefit from having
<code>PF_LESS_THROTTLE</code> set and having that flag bypass the 100ms
delay. It is probably only a matter of time before some patch to add
<code>PF_LESS_THROTTLE</code> to <code>loop</code> devices will be
accepted.</p>

<p>There are three other places where direct reclaim can be throttled. The
first is the function <code>throttle_direct_reclaim()</code>, which was <a
href="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=5515061d22f0f9976ae7815864bfd22042d36848">added</a>
in 3.6 as part of swap-over-NFS support. This throttling is explicitly
disabled for any kernel threads (i.e. processes with no user-space
component). As both <code>nfsd</code> and the <code>loop</code> device
thread are kernel threads, this function cannot affect users of
<code>PF_LESS_THROTTLE</code> and so need not concern us.</p>

<p>The other two are in <code>shrink_inactive_list()</code> (the same
function which holds the <a
href="http://lxr.free-electrons.com/source/mm/vmscan.c?v=3.14#L1559">primary
source</a> of our present pain). The <a
href="http://lxr.free-electrons.com/source/mm/vmscan.c?v=3.14#L1448">first</a>
of these repeatedly calls <tt>congestion_wait()</tt> until there  aren't
too many processes reclaiming memory at 
the same time, as this can upset some heuristics. This has previously led to a
deadlock that was fixed by avoiding the delay whenever <code>__GFP_FS</code>
or <code>__GFP_IO</code> was cleared. Further discussion of this will be
left to next time when we examine the use of <code>__GFP_FS</code> more
closely.</p>

<p>The <a
href="http://lxr.free-electrons.com/source/mm/vmscan.c?v=3.14#L1548">last
delay</a> is near the end of <code>shrink_active_list()</code>; it adds an
extra delay (via <tt>congestion_wait()</tt> again) when it appears that the
flusher threads are struggling to make 
progress. While a livelock triggered by this delay has not been seen in
testing, it is conceivable that the flusher thread could block when the NFS
queue is congested; that could lead to <code>nfsd</code> suffering this
delay as well and so keeping the queue congested. Avoiding this delay in
the same conditions as the other delay seems advisable.</p>

<h4>One down, one to go</h4>

<p>With the livelocks under control, not only for loopback NFS mounts but
potentially for the <code>loop</code> block device as well, we only need to
deal with one remaining deadlock. As we found with this first problem, the
actual change required will be rather small. The effort to understand and
justify that change, which will be explored next week, will be somewhat
more substantial.</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Clusters-Filesystems">Clusters/Filesystems</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems-NFS">Filesystems/NFS</a></td></tr>
            <tr><td><a href="/Archives/GuestIndex/">GuestArticles</a></td><td><a href="/Archives/GuestIndex/#Brown_Neil">Brown, Neil</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/595652/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor637934"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Loopback NFS: theory and practice</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 25, 2015 22:01 UTC (Wed)
                               by <b>sceptrum</b> (guest, #101591)
                              [<a href="/Articles/637934/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Are deadlocks a problem even when using the "soft" option while mounting the loopback NFS server. Will the NFS client eventually timeout, and thus break the cycle?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/637934/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor637963"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Loopback NFS: theory and practice</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Mar 26, 2015 1:32 UTC (Thu)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/637963/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Are deadlocks a problem even when using the "soft" option</font><br>
<p>
I would suggest that this is only relevant if you don't value your data - "soft" can certainly lead to data corruption.<br>
<p>
I suspect that an RPC timeout may well be able to break the deadlock, but I would need to test be sure.  I'm not entirely certain what happens when an WRITE or COMMIT request fails due to timeout.<br>
If the dirty data gets discarded, that should break the deadlock.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/637963/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2014, Eklektix, Inc.<BR>
            
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
