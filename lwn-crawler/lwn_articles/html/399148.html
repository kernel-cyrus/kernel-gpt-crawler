        <!DOCTYPE html>
        <html lang="en">
        <head><title>The 2010 Linux Storage and Filesystem Summit, day 1 [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/399148/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/398846/">Return to the Kernel page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/399148/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>The 2010 Linux Storage and Filesystem Summit, day 1</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Please consider subscribing to LWN</b>
<p>
Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit
<a href="/Promo/nst-nag1/subscribe">this page</a> to join up and keep LWN on
the net.
</blockquote>
<div class="FeatureByline">
           By <b>Jonathan Corbet</b><br>August 9, 2010</br>
           </div>
The fourth Linux storage and filesystem summit was held August&nbsp;8
and&nbsp;9 in Boston, immediately prior to LinuxCon.  This time around, a
number of developers from the memory management community were present as
well.  Your editor was also there; what follows are his notes from the
first day of the summit.
<p>


<h4>Testing tools</h4>
<p>
The first topic of the workshop was "testing and tools," led by Eric
Sandeen.  The 2009 workshop identified a generic test suite as something
that the community would very much like to have.  One year later, quite a
bit of progress has been made in the form of the xfstests package.
As the name suggests, this test suite has its origins in the XFS
filesystem, and it is still somewhat specific to XFS.  But, with the
addition of generic testing last May, about 70 of the 240 tests are now
generic.
Xfstests is concerned primarily with regression testing; it is not,
generally, a performance-oriented test suite.  Tests tend to get added when
somebody stumbles across a bug and wants to verify that it's fixed - and
that it stays fixed.  Xfstests also does not have any real facility for the
creation of test filesystems; tools like <a href="http://lwn.net/Articles/385081/">Impressions</a> are best used
for that purpose.
<p>
About 40 new tests have been added to xfstests over the last year; it is
now heavily used in ext4 development.  Most tests look for specific bugs;
there isn't a whole lot of coverage for extreme situations - millions of files
in one directory and such.  Those tests just tend to take too long.
<p>
It was emphasized that just running xfstests is not enough on its own;
tests must be run under most or all reasonable combinations of mount
options to get good coverage.  Ric Wheeler also pointed out that different
types of storage have very different characteristics.  Most developers, he
fears, tend to test things on their laptops and call the result good.
Testing on other types of storage, of course, requires access to the
hardware; USB sticks are easy, but not all developers can test on
enterprise-class storage arrays.
<p>
Tests which exercise more of the virtual memory and I/O paths would also be
nice.  There is one package which covers much of this ground: FIO,
available from <a href="http://brick.kernel.dk/snaps/">kernel.dk</a>.
Destructive power failure testing is another useful area which Red Hat (at
least) is beginning to do.  There has also been some work done using hdparm
to corrupt individual sectors on disk to see how filesystems respond.  
A wishlist item was better latency measurement, with an emphasis on seeing
how long I/O requests sit within drivers which do their own queueing.  It
was suggested that what is really needed is some sort of central site for
capturing wishlist ideas for future tests; then, whenever somebody has some
time available, those ideas are available.
<p>

In an attempt to better engage the memory management developers in the
room, it was asked: how can we make tests which cover writeback?  The key,
it seems, is to choose a workload which is large enough to force writeback,
but not so large that it pushes the system into heavy swapping.  One simple
test is a large tar run; while that is happening, monitor
<tt>/proc/vmstat</tt> to see when writeback kicks in, and when things get
bad enough that direct reclaim is invoked.
An arguably more representative test can be had with sysbench; again, the
key is to tune it so that the shared buffers fit within physical memory.
<p>
But, as Nick Piggin pointed out, anything that dirties memory is, in the
end, a writeback test.  The key is to find ways of making tests which
adequately model real-world workloads.

<p>
<h4>Memory-management testing</h4>
<p>
Your editor is messing with the timing here, but the session on testing of
memory management changes fits well with the above.  So please ignore the
fact that this session actually happened after lunch.
<p>
The question here is simple: how can memory management changes be tested to
satisfy everybody?  This is a subject which has been coming up for years;
memory management changes seem to be especially subject to "have you tested
with this other kind of workload?" questions.  Developers find this
frustrating; it never seems to be possible to do enough testing to satisfy
everybody, especially since people asking for testing of different
workloads are often unable or unwilling to supply an actual test program.
<p>
It was suggested that the real test should be "put the new code on the
Google cluster and see if the Internet breaks."  There are certain
practical difficulties with this approach, however.
So the question remains: how can a developer conclude that a memory
management change actually works?  Especially in a situation where "works"
means different things to different people?  There is far too wide a
variety of workloads to test them all.  Beyond that, memory management
changes often involve tradeoffs - making one workload better may mean
making another one worse.  Changes which make life better for everybody are
rare.
<p>

Still, it was agreed that a standard set of tests would help.  Some
suggestions were made, including hackbench, netperf, various database
benchmarks (pgbench or sysbench, for example), and the "compilebench" test
which is popular with kernel developers.  There was also some talk of
microbenchmarks; Nick Piggin noted that microbenchmarks are terrible when
it comes to arguing for the inclusion of a change, but they can be useful
for the detection of performance regressions.
<p>
Sometimes running a single benchmark is not enough; many memory management
problems are only revealed when the system comes under a combination of
stresses.  And Andrea Arcangeli made the point that, in the end, only one
test really matters: how much time does it take the system to complete
running a workload which exceeds the amount of physical RAM available?
<p>
There was some discussion of the challenges involved in tracking down
problems; Mel Gorman stated that the debugability of the virtual memory
subsystem is "a mess."  Tracepoints can be useful for this purpose, but
they are hard to get merged, partly due to Andrew Morton's
<a href="http://lwn.net/Articles/330402/">hostility to tracepoints in
general</a>.  There is also ongoing concern 
about the ABI status of tracepoints; what happens when programs (perhaps
being run by large customers of enterprise distributions) depend on
tracepoints which expose low-level kernel details?  Those tracepoints may
no longer make sense after the code changes, but breaking them may not be
an option.

<p>
<h4>Filesystem freeze/thaw</h4>
<p>
The <a href="http://lwn.net/Articles/287435/">filesystem freeze feature</a>
enables a system administrator to suspend writes to a filesystem, allowing
it to be backed up or snapshotted while in a consistent state.  It had its
origins in XFS, but has since become part of the Linux VFS layer.  There
are a few issues with how freeze works in current kernels, though.
<p>
The biggest of these problems is unmounting - what happens when the
administrator unmounts a frozen filesystem?  In current kernels, the whole
thing hangs, leaving the system with an unusable, un-unmountable filesystem
- behavior which does not further the Linux World Domination goal.  So four
possible solutions were proposed:
<p>
<ol>
<li> Simply disallow the unmounting of frozen filesystems.  Al Viro
     stated that this solution is not really an option; there are cases
     where the unmount cannot be disallowed.  When the final process exits
     the namespace in which the filesystem is mounted is one of those
     cases.  Disallowing unmounts would also break the useful
     <tt>umount&nbsp;-l</tt> option, which is meant to work at all times.
<p>
<li> Keep the filesystem frozen across the unmount, so that the filesystem
     would still be frozen after the next mount.  The biggest problem here
     is that there may be changes that the filesystem code needs to write
     to the device; if the system reboots before that can happen, bad
     things can result.
<p>
<li> Automatically thaw filesystems on unmount.
<p>
<li> Add a new <tt>ioctl()</tt> command which will cause the thawing of an
     unmounted filesystem.
</ol>
<p>
Al suggested a variant on #3, in the form of a new freeze command.  The
proper way to handle freeze is to return a file descriptor; as long as that
file descriptor is held open, the filesystem remains frozen.  This solves
the "last process exits" problem because the file descriptor will be closed
as the process exits, automatically causing the filesystem to be thawed.
Also, as Al noted, the <tt>kill</tt> command is often the system recovery
tool of choice for system administrators, so having a suitably-targeted
<tt>kill</tt> cause a frozen filesystem to be thawed makes sense.
<p>
There seemed to be a consensus that the file descriptor approach is the
best long-term solution.  Meanwhile, though, there are tools based on the
older <tt>ioctl()</tt> commands which will take years to replace in the
field.  So we might also see an implementation of #4, above, to help in
the near future.
<p>
<h4>Barriers</h4>
<p>
Contemporary filesystems go to great lengths to avoid losing data - or
corrupting things - if the system crashes.  To that end, quite a bit of
thought goes into writing things to disk in the correct order.  As a simple
example, operations written to a filesystem journal must make it to the
media before the commit record which marks those operations as valid.
Otherwise, the filesystem could end up replaying a journal with random
data, with an outcome that few people would love.
<p>
All of that care is for nothing, though, if the storage device reorders
writes on their way to the media.  And, of course, reordering is something
that storage devices do all the time in the name of increasing
performance.  The solution, for years now, has been "barrier" operations;
all writes issued before a barrier are supposed to complete before any
writes issued after the barrier.  The problem is that barriers have not
always been well supported in the Linux block subsystem, and, when they are
supported, they have a significant impact on performance.  So, even now,
many systems run with barriers disabled.
<p>
Barriers have been discussed among the filesystem and storage developers for
years; it was hoped that this year, with the memory management developers
present as well, some better solutions might be found.
<p>
There was some discussion about the various ways of implementing barriers
and making them faster.  The key point in the discussion, though, was the
assertion that barriers are not really the same as the memory barriers they
were patterned after.  There are, instead, two important aspects to block
subsystem barriers: request ordering and forcing data to disk.
That led, eventually, to one of the clearest decisions in the first day of
the summit: barriers, as such, will be no more.  The problem of ordering
will be placed entirely in the hands of filesystem developers, who will
ensure ordering by simply waiting for operations to complete when needed.
There will be no ordering issues as such in the block layer, but block
drivers will be responsible for explicitly flushing writes to physical
media when needed.  
<p>
Whether this decision will lead to better-performing and more robust
filesystem I/O remains to be seen, but it is a clearer description of the
division of responsibilities than has been seen in the past.
<p>

<h4>Transparent hugepages</h4>
<p>
At this point, the summit split into three tracks for storage, filesystem,
and memory management developers.  Your editor followed the memory
management track; with luck, we'll eventually have writeups from the other
tracks as well.
<p>
Andrea Arcangeli presented his <a
href="http://lwn.net/Articles/359158/">transparent hugepages</a> work,
starting with a discussion of the advantages of hugepages in general.
Hugepages are a feature of many contemporary processors; they allow the
memory management subsystem to use larger-than-normal page sizes in parts
of the virtual address range.  There are a number of advantages to using
hugepages in the right places, but it all comes down to performance.  
<p>
A hugepage takes much of the pressure off the processor's translation
lookaside buffer (TLB), speeding memory access.  When a TLB miss happens
anyway, a 2MB hugepage requires traversing three levels of page table
rather than four, saving a memory access and, again, reducing TLB
pressure.  The result is a doubling of the speed with which initial page
faults can be handled, and better application performance in general.
There can be some costs, especially when entire hugepages must be cleared
or copied; that can wipe out much of the processor's cache. But this cost
tends to be overwhelmed by the performance advantages that hugepages bring.
<p>
Those advantages, incidentally, are multiplied when hugepages are used on
systems hosting virtualized guests.  Using hugepages in this situation can
eliminate much of the remaining cost of running virtualized systems.
<p>
Hugepages on Linux are currently accessed through the hugetlbfs filesystem,
which was <a href="http://lwn.net/Articles/374424/">discussed in great
detail by Mel Gorman</a> on LWN earlier this year.  There are some real
limitations associated with hugetlbfs, though: hugepages are not swappable,
they must be reserved at boot time, there is no mixing of page sizes in the
same virtual memory area, etc.  Many of these problems could be fixed, but,
as Andrea put it, hugetlbfs is becoming a sort of secondary - and inferior
- Linux virtual memory subsystem.  It is time to turn hugepages into
first-class citizens in the real Linux VM.
<p>

Transparent hugepages eliminate much of the need for hugetlbfs by
automatically grouping together sections of a process's virtual address
space into hugepages when warranted.  They take away the hassles of
hugetlbfs and make it possible for the system to use hugepages with no need
for administrator intervention or application changes.  There seems to be a
fair amount of interest in the feature; a Google developer said that the
feature is attractive for internal use.
<p>
At the core of the patch is a new thread called <tt>khugepaged</tt>, which
is charged with scanning memory and creating hugepages where it makes
sense.  Other parts of the VM can split those hugepages back into
normally-sized pieces when the need arises.  Khugepaged works by allocating
a hugepage, then using the migration mechanism to copy the contents of the
smaller component pages over.  There was some talk of trying to defragment
memory and "collapse in place" instead, but it doesn't seem worth the
effort at this point.  The amount of work to be done would be about the
same except in the special case where a hugepage had been split and was
being grouped back together before much had changed - a situation which is
expected to be relatively rare.
<p>
Andrea put up a number of benchmarks showing how transparent hugepages
improve performance; the all-important kernel compile benchmark (seen as a
sort of worst case for hugepages) is 2.5% faster.  Various other benchmarks
show bigger improvements.
<p>
Transparent hugepages, it seems, will be enabled by default in the
RHEL&nbsp;6 kernel.  Andrea would really like to get the feature into
2.6.36, but the merge window is already well advanced and it's not clear
that things will work out that way.  There is still a need to convince
Linus that the feature is worthwhile, and, perhaps, some work to be done to
enable the feature on SPARC systems.
<p>
<h4>mmap_sem</h4>
<p>
The memory map semaphore (<tt>mmap_sem</tt>) is a reader-writer semaphore
which protects the tree of virtual memory area (VMA) structures describing
each address space.  It is, Nick Piggin says, one of the last nasty locking
issues left in the virtual memory subsystem.  Like many busy, global locks,
<tt>mmap_sem</tt> can cause scalability problems through cache line
bouncing.  In this case, though, simple contention for the lock can be a
problem; <tt>mmap_sem</tt> is often held while disk I/O is being performed.
With some workloads, the amount of time that <tt>mmap_sem</tt> is 
held can slow things down significantly.
<p>
Various groups, including developers at HP and Google, have chipped away at
the <tt>mmap_sem</tt> problem in the past, usually by trying to drop the
semaphore in various paths.  These patches have all run into the same
problem, though: Linus hates them.  In particular, he seems to dislike the
additional complication added to the retry paths which must be followed
when things change while the lock is dropped.  So none of this work has
gotten into the mainline.
<p>
There have also been some <a href="http://lwn.net/Articles/387720/">unfair
rwsem</a> proposals aimed at reducing <tt>mmap_sem</tt> contention; these
have run aground over fears of writer starvation.
<p>
According to Nick, the real problem is the red-black tree used to track
allocated address space; the data structure is cache-unfriendly and
requires a global lock for its protection.  His idea is to do away with
this rbtree and associate VMAs directly with the page table entries,
protecting them with the PTE locks.  This approach would eliminate much of
the locking entirely, since the page tables must be traversable without
locks, and solve the <tt>mmap_sem</tt> problem.
<p>
That said, there are some challenges.  A VMA associated with a page table
entry can cover a maximum of 2MB of address space; larger areas would have
to be split into (possibly a large number of) smaller VMAs.  It's not clear
how this mechanism would then interact with hugepages.  The instantiation
of large VMAs would require the creation of the full range of PTEs, which
is not required now; that could hurt applications with very
sparsely-populated memory areas.  Growing VMAs would have its own
challenges.  There is also the issue of free space allocation, a problem
which might be solved by preallocating ranges of addresses to each
thread sharing an address space.  In summary, the list of obstacles to be
overcome before this idea becomes practical looks somewhat daunting.
<p>
The developers in the room seemed to not be entirely comfortable with this
approach, but nobody could come up with a fundamental reason why it would not
work.  So we'll probably be seeing patches from Nick exploring this idea in
the future.
<p>
<h4>copyfile()</h4>
<p>
The <a href="http://lwn.net/Articles/333783/"><tt>reflink()</tt></a> system
call was originally proposed as a sort of fast copy operation; it would
create a new "copy" of a file which shared all of the data blocks.  If one
of the files were subsequently written to, a copy-on-write operation would
be performed so that the other file would not change.  LWN readers last
heard about this patch last September, when Linus <a
href="http://lwn.net/Articles/353048/">refused to pull it</a> for 2.6.32.
Among other things, he didn't like the name.
<p>
So now <tt>reflink()</tt> is back as <tt>copyfile()</tt>, with some
proposed additional features.  It would make the same copy-on-write copies
on filesystems that support it, but <tt>copyfile()</tt> would also be able
to delegate the actual copy work to the underlying storage device when it
makes sense.  For example, if a file is being copied on a network-mounted
filesystem, it may well make sense to have the server do the actual copy
work, eliminating the need to move the data over the network twice.  The
system call might also do ordinary copies within the kernel if nothing
faster is available.
<p>
The first question that was asked is: should <tt>copyfile()</tt> perhaps be
an asynchronous interface?  It could return a file descriptor which could
be polled for the status of the operation.  Then, graphical utilities could
start a copy, then present a progress bar showing how things were going.
Christoph Hellwig was adamant, though, that <tt>copyfile()</tt> should be a
synchronous operation like almost all other Linux system calls; there is no
need to create something weird and different here.  Progress bars neither
justify nor require the creation of asynchronous interfaces.
<p>
There was also opposition to the mixing of the old <tt>reflink()</tt> idea
with that of copying a file.  There is little perceived value in creating a
bad version of <tt>cp</tt> within the kernel.  The two ideas were mixed
because it seems that Linus seems to want it that way, but, after this discussion,
they may yet be split apart again.
<p>
<h4>Dirty limits</h4>
<p>
Jan Kara led a short discussion on the problem of dirty limits.  The tuning
knob found at <tt>/proc/sys/vm/dirty_ratio</tt> contains a number
representing a percentage of total memory.  Any time that the number of
dirty pages in the system exceeds that percentage, processes which are
actually writing data will be forced to perform some writeback directly.
This policy has a couple of useful results: it helps keep memory from
becoming entirely filled with dirty pages, and it serves to throttle the
processes which are creating dirty pages in the first place.
<p>
The default value for <tt>dirty_ratio</tt> is 20, meaning that 20% of
memory can be dirty before processes are conscripted into writeback
duties.  But that turns out to be too low for a number of applications.  In
particular, it seems that many Berkeley DB applications exhibit behavior
where they dirty a lot of pages all over memory; setting
<tt>dirty_ratio</tt> too low causes a lot of excessive I/O and serious
performance issues.  For this reason, distributions like RHEL raise this
limit to 40% by default.
<p>
But 40% is not an ideal number either; it can lead to a lot of wasted
memory when the system's workloads are mostly sequential.  Lots of dirty
pages can also cause <tt>fsync()</tt> calls to take a very long time,
especially with the ext3 filesystem.  What's really needed is a way to set
this parameter in a more automatic, adaptive way, but exactly how that
should be done is not entirely clear.
<p>
What is likely to happen in the short term is that a user-space daemon will
be written to experiment with various policies for <tt>dirty_ratio</tt>.
Some VM tracepoints can be used to catch events and tune things
accordingly.  A system which is handling a lot of <tt>fsync()</tt> calls
should probably have a lower value of <tt>dirty_ratio</tt>, for example.
In the absence of reasons to the contrary, the daemon can try to nudge the
limit higher and try to see if applications perform better.  This kind of
heuristic experimentation has its hazards, but there does not seem to be a
better method on offer at the moment.
<p>
<h4>Topology and alignment</h4>
<p>
There was a brief session on storage device topology issues; unfortunately,
it was late in the day and your editor's notes are increasingly fuzzy.
Much of the discussion had to do with 4K-sector disks.  There are still
issues, it seems, with drives which implement strange sector alignments in
an attempt to get better performance with some versions of Windows.  Linux
can cope with these drives, but only if the drives themselves export
information on what they are doing.  Not all hardware provides that
information, unfortunately.
<p>
Meanwhile, the amount of software which does make use of the topology
information exported through the kernel is increasing.  Partitioning tools
are getting smarter, and the device mapper now uses this information
properly.  The readahead code will be tweaked to create properly-aligned
requests when possible.
<p>
<a name="lightning"></a>
<h4>Lightning talks</h4>
<p>
The last session of the day was dedicated to three lightning talks.  The
first, by Matthew Wilcox, had to do with merging of git trees.  Quite a bit
of work in the VM/filesystem/storage area depends on changes made in a
number of different trees.  Making those trees fit together can be a bit of
a challenge.  That problem can be solved in linux-next, but those solutions
do not necessarily carry over into the mainline, where trees may be pulled
in just about any order - or not at all.  The result is a lot of work and
merge-window scrambling by developers, who are getting a little tired of
it.
<p>
So, it was asked, is it time for a git tree dedicated to storage as a
whole, and a storage maintainer to go with it?  The idea was to create
something like David Miller's networking tree, which is the merge point for
almost all networking-related patches.  James Bottomley made the mistake of
suggesting that this kind of discussion could not go very far without a
volunteer to manage that tree; he was then duly volunteered for the job.
<p>
The discussion moved on to how this tree would work, and, in particular,
whether its maintainer would become the "overlord of storage," or whether
it would just be a more convenient place to work out merge conflicts.  If
its maintainer is to be a true overlord, a fairly hardline approach will
need to be taken with regard to when patches would have to be ready for
merging.  It's not clear whether the storage community is ready to deal
with such a maintainer.  So, for the near future, James will run the tree
as a merge point to see 
whether that helps developers get their code into the mainline.  If it
seems like there is need for a real storage maintainer, that question will
be addressed after a development cycle or two.
<p>
Dan Magenheimer presented his <a
href="http://lwn.net/Articles/386090/">Cleancache</a> proposal, mostly with
an eye toward trying to figure out a way to get it merged.  There is still
some opposition to it, and its per-filesystem hooks in particular.  It's
hard to see how those hooks can be avoided, though; Cleancache is not
suitable for all filesystems and, thus, may not be a good fit for the VFS
layer.  The crowd seemed reasonably amenable to merging the patches, but
the chief opponent - Christoph Hellwig - was not in the room at the time.
So no real conclusions have been reached.
<p>
The final lightning talk came from Boaz Harrosh, who talked about "stable
pages."  Currently, pages which are currently under writeback can be
modified by filesystem code.  That's potentially a data integrity problem,
and it can be fatal in situations where, for example, checksums of page
contents are being made.  That is why the RAID5 code must copy all pages
being written to an array; changing data would break the RAID5 checksums.
What, asked Boaz, would break if the ability to change pages under
writeback were withdrawn?
<p>
The answer seems to be that nothing would break, but that some filesystems
might suffer performance impacts.  The only way to find out for sure,
though, is to try it.  As it happens, this is a relatively easy experiment
to run, so filesystem developers will probably start playing with it
sometime soon.
<p>
That was the end of the first day of the summit; reports from the second
day will be posted as soon as they are ready.<br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Block_layer">Block layer</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Filesystems-Workshops">Filesystems/Workshops</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_and_Memory-Management_Summit-2010">Storage, Filesystem, and Memory-Management Summit/2010</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/399148/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor399154"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 4:51 UTC (Mon)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/399154/">Link</a>] (13 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; barriers, as such, will be no more.</font><br>
<p>
Hurray!!!!! I've never liked barriers, as such.<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399154/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399156"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 5:30 UTC (Mon)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/399156/">Link</a>] (11 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
on the other hand, forcing the filesystem to figure out how to know when the hardware has completed a write is not very good either.<br>
<p>
And if the hardware supports a 'do not reorder across this' barrier, the need to fully flush things to disk before writing the things that would be after the barrier is a significant performance loss<br>
<p>
I don't see how pushing the implementation further from the hardware will help, but we'll see what happens.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399156/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399170"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 7:02 UTC (Mon)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/399170/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The hardware has lower latency for some operations, but the filesystem has more knowledge about what is required.  Moving decisions away from the hardware can be bad, but moving them closer to the filesystem can be good.  Finding the right balance is hard.<br>
<p>
As you say, we'll see what happens.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399170/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399174"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 8:17 UTC (Mon)
                               by <b>dlang</b> (guest, #313)
                              [<a href="/Articles/399174/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
the latency is that without the ability to pass the instruction on to the lower levels, the only thing the filesystem can do is the let all the queues empty<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399174/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399175"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 8:25 UTC (Mon)
                               by <b>koverstreet</b> (subscriber, #4296)
                              [<a href="/Articles/399175/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Only if there's nothing else to put in the queues, and that's not the case you need to optimize for as much.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399175/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor399172"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 8:13 UTC (Mon)
                               by <b>koverstreet</b> (subscriber, #4296)
                              [<a href="/Articles/399172/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Considering that Chris Mason is in favor of this... who is a filesystem author... that should tell you something.<br>
<p>
The problem is mainly that current barriers don't really mean any one thing, they're poorly defined... and what filesystems really need is to know when something is on disk.<br>
<p>
Also, ordering is really not a simple matter. It's not just a matter of the disk reordering it... it's the queue, and any virtual block devices in between (raid/lvm/caching (I'm the author of bcache, so this has been on my mind lately). Introducing an artificial global ordering on all the ios you see is a pain in the ass... so if the filesystems don't need it, and it isn't needed for performance, that's a lot of complexity you can cut out.<br>
<p>
Personally I think being able to specify an ordering on bios you submit would be a useful thing, partly in filesystem complexity and with higher latency storage it should potentially be a performance gain. But do not think it's a simple thing to implement, or necessary - and the current barriers certainly aren't that, so getting rid of them is a good thing.<br>
<p>
I was just thinking today that if we are going to try and implement the ability to order in flight bios, probably the way to do it would be to first implement it only in the generic block layer, _maybe_ the io scheduler at first - and come up with some working semantics. The generic block layer could support it regardless of hardware support simply by waiting to queue a bio until it had no outstanding dependencies.<br>
<p>
Such an interface could then live or die depending on if filesystems were actually able to make good use of it - if it makes life harder for them, it's probably not worth it. If it did prove useful, the implementation could be extended down the layers till it got to the hardware.<br>
<p>
-------------<br>
<p>
Just musing for a bit about what that interface might be - here's a guess:<br>
You probably want a bio to be able to depend on multiple bios, the way that looks sanest to me is add two fields:<br>
atomic_t bi_depends            /* bio may not be started until 0 */<br>
struct bio *bi_parent          /* if not NULL, decrement bi_parent-&gt;bi_depends */<br>
<p>
You'd then have to add a bit in bi_flags to indicate error on a depending bio - so it was never started: if when completing a bio, before you decrement bi_parent-&gt;bi_depends, if (error) set the error flag on the parent.<br>
<p>
With that feature I don't know if you could use NCQ - I'm not a SCSI guy - but without it it seems fairly useless, or else dangerous to use; you can't, for example, rewrite your superblock to point to the new btree root if writing the new root failed. I suppose you could use it if you used a log or a journal to write the current btree root, and then pointers to btree nodes contained the checksum of the node they pointed to - and both are good ideas, but there's still plenty of other situations where you wouldn't be able to recover (and in that case you don't really need write ordering anyways).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399172/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399385"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 17:29 UTC (Tue)
                               by <b>butlerm</b> (subscriber, #13312)
                              [<a href="/Articles/399385/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The straightforward way to use barriers without killing I/O performance is to have a concept of I/O "threads" (typically one or two per mounted filesystem) and make the barriers apply on a per I/O thread basis.  Very easy to understand and to implement - when a barrier is issued on one I/O thread requests on other I/O threads are allowed to proceed unimpeded. <br>
<p>
The barrier itself can be implemented using a (queued) cache flush operation on devices that do not support barriers, a simple write barrier operation on devices that support a single I/O thread, and an I/O thread specific barrier operation on devices that support multiple I/O threads.<br>
<p>
A typical journaled filesystem would normally have a minimum of two I/O threads per mounted filesystem or filesystem group - one for metadata operations and one for data operations. The idea of course is to allow a write barrier operation on the metadata I/O thread to hold up only future metadata writes  without affecting outstanding data writes (i.e. writeback) on the data I/O thread.<br>
<p>
If something like this is not done, every metadata sync operation (fsync for example) will necessarily require a complete flush of volatile device write caches for the pertinent devices, which is not exactly ideal if there is a considerable amount of data that doesn't really need to be flushed.<br>
<p>
The SCSI architecture model allows parallel I/O threads on a _single_ device to be implemented using what they call "task sets", and it would be unfortunate if there were not a way for filesystems to take advantage of this capability, given the potential performance gains possible for any application that issues fsync or fdatasync operations in the presence of significant I/O contention from other threads processes. <br>
<p>
In fact it would be ideal to dynamically allocate I/O threads to files on which fdatasync operations are regularly issued, so that the underlying device can write just those blocks to disk (or non-volatile cache) without the need to write anything else (assuming the size of the file/inode has not changed).<br>
<p>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399385/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399473"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 2:28 UTC (Wed)
                               by <b>koverstreet</b> (subscriber, #4296)
                              [<a href="/Articles/399473/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yeah, the idea I sketched out is roughly equivalent (in behavior) to doing it with threads. I should make it explicit that my idea doesn't do anything filesystems can't do themselves.<br>
<p>
Threads are unwieldy when you want to express something more complicated than linear dependencies. Like you suggested, if you're just segregating metadata and data that's fine, but the actual dependencies are in practice usually more complex, so - provided you have an easy way of expressing them - it could in theory be a performance gain.<br>
<p>
Anyway, if you want to pipeline ios all the way down to the SCSI layer you need a way of expressing dependencies to the block layer, which needs more than threads.<br>
<p>
I might have to write an actual patch and see what people think...<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399473/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399755"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 18:21 UTC (Thu)
                               by <b>butlerm</b> (subscriber, #13312)
                              [<a href="/Articles/399755/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The advantage of threads is that they are simple, are already implemented by many existing block devices (SCSI ones at any rate), and allow the optimization of many common cases - journal write before metadata write without a round trip (or worse a cache flush) in between, for example.<br>
<p>
They also make it very convenient for a filesystem to gain notification when a series of block writes have been committed to disk without being too involved with the low level details of how that is known to be the case.<br>
<p>
On some devices any write barrier is most efficiently translated into a full cache flush, on others completion of a series of writes with force unit access specified. If the block interface does not provide I/O threads with write barriers or the equivalent, presumably a filesystem would be forced to choose one or the other, which would be highly inefficient in a number of cases.<br>
<p>
With the proper threaded interface, the lower level device driver can choose how to implement the write barrier most efficiently.  SATA devices (which seem to be unusually backward in this regard) probably need a full cache flush.  Other devices you can either issue an explicit barrier, or you can efficiently wait for a series of force unit access writes to individually complete.  The filesystem shouldn't have to care about what is most efficient for any given device.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399755/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor400906"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 19, 2010 13:13 UTC (Thu)
                               by <b>cypherpunks</b> (guest, #1288)
                              [<a href="/Articles/400906/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I've had a similar idea, which is specifically designed for easy hardware implementation: allow an operation to have a (small integer) tag, then provide a command to "wait for all operations with tag #k to complete".<br>
<p>
More generally, you could let every operation have a prerequisite tag that must be completed (you need one reserved tag number which is never used to specify commands with no prerequisites), and have the wait operation be a NOP with a prerequisite.<br>
<p>
To merge threads, the wait operation can have a tag #n which differs from the #k it is waiting for.  After it is issued, waiting for #n effectively waits for both.<br>
<p>
Now, you can merge independent operation streams by doing address translation on tags.  And you can compress tag space (down to simple barriers, in the limiting case) by allowing false sharing.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/400906/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor399405"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 17:57 UTC (Tue)
                               by <b>butlerm</b> (subscriber, #13312)
                              [<a href="/Articles/399405/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Correction: That should really be SCSI "I_T_L_Q nexus" or execution queue per I/O thread, not a SCSI "task set".<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399405/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399788"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 19:19 UTC (Thu)
                               by <b>butlerm</b> (subscriber, #13312)
                              [<a href="/Articles/399788/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
It turns out SCSI only supports ordering within the context of an initiator target (I_T) nexus, which means any given initiator usually only gets one I/O thread per device.  The way around that limitation is to use to establish separate connections (I_T nexuses) for each I/O thread, but that probably isn't practical in most cases, even on something like iSCSI.<br>
<p>
That is not to say that the SCSI folks shouldn't add real I/O thread support, because a write barrier at the device level (for all practical purposes) is not much more useful than a full cache flush.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399788/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor436979"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2011 14:55 UTC (Tue)
                               by <b>bredelings</b> (subscriber, #53082)
                              [<a href="/Articles/436979/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;Introducing an artificial global ordering on all the ios you see is a pain</font><br>
<font class="QuotedText">&gt;in the ass...</font><br>
Sure, what we want is a *partial* ordering, right?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/436979/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor399260"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 20:11 UTC (Mon)
                               by <b>masoncl</b> (subscriber, #47138)
                              [<a href="/Articles/399260/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt;&gt; barriers, as such, will be no more.</font><br>
<p>
<font class="QuotedText">&gt; Hurray!!!!! I've never liked barriers, as such.</font><br>
<p>
Grin, there seems to be a large party dancing around the grave of the ordered barriers code. <br>
<p>
But to clarify for the comments below, we do still want to issue cache flushes, the filesystems just promise to use wait_on_buffer/page for everything we care about first. <br>
<p>
Most of the time these waits are already there...reiserfs is the biggest exception but that is very easily fixed since it is inside a big if (ordered_barriers) statement.<br>
<p>
-chris<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399260/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399155"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 5:20 UTC (Mon)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/399155/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; but there does not seem to be a better method on offer at the moment. </font><br>
<p>
As far as I can see, the main reason for setting dirty_ratio below about 50% is to limit that time it takes for "sync" to complete (and fsync on ext3 data=ordered filesystems)  (as you go above 50% direct reclaim will trigger significantly more often and slow down memory allocation a lot).<br>
<p>
So the tunable should be "how long is sync allowed to take".  Then you need an estimate of the throughput of each bdi, and don't allow any bdi to gather more dirty memory than that estimate multiplied by the tunable.<br>
<p>
Of course this is much more easily said than done - getting a credible estimate in an efficient manner is non-trivial.  You can only really measure throughput during intensive write-out, and that probably happens mostly once dirty_ratio is reached, which is a bit late to be setting dirty_ratio.<br>
<p>
I suspect some adaptive thing could be done - the first sync might be too slow, but long term it would sort it self out.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399155/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399173"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 8:24 UTC (Mon)
                               by <b>koverstreet</b> (subscriber, #4296)
                              [<a href="/Articles/399173/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
My thought when I read that is that what we really want is some statistics on dirty data - probably index it by length of sequential area (and track the average sequential size), and some heuristics for how likely dirty data is to be redirtied, if someone can come up with useful ones.<br>
<p>
The idea being that if you're say, copying iso files there's no point in queuing up a gigabyte's worth - but bdb doing random io should be allowed to use more memory. Especially if you maintained those statistics per process, you'd be in good shape to do that.<br>
<p>
Having never looked at the writeback code I've no idea what it does already, but it seems to me once you're keeping track of sequential chunks of dirty data it seems to me it'd be a great idea to write them out roughly in order of sequential size - writing out the isos you're copying before your berkeley db.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399173/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399181"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 8:56 UTC (Mon)
                               by <b>james_</b> (guest, #55070)
                              [<a href="/Articles/399181/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
As an example of a case where setting the ratio low is advantageous:<br>
<p>
We were testing a NAS system recently. Our tests use 54 systems writing to the NAS server. The default value of  /proc/sys/vm/dirty_ratio was 40. We saw very bad performance when we applied a large write to the system. The vendors technical support noted that the writes where going to the NAS out of order and that because we had a large number of writes we where defeating the NASs cache forcing the out of order writes to become a read modify write cycle. By dropping the value to for example 2 we saw the NAS system perform.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399181/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399185"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 9:31 UTC (Mon)
                               by <b>neilbrown</b> (subscriber, #359)
                              [<a href="/Articles/399185/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I completely agree - the case where I have seen the need for a low ratio was also when the writes were going out via NFS.  The machine had, I think, 32Gig of RAM, so 40% was a lot.  Even 1%, the smallest non-zero setting, took longer to flush than we really wanted.<br>
<p>
Problems with out-of-order writes is an interesting twist on that!<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399185/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor400341"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 15, 2010 17:52 UTC (Sun)
                               by <b>kleptog</b> (subscriber, #1183)
                              [<a href="/Articles/400341/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I have a another case where the default settings don't work well. A process that is somewhat realtime produces 30MB/s of data writing to disk. Under the default settings the kernel will wait 30 seconds before writing anything (900MB) and if it doesn't get it out in a reasonable way that the process gets stuck because it used up all of the 20% of memory for its data.<br>
<p>
The solution is to have the kernel check much more often the amount of data waiting (every second rather than every 5 seconds) and drastically reduce the amount of dirty memory there's allowed to be before write back happens.<br>
<p>
Without this the kernel suddenly realises it has more than a gigabyte of data to writeback (20% of 8GB = 1.6GB) and manages to starve other processes trying to get it out. Whereas if it just writebacks small amounts in the background continuously everything goes smoothly. 1% works well, since that's what the storage subsystem can handle quickly.<br>
<p>
Pity it's a global setting though, other processes would probably work better with a higher writeback threshold, but you can't pick and choose.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/400341/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor400905"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 19, 2010 13:04 UTC (Thu)
                               by <b>cypherpunks</b> (guest, #1288)
                              [<a href="/Articles/400905/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
I can't help thinking that the solution looks a lot like a PID controller.  That is, page write speed is defined by the sum of three terms: one proportional to the amount of excess dirty pages, one proportional to the integral, and one proportional to the derivative.<br>
<p>
The latter is the "feed-forward" term, and helps respond quickly to sudden changes.  If the rate of page dirtying increases sharply, the rate of writeback should likewise take a sudden jump.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/400905/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
<a name="CommAnchor399193"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 12:16 UTC (Mon)
                               by <b>theraphim</b> (subscriber, #25955)
                              [<a href="/Articles/399193/">Link</a>] (7 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Progress bars neither justify nor require the creation of asynchronous interfaces.</font><br>
<p>
LOL.<br>
Seriously.<br>
How one is supposed to figure out the state of the operation?<br>
Let's say I'm trying to copy 500gig in a single file, somewhere on the network.<br>
Is there any way I can monitor this? Am I making any progress during the past 1.5 hours of silence, or I was cut off the net for past hour?<br>
<p>
Progress bars can be useful, and they are not always "bars".<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399193/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399200"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 12:43 UTC (Mon)
                               by <b>cesarb</b> (subscriber, #6266)
                              [<a href="/Articles/399200/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; Is there any way I can monitor this?</font><br>
<p>
Just stat() the destination?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399200/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399203"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 13:58 UTC (Mon)
                               by <b>theraphim</b> (subscriber, #25955)
                              [<a href="/Articles/399203/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
There are different ways in which copy can be implemented. Especially if it's a copy handled by some remote filesystem.<br>
Not all of them stat-friendly.<br>
The similarly wrong approach would be to use "df" to see if space is being eaten.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399203/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399214"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 14:46 UTC (Mon)
                               by <b>rvfh</b> (guest, #31018)
                              [<a href="/Articles/399214/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Maybe duplicate the file descriptor and monitor the offset?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399214/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399270"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 21:24 UTC (Mon)
                               by <b>theraphim</b> (subscriber, #25955)
                              [<a href="/Articles/399270/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Here you assume you are doing linear copy in a single copying thread. There are cases in which this assumption is untrue, for example, when copying large file from one storage cluster to another you may want to copy chunks in parallel between the chunk servers.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399270/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
<a name="CommAnchor399213"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 14:46 UTC (Mon)
                               by <b>sync</b> (guest, #39669)
                              [<a href="/Articles/399213/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
A good copy would preallocate the destination file. stat() wouldn't work with this (always 100%).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399213/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399316"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 5:16 UTC (Tue)
                               by <b>lkundrak</b> (subscriber, #43452)
                              [<a href="/Articles/399316/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How do you monitor progress of, say, fsync()? I mean, how many system calls were designed with progress monitoring in mind?<br>
<p>
I'm wondering why a call with less than file granularity was not considered (at least I am not aware of). If you could pick smaller units (multiplies of filesystem blocks?) to clone, you could probably also find more interesting uses apart from being able to reasonably control/monitor progress.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399316/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor400909"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 19, 2010 13:18 UTC (Thu)
                               by <b>renox</b> (guest, #23785)
                              [<a href="/Articles/400909/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
If I understood well, this 'copy' is a 'virtual copy' ie a kind of hardlink with COW semantic so this should be a fast operation so in this case you don't really need a progress bar.<br>
For a real copy, the issue is there though.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/400909/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399219"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 16:10 UTC (Mon)
                               by <b>nix</b> (subscriber, #2304)
                              [<a href="/Articles/399219/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <blockquote>
a Google developer said that the feature is attractive for internal use
</blockquote>
It's attractive for any application that has a heap of >2Mb (for the portions of the heap before the end), which is most of them. It'll probably even speed swapping: swapping in enforced 2Mb chunks is much more suited to modern disks than swapping in tiny little 4K pieces.

      
          <div class="CommentReplyButton">
            <form action="/Articles/399219/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399938"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 13, 2010 11:18 UTC (Fri)
                               by <b>i3839</b> (guest, #31386)
                              [<a href="/Articles/399938/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That's not generally true for SSDs, especially if NCQ is supported. When swapping in latency is usually more important than throughput, so you would want to only swap-in what you need. Maximizing thoughput by reading stuff you don't need is slightly silly.<br>
<p>
With write-out 8 channels * 4 NAND chips per channel * 4KB page size = 128KB should be enough to get max write throughput (less for SLC). Buf if you're swapping, you're generally also swapping in, and writing a lot data will kill read latency, so you might want to limit the writes anyway.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399938/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399241"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Typo ?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 18:12 UTC (Mon)
                               by <b>abacus</b> (guest, #49001)
                              [<a href="/Articles/399241/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <em>There has also been some work done using hdparm to corrupt individual sectors in memory [ ... ]</em><br><br>

Shouldn't that read "on disk" instead of "in memory" ?
      
          <div class="CommentReplyButton">
            <form action="/Articles/399241/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor399243"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Typo ?</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 18:21 UTC (Mon)
                               by <b>rvfh</b> (guest, #31018)
                              [<a href="/Articles/399243/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Please send typos to lwn@lwn.net :-)<br>
<p>
For more info: <a href="http://lwn.net/op/FAQ.lwn#contact">http://lwn.net/op/FAQ.lwn#contact</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399243/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399258"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 9, 2010 20:07 UTC (Mon)
                               by <b>alonz</b> (subscriber, #815)
                              [<a href="/Articles/399258/">Link</a>] 
      </p>
      
      </div>
      </summary>
      WRT the freeze issue: isn't it easier to simply use the fd that issued the ioctl(FIFREEZE) as the &ldquo;controlling&rdquo; one?
<p>
That is&mdash;if this fd is closed, automatically thaw the filesystem?
<p>
This may require some magic somewhere in the VFS, but provides Al's new semantics without changing existing code&hellip;
      
          <div class="CommentReplyButton">
            <form action="/Articles/399258/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399371"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 10, 2010 14:59 UTC (Tue)
                               by <b>stevef</b> (guest, #7712)
                              [<a href="/Articles/399371/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Resurrecting the copyfile id sounds great.  CIFS has a copyfile equivalent on the wire.  SMB2 has a "copy chunk" request (copy all or part of a file by range on the server).<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399371/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor401997"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 25, 2010 21:38 UTC (Wed)
                               by <b>joib</b> (subscriber, #8541)
                              [<a href="/Articles/401997/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
FWIW, the NFS people are also planning to add something similar to the NFS protocol. See <a rel="nofollow" href="http://tools.ietf.org/html/draft-lentini-nfsv4-server-side-copy-04">http://tools.ietf.org/html/draft-lentini-nfsv4-server-sid...</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/401997/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
<a name="CommAnchor399598"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 11, 2010 16:53 UTC (Wed)
                               by <b>ebiederm</b> (subscriber, #35028)
                              [<a href="/Articles/399598/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Did no one talk about fixing our I/O path on swap?<br>
<p>
Not long ago I had an emacs session go crazy pushing my laptop deep into<br>
swap.  I logged in via ssh and killed the emacs free up about 4G and removing the need for the system to be in swap at all.<br>
<p>
Running swapoff -a I only achieved about 1MB/s instead of the 15-30MB/s<br>
that the disk should have been able to achieve.<br>
<p>
Grumble.  All of the fancy algorithms in the world don't help unless we get the fundamentals right.<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399598/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor399712"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Aug 12, 2010 12:48 UTC (Thu)
                               by <b>jlayton</b> (subscriber, #31672)
                              [<a href="/Articles/399712/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
<font class="QuotedText">&gt; The final lightning talk came from Boaz Harrosh, who talked about "stable</font><br>
<font class="QuotedText">&gt; pages." Currently, pages which are currently under writeback can be</font><br>
<font class="QuotedText">&gt; modified by filesystem code. That's potentially a data integrity problem,</font><br>
<font class="QuotedText">&gt; and it can be fatal in situations where, for example, checksums of page</font><br>
<font class="QuotedText">&gt; contents are being made.</font><br>
<p>
This would be a boon for network filesystems too. Right now, it's rather nasty to deal with things like signing in NFS and CIFS. We have to take a checksum of the packet contents, but by the time you do that the page contents can change.<br>
<p>
This is especially a problem with CIFS as the server will often drop the connection if the packet integrity seems to be compromised, and dropped connections with CIFS can mean the loss of a lot of state (open files, locks, etc).<br>
<p>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/399712/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor437048"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">The 2010 Linux Storage and Filesystem Summit, day 1</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Apr 5, 2011 23:13 UTC (Tue)
                               by <b>butlerm</b> (subscriber, #13312)
                              [<a href="/Articles/437048/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
This sounds like a good idea, but suppose a file had several pages that were mapped into some processes' memory with PROT_WRITE access.  Wouldn't this require the filesystem to do the following?:<br>
<p>
(1) temporarily mark those pages as write only while writeback was in progress<br>
(2) catch any page faults during that time<br>
(3) either duplicate the page using a copy on write strategy or hold the faulting process until writeback on that page was complete?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/437048/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2010, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
