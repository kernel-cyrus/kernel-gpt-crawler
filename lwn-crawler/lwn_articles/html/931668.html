        <!DOCTYPE html>
        <html lang="en">
        <head><title>Peer-to-peer DMA [LWN.net]</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
<meta HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="robots" CONTENT="noai, noimageai">
        <link rel="icon" href="https://static.lwn.net/images/favicon.png"
              type="image/png">
        <link rel="alternate" type="application/rss+xml" title="LWN.net headlines" href="https://lwn.net/headlines/rss">
<link rel="alternate" type="application/rss+xml" title="Comments posted to this article" href="https://lwn.net/headlines/931668/">
        <link rel="stylesheet" href="/CSS/lwn">
<link rel="stylesheet" href="/CSS/nosub">

        
<script type="text/javascript">var p="http",d="static";if(document.location.protocol=="https:"){p+="s";d="engine";}var z=document.createElement("script");z.type="text/javascript";z.async=true;z.src=p+"://"+d+".adzerk.net/ados.js";var s=document.getElementsByTagName("script")[0];s.parentNode.insertBefore(z,s);</script>
<script type="text/javascript">
var ados_keywords = ados_keywords || [];
if( location.protocol=='https:' ) {
        ados_keywords.push('T:SSL');
} else {
        ados_keywords.push('T:HTTP');
}

var ados = ados || {};
ados.run = ados.run || [];
ados.run.push(function() {

ados_add_placement(4669, 20979, "azk13321_leaderboard", 4).setZone(16026);

ados_add_placement(4669, 20979, "azk93271_right_zone", [5,10,6]).setZone(16027);

ados_add_placement(4669, 20979, "azk31017_tracking", 20).setZone(20995);



ados_setKeywords(ados_keywords.join(', ')); 
ados_load();
});</script>

        </head>
        <body>
        <a name="t"></a>
<div id="menu"><a href="/"><img src="https://static.lwn.net/images/logo/barepenguin-70.png" class="logo"
                 border="0" alt="LWN.net Logo">
           <span class="logo">LWN<br>.net</span>
           <span class="logobl">News from the source</span></a>
           <a href="/"><img src="https://static.lwn.net/images/lcorner-ss.png" class="sslogo"
                 border="0" alt="LWN"></a><div class="navmenu-container">
           <ul class="navmenu">
        <li><a class="navmenu" href="#t"><b>Content</b></a><ul><li><a href="/current/">Weekly Edition</a></li><li><a href="/Archives/">Archives</a></li><li><a href="/Search/">Search</a></li><li><a href="/Kernel/">Kernel</a></li><li><a href="/Security/">Security</a></li><li><a href="/Calendar/">Events calendar</a></li><li><a href="/Comments/unread">Unread comments</a></li><li><hr></li><li><a href="/op/FAQ.lwn">LWN FAQ</a></li><li><a href="/op/AuthorGuide.lwn">Write for us</a></li></ul></li>
<li><a class="navmenu" href="#t"><b>Edition</b></a><ul><li><a href="/Articles/931576/">Return to the Front page</a></li></ul></li>
</ul></div>
</div> <!-- menu -->
<div class="not-handset"
            	     style="margin-left: 10.5em; display: block;">
                   <div class="not-print"> <div id="azk13321_leaderboard"></div> </div>
                </div>
            <div class="topnav-container">
<div class="not-handset"><form action="https://lwn.net/Login/" method="post" name="loginform"
                 class="loginform">
        <label><b>User:</b> <input type="text" name="uname" value="" size="8" id="uc" /></label> 
		<label><b>Password:</b> <input type="password" name="pword" size="8" id="pc" /></label> <input type="hidden" name="target" value="/Articles/931668/" /> <input type="submit" name="submit" value="Log in" /></form> |
           <form action="https://lwn.net/subscribe/" method="post" class="loginform">
           <input type="submit" name="submit" value="Subscribe" />
           </form> |
           <form action="https://lwn.net/Login/newaccount" method="post" class="loginform">
           <input type="submit" name="submit" value="Register" />
           </form>
        </div>
               <div class="handset-only">
               <a href="/subscribe/"><b>Subscribe</b></a> /
               <a href="/Login/"><b>Log in</b></a> /
               <a href="/Login/newaccount"><b>New account</b></a>
               </div>
               </div><div class="maincolumn flexcol">
<div class="middlecolumn">
<div class="PageHeadline">
<h1>Peer-to-peer DMA</h1>
</div>
<div class="ArticleText">
<blockquote class="ad">
<b>Please consider subscribing to LWN</b>
<p>
Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit
<a href="/Promo/nst-nag1/subscribe">this page</a> to join up and keep LWN on
the net.
</blockquote>
<div class="FeatureByline">
           By <b>Jake Edge</b><br>May 16, 2023</br>
           <hr>
<a href="/Articles/lsfmmbpf2023">LSFMM+BPF</a>
</div>
<p>
In a plenary session on the first day of the <a
href="/Articles/lsfmmbpf2023">2023 Linux Storage, Filesystem,
Memory-Management and BPF Summit</a>, 
Stephen Bates led a discussion about <a
href="/Articles/767281/">peer-to-peer DMA</a> (P2PDMA).  The idea is to
remove the host system's participation in a transfer of data from one
PCIe-connected device to another.  The feature was originally aimed at NVMe
SSDs so that data could simply be copied directly to and from the storage
device without needing to move it to system memory and then from
there to somewhere else.
</p>

<h4>Background</h4>

<p>
The idea goes back to 2012 or so, Bates said, when he and Logan Gunthorpe
(who did "most of the real work") 
were working on NVMe SSDs, RDMA, and NVMe over fabrics (before it was a
standard, 
he thought).  Some customers suggested that being able to DMA directly between
devices would be useful.  With devices that exposed some memory (which would
be called a "controller memory buffer" or CMB today) they got the precursor
to P2PDMA working.  There are some user-space implementations of the
feature, including for <a href="https://spdk.io/">SPDK</a> and NVIDIA's <a
href="https://developer.nvidia.com/blog/gpudirect-storage/">GPUDirect
Storage</a>, which allows copies directly between NVMe namespaces and GPUs.
</p>

<a href="/Articles/931947/">
<img src="https://static.lwn.net/images/2023/lsfmb-bates-sm.png" border=0 hspace=5 align="right"
alt="[Stephen Bates]" title="Stephen Bates" width=250 height=300>
</a>

<p>
Traditional DMA has some downsides when moving data between two PCIe
devices, such as an NVMe SSD and an RDMA network card.  All of the DMA
operations come into the system memory from one of the devices, then have
to be copied out of the 
system memory to the other device, which doubles the amount of
memory-channel bandwidth required. If user-space applications are also
trying to access the RAM on the same physical DIMM as the DMA operation,
there can 
be various quality-of-service problems as well.
</p>

<p>
P2PDMA avoids those problems, but comes with a number of challenges, he
said.  The original P2PDMA implementation for Linux was in-kernel-only;
there were some hacks that allowed access from user space, but they were
never merged into the mainline.  More recently, though, the&nbsp;6.2 kernel
has support for user-space access to P2PDMA, at least in some
circumstances.  P2PDMA is available in the NVMe driver but only devices
that have a CMB can be a DMA source or destination. NVMe devices are the
only systems currently supported as DMA masters as well. 
</p>

<p>
Bates is unsure whether Arm64 is a fully supported architecture currently, as
there is "some weird problem" that Gunthorpe is working through, but x86 is
fully supported. An IOMMU
plays a big role for P2PDMA because it needs to translate physical and
virtual addresses of 
various sorts between the different systems; "believe me, DMAing to the
wrong place is never a good thing".  The IOMMU can also play a safeguard
role to ensure that errant DMA operations are not actually performed.
</p>

<p>
Currently, there is work on allowlists and blocklists for devices that do
and do not work
correctly, but the situation is generally improving.  Perhaps because of
the GPUDirect efforts, support for P2PDMA in CPUs and PCIe devices seems to
be getting better.  He pointed to his <a
href="https://github.com/sbates130272/p2pmem-test">p2pmem-test
repository</a> for the user-space component that can be used to test the
feature in a virtual machine (VM).  As far as he knows, no other PCIe drivers
beyond the NVMe driver implement P2PDMA, at least so far.
</p>

<h4>Future</h4>

<p>
Most NVMe drivers are block devices that are
accessed via logical block addresses (LBAs), but there are devices with
object-storage capabilities as well.  There is also a computational storage
interface coming soon (which was the topic of the next session) for doing
computation (e.g. compression) on data that is present on a device.  NVMe
namespaces for byte-addressable storage are coming as well; those are not
"load-store interfaces", which would be accessible from the CPU via load
and store instructions as with RAM, but are instead storage interfaces
available at byte granularity. 
Supporting P2PDMA for the NVMe persistent memory region (PMR), which is 
load-store accessible and backed by some kind of persistent data (e.g. battery
backed-up RAM), is a
possibility on the horizon, though he has not heard of any NVMe PMR drives
in development.
PMR devices
could perhaps overlap the use cases of <a
href="https://www.computeexpresslink.org/">CXL</a>, he said.
</p>

<p>
Better VM and IOMMU support is in the works.  PCIe has various mechanisms
for handling and caching memory-address translations, which could be used
to improve P2PDMA.  Adding more features to QEMU (e.g. <a
href="https://en.wikipedia.org/wiki/Single-root_input/output_virtualization">SR-IOV</a>)
is important because it is difficult to debug problems using real
hardware.  Architecture support is also important; there may still be
problems with Arm64 support, but there are other important architectures, like
RISC-V, that need to have P2PDMA support added.
</p>

<p>
CXL had been <a href="/Articles/931282/">prominently featured in the
previous session</a>, so Bates said he wanted to dig into it a bit.  P2PDMA
came about in a world where CXL did not exist, but now that it does, he
thinks there are an interesting set of use cases for P2PDMA in a CXL
world.  Electrically and physically, CXL is the same as PCIe, which means
that both types of devices can plug into the same bus slots.  They are
different at the data link layer, but work has been done on <a
href="https://en.wikipedia.org/wiki/Compute_Express_Link#Protocols">CXL.io</a>,
which translates PCIe to CXL.
</p>

<p>
That means that an NVMe drive that has support for CXL <a
href="https://en.wikipedia.org/wiki/Flit_(computer_networking)">flow-control
units</a> (flits) can be plugged into a CXL port and can then be used as
a storage device via the NVMe driver on the host.  He and a colleague had
modeled that using QEMU the previous week, which may be the first
time it had ever been done.  He believes it worked but more testing is needed.
</p>

<p>
Prior to CXL&nbsp;3.0, doing P2PDMA directly between CXL memory and an NVMe
SSD was 
not really possible because of cache-coherency issues.  CXL&nbsp;3.0 added
a way for CXL to tell the CPUs that it was about to do DMA for a particular
region of physical memory and ask the CPUs to update the CXL memory
from their caches.  The unordered I/O (UIO) feature added that ability, which
can be used to move large chunks of data from or to storage devices at
hardware speeds without affecting the CPU or its memory interconnects.
Instead of a storage device, an NVMe network device could be used to move
data directly out of CXL memory to the network.
</p>

<p>
Bates said that peer-to-peer transfers of this sort are becoming more and
more popular, though many people are not using P2PDMA to accomplish them.
That popularity will likely translate to more users of P2PDMA over time,
however.  At that point, LSFMM+BPF organizer Josef Bacik pointed out that
time had expired on the slot, so the memory-management folks needed to head
off to <a href="/Articles/931406/">their next session</a>, while the
storage and filesystem developers continued the discussion.
</p>

<p>
David Howells asked if Bates had spoken with graphics developers about P2PDMA
since it seems like they might be interested in using it to move, say,
textures from storage to a GPU.  Bates said that
he had been focusing on cloud and enterprise kinds of use cases, so he had
not contacted graphics developers.  The large
AI clusters are using peer-to-peer transfers to GPUs, though typically
via the 
GPUDirect mechanism.
</p>

<p>
The NVMe community has been defining new types of namespaces lately, Bates
said.  The LBA namespace is currently used 99% of the time, but there are
others coming as he had noted earlier.  All of those namespace types and
command sets can be used over both PCIe and CXL, but they can also be used
over fabrics with RDMA or TCP/IP.  Something that is not yet
in the standard, but he hopes is coming, is providing a way to present an
NVMe namespace (or a sub-region of it) as a byte-addressable, load-store
region that P2PDMA can then take advantage of.
</p>

<p>
There was a digression on what was meant by load-store versus DMA for
these kinds of operations.  Bates said that for accessing data on a device,
DMA means that some kind of
descriptor is sent to a data-mover that would simply move the
data as specified, whereas load-store means that a CPU is involved in doing
a series of
load and store operations.  So there would be a new NVMe command requesting
that a region be exposed as a CMB, a PMR, or "something new that we haven't
invented yet"; the CPU (or some other device, such as a DMA data-mover) can
then do load-store accesses on the region. 
</p>

<p>
One use case that he described would be having an extremely hot
(i.e. frequently accessed) huge file on
an NVMe drive, but wanting to be able to access it directly with loads and
stores. 
A few simple NVMe commands could prepare this data to be byte-accessible,
which could then be mapped into the application's address space using <a
href="https://man7.org/linux/man-pages/man2/mmap.2.html"><tt>mmap()</tt></a>;
it would be like having the file in memory without the possibility of page
faults when accessing it.
</p><br clear="all"><table class="IndexEntries">
           <tr><th colspan=2>Index entries for this article</th></tr>
           <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#Compute_Express_Link_CXL">Compute Express Link (CXL)</a></td></tr>
            <tr><td><a href="/Kernel/Index">Kernel</a></td><td><a href="/Kernel/Index#NVMe">NVMe</a></td></tr>
            <tr><td><a href="/Archives/ConferenceIndex/">Conference</a></td><td><a href="/Archives/ConferenceIndex/#Storage_Filesystem_Memory-Management_and_BPF_Summit-2023">Storage, Filesystem, Memory-Management and BPF Summit/2023</a></td></tr>
            </table><br clear="all">
<hr width="60%%" align="left">
            <form action="/Login/" method="post">
            <input type="hidden" name="target" value="/Articles/931668/" />
            <input type="submit" name="login" value="Log in" /> to post comments
            <p>
        
</div> <!-- ArticleText -->
<p><a name="Comments"></a>
<a name="CommAnchor932064"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 16, 2023 15:38 UTC (Tue)
                               by <b>sbates</b> (subscriber, #106518)
                              [<a href="/Articles/932064/">Link</a>] (6 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Thanks for the write up! I just wanted to clarify one important point in the first paragraph. "The idea is to remove the host system's participation in a transfer of data" is not technically correct. The host driver is still issuing the DMA requests to the PCIe device(s) and so it is still participating. The main difference now is that this DMA traffic ideally goes directly from one PCIe device to the other without needing to be "bounced" through system memory.<br>
<p>
Stephen<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932064/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932069"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 16, 2023 19:06 UTC (Tue)
                               by <b>MattBBaker</b> (guest, #28651)
                              [<a href="/Articles/932069/">Link</a>] (5 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
Yep, someone has to drive the queues. I keep hoping that we see a return of something like the Cell CPU where there was a high clock core that could be the designated DMA driver.<br>
<p>
The biggest problem the larger community will suffer making these neat toys work is going to be that most programming models assume a 'sender/receiver' model where the local worker is either the sender or the receiver, with no provision for the running thread being 'neither'.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932069/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932084"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2023 10:54 UTC (Wed)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/932084/">Link</a>] (4 responses)
      </p>
      
      </div>
      </summary>
      <p>Why would you need a high clock core to drive DMA? My understanding is that you'd be getting the NVMe device (or the NIC, or the GPU) to do the actual DMA transfers, and the host's involvement is limited to sending DMA descriptors to the device doing the transfer.
<p>For NVMe to GPU transfers on a modern system with large BARs, the host is effectively putting one descriptor into the queue for each transfer - the GPU exposes all its VRAM to the NVMe device, and an NVMe scatter-gather list can transfer an entire GPU's worth of data as a result of a single SGL programmed into the NVMe queue by the host. One interrupt per several gigabytes of data transfer doesn't need a high clock rate core dedicated to it.


      
          <div class="CommentReplyButton">
            <form action="/Articles/932084/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932208"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2023 20:21 UTC (Wed)
                               by <b>MattBBaker</b> (guest, #28651)
                              [<a href="/Articles/932208/">Link</a>] (3 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
That works fine if your system is just one card that needs to be fed from one source. When you're pushing 6 cards on one system with a bunch of NVMe drives and multiple high performance NICs, suddenly the speed at which queues can be serviced both to issue new commands and drive completions matters a lot<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932208/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932211"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2023 20:56 UTC (Wed)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/932211/">Link</a>] (2 responses)
      </p>
      
      </div>
      </summary>
      <p>NVMe (and most DMA-capable devices are similar in this respect) already has command queues for this sort of purpose - I can put together a long queue (65,536 commands in the case of NVMe, each of which has its own SGL) of commands that do all the data transfers I want to do with that drive, and let it get on with the transfers. And NVMe allows for 65,535 queues per device, so I can have a lot of queued transfers.
<p>Even with a few hundred drives to push, the limiting factor is the processing that has to be done after the device completes the DMA, and the Cell model of a high clock speed, low performance, low latency DMA driving CPU isn't helpful for that.


      
          <div class="CommentReplyButton">
            <form action="/Articles/932211/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932894"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 24, 2023 20:54 UTC (Wed)
                               by <b>flussence</b> (guest, #85566)
                              [<a href="/Articles/932894/">Link</a>] (1 responses)
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
While those numbers are impressive-looking (compared to SATA), those are only theoretical upper limits in the protocol. No NVMe drive is going to have 16GB of onboard DRAM just for command buffers.<br>
<p>
I'm not entirely sure the correct way to look up actual limits but I can make an educated guess: `nvme id-ctrl /dev/nvme0` gives me "sqes: 0x66, cqes: 0x44" which sounds more realistic.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932894/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    <a name="CommAnchor932953"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 25, 2023 11:10 UTC (Thu)
                               by <b>farnz</b> (subscriber, #17727)
                              [<a href="/Articles/932953/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <p>You're looking at the wrong number there - SQES tells you the smallest and largest sizes permitted for an SQE, and is two 4 bit fields for maximum and minimum, as log<sub>2</sub> of the entry size. Thus 0x66 is an entry exactly 64 bytes in size. You'd need to access the MQES register to find out how many queue entries the device supports, and you can split those between as many queues are you need. Practically, one queue per CPU is common.
<p>And if you don't like reasoning from the upper limits, there's a different direction; a high end NVMe device currently does between 1 and 2 million IOPS. To trigger a single IOP, you need to write a 64 byte queue entry, followed by a write to a doorbell to tell the NVMe device that there's a new entry to look at; note that queues do not have to be on the drive, they can be in host memory. A 64 bit CPU should be able to write at least 8 bytes per clock cycle, and read 8 bytes in the same clock cycle, unless limited by I/O or memory speeds, so 9 clock cycles is enough to trigger a fresh I/O - and because a completion entry is 32 bytes, you can detect completion overlapped with writing the next I/O entry . So, given an NVMe device that's more than an order of magnitude better than today's best, offering 100M IOPS, or more realistically, 50 of today's best devices,  you need a 900 MHz CPU to keep the NVMe device saturated regardless of I/O size.
<p>In practice, you're going to be doing larger I/Os if you can, which reduces the needed clock speed further. On the other hand, you're not simply submitting I/O all the time - you're also doing other work, which increases the needed performance (but notably <em>not</em> the clock speed - a 1 GHz CPU doing 5 IPC is the same as a 5 GHz CPU doing 1 IPC for this analysis).


      
          <div class="CommentReplyButton">
            <form action="/Articles/932953/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</details>
</details>
</details>
</details>
</details>
</details>
<a name="CommAnchor932156"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 17, 2023 13:28 UTC (Wed)
                               by <b>bgoglin</b> (subscriber, #7800)
                              [<a href="/Articles/932156/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The idea goes back to way before 2012. This paper from 2001 implemented the idea between SCSI disks and Myrinet NICs <a href="https://ieeexplore.ieee.org/document/923202">https://ieeexplore.ieee.org/document/923202</a><br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932156/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor932240"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA synchronization</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted May 18, 2023 3:00 UTC (Thu)
                               by <b>DemiMarie</b> (subscriber, #164188)
                              [<a href="/Articles/932240/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
How is synchronization and flow control handled?  What happens if e.g. a NIC asks an NVMe device for data that isnâ€™t ready yet?<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/932240/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
<a name="CommAnchor934472"></a>
    <details class="CommentBox" open>
      <summary><h3 class="CommentTitle">Peer-to-peer DMA</h3>
      <div class="AnnLine">
      <p class="CommentPoster"> Posted Jun 13, 2023 7:09 UTC (Tue)
                               by <b>daenzer</b> (subscriber, #7050)
                              [<a href="/Articles/934472/">Link</a>] 
      </p>
      
      </div>
      </summary>
      <div class="FormattedComment">
The amdgpu/amdkfd driver has supported PCIe P2PDMA between AMD GPUs since 6.0.<br>
</div>

      
          <div class="CommentReplyButton">
            <form action="/Articles/934472/comment" method="post">
            <input type="submit" value="Reply to this comment">
            </form>
          </div>
        
     <p>
     
    </details>
</div> <!-- middlecolumn -->
<div class="rightcol not-print">
<div id="azk93271_right_zone"></div>
</div>
</div> <!-- maincolumn -->

            <br clear="all">
            <center>
            <P>
            <span class="ReallySmall">
            Copyright &copy; 2023, Eklektix, Inc.<BR>
            This article may be redistributed under the terms of the
              <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons CC BY-SA 4.0</a> license<br>
            Comments and public postings are copyrighted by their creators.<br>
            Linux  is a registered trademark of Linus Torvalds<br>
            </span>
            </center>
            
            </body></html>
