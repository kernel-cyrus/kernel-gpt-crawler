# Finer-grained kernel address-space layout randomization [LWN.net]

> **Benefits for LWN subscribers**
> 
> The primary benefit from [subscribing to LWN](/Promo/nst-nag5/subscribe) is helping to keep us publishing, but, beyond that, subscribers get immediate access to all site content and access to a number of extra site features. Please sign up today! 

By **Jake Edge**  
February 19, 2020 

The idea behind [kernel address-space layout randomization](/Articles/569635/) (KASLR) is to make it harder for attackers to find code and data of interest to use in their attacks by loading the kernel at a random location. But a single random offset is used for the placement of the kernel text, which presents a weakness: if the offset can be determined for anything within the kernel, the addresses of other parts of the kernel are readily calculable. A new "finer-grained" KASLR patch set seeks to remedy that weakness for the text section of the kernel by randomly reordering the functions within the kernel code at boot time. 

Kristen Carlson Accardi [posted](/ml/linux-kernel/20200205223950.1212394-1-kristen@linux.intel.com/) an RFC patch set that implemented a proof-of-concept for finer-grained KASLR in early February. She identified three weaknesses of the existing KASLR: 

  * low entropy in the randomness that can be applied to the kernel as a whole 
  * the leak of a single address can reveal the random offset applied to the kernel, thus revealing the rest of the addresses 
  * the kinds of information leaks needed to reveal the offset abound 

So, the "tl;dr" is: ""This patch set rearranges your kernel code at load time on a per-function level granularity, with only around a second added to boot time."" 

The changes required are in two main areas. When the kernel is built, a GCC option is used to place each function in its own `.text` section. The relocation addresses can be used to allow shuffling the text sections as the kernel is loaded, just after it is decompressed. There are, she noted, tables of addresses in the kernel for things like exception handling and kernel probes (kprobes), but those can be handled too: 

Most of these tables generate relocations and require a simple update, and some tables that have assumptions about order require sorting after the update. In order to modify these tables, we preserve a few key symbols from the objcopy symbol stripping process for use after shuffling the text segments. 

The second area of changes is in the loading of the kernel into memory; the boot process was changed to parse the `vmlinux` ELF file to retrieve the key symbols and collect up a list of `.text.*` sections to be reordered. The function order is then randomized and any tables are updated as needed: 

The existing code which updated relocation addresses was modified to account for not just a fixed delta from the load address, but the offset that the function section was moved to. This requires inspection of each address to see if it was impacted by a randomization. We use a bsearch to make this less horrible on [performance]. 

For debugging the proof-of-concept, a pseudo-random-number generator (PRNG) was used so that the same order could be generated by giving it the identical seed. The patch adding the PRNG, which was authored by Kees Cook, might provide some performance benefits, but Andy Lutomirski [objected](/ml/linux-kernel/487F8A1B-3FBC-4A0E-B6EC-5FE0F70D18DD@amacapital.net/) to using a new, unproven algorithm; he suggested using a deterministic random bit generator (DRBG), such as [ChaCha20](https://en.wikipedia.org/wiki/Salsa20#ChaCha_variant). Similarly, Jason A. Donenfeld [was concerned](/ml/linux-kernel/20200206151001.GA280489@zx2c4.com/) that the random-number sequence could be predicted from just a few leaked address values, which might defeat the purpose of the feature. Cook [said](/ml/linux-kernel/202002060345.FAF7517CA4@keescook/) that using ChaCha20 instead was a better idea moving forward. 

The patch set removes access to the `/proc/kallsyms` file, which lists addresses of kernel symbols, for non-root users. Currently `kallsyms` simply gives addresses of all zeroes when non-root users read it, but the list of symbols is given in the order they appear in the kernel text; that would give away the randomized layout of the kernel, so access was disabled. Cook [pointed out](/ml/linux-kernel/202002060428.08B14F1@keescook/) that making the `kallsyms` file unreadable has, in the past, ""seemed to break weird stuff in userspace"". He suggested either sorting the symbol names alphabetically in the output—or perhaps just waiting to see if there were any complaints. 

#### Impacts

Accardi measured the impact on boot time in a VM and found that it took roughly one second longer to boot, which is fairly negligible for many use cases. The run-time performance is harder to characterize; the all-important kernel build benchmark was about 1% slower than building on the same kernel with just KASLR enabled. Some other workloads performed much worse, ""while others stayed the same or were mysteriously better"". It probably is greatly dependent on the code flow for the workload, which might make for an area to research in the future; optimizing the function layout for the workload [has been shown [PDF]](https://research.fb.com/wp-content/uploads/2017/01/cgo2017-hfsort-final1.pdf) to have a positive effect on performance. 

Adding the extra information to the `vmlinux` ELF file to support finer-grained KASLR increases its size, but there is a much bigger effect from the need to increase the boot heap size. Randomizing the addresses of the sections requires a much bigger heap, 64MB, than current boot heaps (64KB for all compressors except bzip2, which needs 4MB). The problem is that a larger boot heap ends up increasing the size of the kernel image by adding a zero-filled section to accommodate the heap. 

One of Cook's [patches](/ml/linux-kernel/20200205223950.1212394-12-kristen@linux.intel.com/), which was included in Accardi's patch set, seeks to remedy that problem, but it turned out that the underlying problem was a bug in how the sections in the kernel object are laid out. Arvind Sankar [pointed to](/ml/linux-kernel/20200206001103.GA220377@rani.riverdale.lan/) his [patch set](/ml/linux-kernel/20200109150218.16544-1-nivedita@alum.mit.edu/) from January that would fix the problem, which Cook [thought](/ml/linux-kernel/202002060251.681292DE63@keescook/) was a much better solution to the problem. 

Lutomirski also [suggested](/ml/linux-kernel/CALCETrVnCAzj0atoE1hLjHgmWjWAKVdSLm-UMtukUwWgr7-N9Q@mail.gmail.com/) that the sort mechanism being used on the symbol names was too expensive; the swap function being used in the `sort()` call did quite a bit of unneeded work if a bit more memory was available: 

Unless you are severely memory-constrained, never do a sort with an expensive swap function like this. Instead allocate an array of indices that starts out as [0, 1, 2, ...]. Sort *that* where the swap function just swaps the indices. Then use the sorted list of indices to permute the actual data. The result is exactly one expensive swap per item instead of one expensive swap per swap. 

Cook [said](/ml/linux-kernel/202002060353.A6A064A@keescook/) that he thought there were a number of areas where the tradeoff of memory versus speed need to be considered. The amount of memory being used by the proof-of-concept is much greater than he expected (58MB in his tests). One of the problems there is that the version of `free()` used when decompressing the kernel image does not actually free any memory. But Accardi [thought](/ml/linux-kernel/c9946c229f6f53379deeef00fbdee88fe2fdd96e.camel@linux.intel.com/) that the boot latency of a second or so was not likely to deter those who are interested in having the protection—boot-time minimalists are not likely to use finer-grained KASLR anyway, she said. 

#### Security and alignment

In the cover letter, Accardi analyzed the security properties of the patch set, noting that information leaks are often considered to require local access to the system, but that [CVE-2019-0688](https://nvd.nist.gov/vuln/detail/CVE-2019-0688) demonstrated a remote address leak for Windows. The patch set assumes that information leaks are plentiful, so it is trying to make it harder for attackers even in the presence of these leaks. Quantifying the added difficulty is dependent on a number of factors: 

Firstly and most obviously, the number of functions you randomize matters. This implementation keeps the existing .text section for code that cannot be randomized - for example, because it was assembly code, or we opted out of randomization for performance reasons. The less sections to randomize, the less entropy. In addition, due to alignment (16 bytes for x86_64), the number of bits in a address that the attacker needs to guess is reduced, as the lower bits are identical. 

She suggested that other alignments could be considered down the road and that execute-only memory (XOM), if it lands, would make the finer-grained technique more effective against certain kinds of attacks. Function sections could perhaps simply be byte-aligned and padded with [`INT3`](https://en.wikipedia.org/wiki/INT_\(x86_instruction\)#INT3) instructions, so that a wrong guess would trigger a trap. But the required alignment of functions on Intel processors is somewhat more complicated. Cook [said](/ml/linux-kernel/202002060408.84005CEFFD@keescook/) that 16-byte function alignment, as it is now in the kernel, is wasting some space (and some entropy in the function start addresses) when using finer-grained KASLR: 

I know x86_64 stack alignment is 16 bytes. I cannot find evidence for what function start alignment should be. It seems the linker is 16 byte aligning these functions, when I think no alignment is needed for function starts, so we're wasting some memory (average 8 bytes per function, at say 50,000 functions, so approaching 512KB) between functions. If we can specify a 1 byte alignment for these orphan sections, that would be nice, as mentioned in the cover letter: we lose a 4 bits of entropy to this alignment, since all randomized function addresses will have their low bits set to zero. 

Jann Horn [pointed out](/ml/linux-kernel/CAG48ez19kRC_5+ykvQCnZxLq6Qg3xUy7fEMf3pYrG46vBZt6jQ@mail.gmail.com/) that Intel recommends 16-byte alignment for branch targets; other alignments might result in less efficient calls. Sankar [noted](/ml/linux-kernel/20200206152949.GA3055637@rani.riverdale.lan/) that the current alignment is not that detrimental to the entropy, but Lutomirski [said](/ml/linux-kernel/B1282A43-1246-4956-917C-72135D9F0328@amacapital.net/) there is another thing to consider: 

There is a security consideration here that has nothing to do with entropy per se. If an attacker locates two functions, they learn the distance between them. This constrains what can fit in the gap. Padding reduces the strength of this type of attack, as would some degree of random padding. 

He also [said](/ml/linux-kernel/B413445A-F1F0-4FB7-AA9F-C5FF4CEFF5F5@amacapital.net/) that there is a bug with some Intel processors that cannot handle certain kinds of jump instructions that span a cache-line boundary. Peter Zijlstra [looked at](/ml/linux-kernel/20200207092423.GC14914@hirez.programming.kicks-ass.net/) the [erratum document [PDF]](https://www.intel.com/content/dam/support/us/en/documents/processors/mitigations-jump-conditional-code-erratum.pdf) and thought it implied a need for 32-byte alignment for functions. Handling that may actually require a change to the kernel overall, Cook [thought](/ml/linux-kernel/202002091742.7B1E6BF19@keescook/). 

The reaction to the idea of finer-grained KASLR was generally positive. No objections to the goals or the techniques used (at a high level) were heard, anyway. It seems like a nice incremental improvement to KASLR. It can also coexist with [various control-flow integrity](/Articles/810077/) (CFI) measures that are working their way upstream. As Accardi noted, the idea is not new and there has been quite a bit of research into it. OpenBSD uses a [similar technique](/Articles/727697/) to randomize its kernel at boot time, for example. There is more work to do, of course, but it would not be a surprise to see finer-grained KASLR in the mainline sometime this year. 

  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Security/Kernel hardening](/Kernel/Index#Security-Kernel_hardening)  
[Security](/Security/Index/)| [Linux kernel](/Security/Index/#Linux_kernel)  
  


* * *

to post comments 
