# The shrinking role of semaphores [LWN.net]

> **Please consider subscribing to LWN**
> 
> Subscriptions are the lifeblood of LWN.net. If you appreciate this content and would like to see more of it, your subscription will help to ensure that LWN continues to thrive. Please visit [this page](/Promo/nst-nag1/subscribe) to join up and keep LWN on the net. 

By **Jonathan Corbet**  
April 7, 2023 

The kernel's handling of concurrency has changed a lot over the years. In 2023, a kernel developer's toolkit includes tools like completions, highly optimized mutexes, and a variety of [lockless algorithms](/Articles/844224/). But, once upon a time, concurrency control came down to the use of simple semaphores; a discussion on a small change to the semaphore API shows just how much the role of semaphores has changed over the course of the kernel's history. 

At its core, a semaphore is an integer counter used to control access to a resource. Code needing access must first decrement the counter — but only if the counter's value is greater than zero; otherwise it must wait for the value to increase. Releasing the semaphore is a matter of incrementing the counter. In the Linux kernel implementation, acquisition of a semaphore happens with a call to [`down()`](https://elixir.bootlin.com/linux/v6.2.9/source/kernel/locking/semaphore.c#L43) (or one of a few variants); if the semaphore is unavailable, `down()` will wait until some other thread releases it. The release operation, unsurprisingly, is called [`up()`](https://elixir.bootlin.com/linux/v6.2.9/source/kernel/locking/semaphore.c#L176). In the classic literature, as defined by Edsger Dijkstra, those operations are called `P()` and `V()` instead. 

The 0.01 kernel release in 1991 did not have semaphores — or much of any other concurrency-control mechanism, in truth. In the beginning, the kernel only ran on uniprocessor systems and, like most Unix systems at that time, the kernel had exclusive access to the CPU for as long as it chose to run. A process running in the kernel would not be preempted and would continue to execute until it explicitly blocked on an event or returned to user space, so data races were rarely a problem. The one exception was hardware interrupts; to prevent unwanted concurrency from interrupts, the code was liberally sprinkled with `cli()` and `sti()` calls to block (and unblock) interrupts where needed. 

In May 1992, the 0.96 release brought a number of significant changes, including some initial "networking" support; it enabled Unix-domain sockets using a Linux-specific `socketcall()` system call. Perhaps most significant in this release, though, was the addition of support for SCSI devices; good SCSI support would be a key factor during the early adoption phase of Linux. The SCSI subsystem brought with it the first mention of semaphores in the kernel, buried deep down within the driver layer. Like many that would follow, SCSI semaphores were _binary_ semaphores, meaning that their initial value was set to one, allowing only a single thread to have access to the resource (a SCSI host controller) that it managed. The 0.99.10 release in June 1993 brought a reimplemented network layer and support for System V semaphores in user space, but still no general support for semaphores in the kernel. 

#### The addition of semaphores

The first implementation of general-purpose semaphores for the kernel itself showed up in the 0.99.15c release in February 1994. The initial user was the virtual filesystem layer, which added a semaphore to [the `inode` structure](https://elixir.bootlin.com/linux/0.99.15c/source/include/linux/fs.h#L160); no other users had been added by the 1.0 release one month later. The 2.0 release (June 1996) saw a slow growth in the number of semaphores, as well as the addition of the infamous big kernel lock (BKL), which was not a semaphore. 

That was the beginning of SMP support and, even then, kernel code ran under the BKL by default, so most kernel code was limited in the amount of concurrency it had to deal with. In essence, the BKL existed so that kernel code could run under the same assumption of exclusive access to the CPU that had been wired deeply into the code since the beginning; it only allowed one CPU to be running in the kernel at any given time. So disabling interrupts was still by far the most common concurrency-control mechanism in use in the kernel. 

By the 2.2 release (January 1999), there were 71 `struct semaphore` declarations in the kernel; by 2.4.0 (January 2001) that number had grown to 138, and by 2.6.0 (December 2003) it was 332. The 2.6.14 release, in October 2005, had 483 semaphore declarations. By this time, disabling interrupts was falling out of favor as a way to control concurrency — the cost on system performance as a whole was simply too high — and the big kernel lock had become a scalability problem in its own right. 

Meanwhile, the first spinlock infrastructure was added in the 2.1.23 development kernel, though it was not really used until a spinlock was added to the scheduler in 2.1.30. Unlike a semaphore, a spinlock is a pure mutual-exclusion primitive, without a semaphore's count. It also is a non-sleeping lock; code waiting for a spinlock would simply "spin" in a tight loop until the lock became available. Until this addition, semaphores had been the only generalized mutual-exclusion mechanism supported by the kernel. 

Spinlocks were better than semaphores for many situations, but they came with the restriction that code holding a spinlock is not allowed to sleep; that meant that there was still a need for a semaphore-like structure. Around the end of 2005, though, developers [started thinking](/Articles/163842/) that a better solution might exist for the binary-semaphore case — which was how most semaphores were used. An initial "mutex" implementation turned out to perform worse than semaphores did but, as happened frequently in that era, Ingo Molnar [showed up](/Articles/165039/) with a faster alternative within days. Mutexes were soon added to the kernel as an alternative to semaphores, and the process of converting semaphores to mutexes began. 

#### A slow transition

When mutexes were introduced, developers worried that they would force a flag-day change where all binary semaphores would be changed over to the new type. But mutexes were added alongside the old type, allowing the two to coexist and code to be converted at leisure. As a result, unsurprisingly, there are still over 100 semaphores declared in the kernel, the bulk of which appear to be binary semaphores. It is hard to find patches that add new semaphores, though; the most recent would appear to be [this driver patch](https://git.kernel.org/linus/63fbae0a74c3) in August 2022. Most kernel developers, it seems, have no reason to think about semaphores much of the time. 

Modules maintainer Luis Chamberlain has recently been working on a problem where the arrival of a large number of requests to load modules in a short time can create difficulties for the memory-management subsystem. After some discussion, he posted [a proposal](/ml/linux-kernel/ZB4BP0ZgxNirBNOJ@bombadil.infradead.org/) for a mechanism that would simply limit the number of module-load operations that can be underway at any given time. Linus Torvalds quickly [answered](/ml/linux-kernel/CAHk-=whkj6=wyi201JXkw9iT_eTUTsSx+Yb9d4OgmZFjDJA18g@mail.gmail.com/), reminding Chamberlain that semaphores (""a *classic* concurrency limiter"") could be used for that purpose. The patch has since [been reworked](/ml/linux-kernel/20230329053149.3976378-5-mcgrof@kernel.org/) along those lines. 

As part of the associated discussion, though, Peter Zijlstra [noted](/ml/linux-kernel/20230329091935.GP4253@hirez.programming.kicks-ass.net/) that [the `DEFINE_SEMAPHORE()` macro](https://elixir.bootlin.com/linux/v6.3-rc5/source/include/linux/semaphore.h#L21), which declares and initializes a static semaphore, sets the initial value to one, creating a binary semaphore by default. Since, as he said, ""binary semaphores are a special case"", it would have been better to have `DEFINE_SEMAPHORE()` take an additional argument to specify what the initial value should be. Torvalds [agreed](/ml/linux-kernel/CAHk-=whF6Ta_KcJP2eC78+Mstv+vAku8ATRMbv98sf9VhdvySQ@mail.gmail.com/) that this change would make sense: ""So let's just make it clear that the only reason to use semaphores these days is for counting semaphores, and just make DEFINE_SEMAPHORE() take the number."" Semaphores, he said, are now ""almost entirely a legacy thing"". Zijlstra has since [posted a patch](/ml/linux-kernel/20230330115626.GA124812@hirez.programming.kicks-ass.net/) to that effect. 

This minor change to the semaphore API is not likely to affect too many developers. There is still, though, the open question of the dozens of binary semaphores still in use. There would be value in converting them over to mutexes; the performance would be better, and the resulting code would look more familiar to current developers. As Sergey Senozhatsky [pointed out](/ml/linux-kernel/20230331034209.GA12892@google.com/), though, it is not possible to mechanically convert those users without taking a close look. There is, for example, a binary semaphore that persists in the `printk()` code because `mutex_unlock()` cannot be called from interrupt context, while `up()` can. 

It just goes to show that in the kernel, as elsewhere, old code can persist for a long time. The use of binary semaphores was arguably outmoded in 2006, but many uses remain and it took until 2023 to change the initializer to not create a binary semaphore by default. Kernel developers may come and go, but kernel code, at least sometimes, can stay around for a lot longer.  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Locking mechanisms/Semaphores](/Kernel/Index#Locking_mechanisms-Semaphores)  
[Kernel](/Kernel/Index)| [Semaphores](/Kernel/Index#Semaphores)  
  


* * *

to post comments 
