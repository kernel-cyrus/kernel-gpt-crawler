# 3.0 and RCU: what went wrong [LWN.net]

> **Did you know...?**
> 
> LWN.net is a subscriber-supported publication; we rely on subscribers to keep the entire operation going. Please help out by [buying a subscription](/Promo/nst-nag4/subscribe) and keeping LWN on the net. 

July 27, 2011

This article was contributed by Paul McKenney

My goal has always been for my code to go in without so much as a ripple. Although I don't always meet that goal, I can't recall any recent failure quite as spectacular as [RCU in v3.0](http://lwn.net/Articles/452269/). My v3.0 code didn't just cause a few ripples, it bellyflopped. It is therefore worthwhile to review what happened and why it happened in order to avoid future bellyflops and trainwrecks. 

This post-mortem will cover the following topics: 

  1. Overview of preemptible RCU read-side code
  2. Steaming towards the trainwreck
  3. Fixes
  4. Current status
  5. Preventing future bellyflops and trainwrecks

It will end with the obligatory answers to the quick quizzes. 

####  Overview of preemptible RCU read-side code

Understanding the trainwreck requires reviewing a small amount of `TREE_PREEMPT_RCU`'s read-side code. First, let's look at `__rcu_read_lock()`, which, in preemptible RCU, does the real work for `rcu_read_lock()`: 
    
    
      1 void __rcu_read_lock(void)
      2 {
      3   current->rcu_read_lock_nesting++;
      4   barrier();
      5 }
    

This is quite straightforward: line 3 increments the per-task `->rcu_read_lock_nesting` counter and line 4 ensures that the compiler does not bleed code from the following RCU read-side critical section out before the `__rcu_read_lock()`. In short, `__rcu_read_lock()` does nothing more than to increment a nesting-level counter. 

The `__rcu_read_unlock()` function, which, in preemptible RCU, does the real work for `rcu_read_unlock()`, is only slightly more complex: 
    
    
      1 void __rcu_read_unlock(void)
      2 {
      3   struct task_struct *t = current;
      4 
      5   barrier();
      6   --t->rcu_read_lock_nesting;
      7   barrier();
      8   if (t->rcu_read_lock_nesting == 0 &&
      9       unlikely(ACCESS_ONCE(t->rcu_read_unlock_special)))
     10     rcu_read_unlock_special(t);
     11 }
    

Line 5 prevents the compiler from bleeding code from the RCU read-side critical section out past the `__rcu_read_unlock()`, line 6 decrements the per-task nesting-level counter, so that thus far `__rcu_read_unlock()` is the inverse of `__rcu_read_lock()`. 

However, if the value of the nesting counter is now zero, we now need to check to see if anything unusual happened during the just-ended RCU read-side critical section, which is the job of lines 8 and 9. Line 7 prevents the compiler from moving this check to precede the decrement on line 6 because otherwise something unusual might happen just after the check but before the decrement, which would in turn mean that `__rcu_read_unlock()` would fail to clean up after that unusual something. The "unusual somethings" are: 

  1. The RCU read-side critical section might have blocked or been preempted. In this case, the per-task `->rcu_read_unlock_special` variable will have the `RCU_READ_UNLOCK_BLOCKED` bit set. 

  2. The RCU read-side critical section might have executed for more than a jiffy or two. In this case, the per-task `->rcu_read_unlock_special` variable will have the `RCU_READ_UNLOCK_NEED_QS` bit set. 




In either case, the per-task `->rcu_read_unlock_special` will be non-zero, so that `__rcu_read_unlock()` will invoke `rcu_read_unlock_special()`, which we look at next: 
    
    
      1 static void rcu_read_unlock_special(struct task_struct *t)
      2 {
      3   int empty;
      4   int empty_exp;
      5   unsigned long flags;
      6   struct rcu_node *rnp;
      7   int special;
      8 
      9   if (in_nmi())
     10     return;
     11   local_irq_save(flags);
     12   special = t->rcu_read_unlock_special;
     13   if (special & RCU_READ_UNLOCK_NEED_QS) {
     14     rcu_preempt_qs(smp_processor_id());
     15   }
     16   if (in_irq()) {
     17     local_irq_restore(flags);
     18     return;
     19   }
     20   if (special & RCU_READ_UNLOCK_BLOCKED) {
     21     t->rcu_read_unlock_special &= ~RCU_READ_UNLOCK_BLOCKED;
     22 
     23     /* Clean up after blocking. */
     24 
     25   }
     26 }
    

Lines 9 and 10 take an early exit if we are executing in non-maskable interrupt (NMI) context. The reason for this early exit is that NMIs cannot be interrupted or preempted, so there should be no `rcu_read_unlock_special()` processing required. Otherwise, line 11 disables interrupts and line 12 takes a snapshot of the per-task `->rcu_read_unlock_special` variable. Line 13 then checks to see if the just-ended RCU read-side critical section ran for too long, and, if so, invokes `rcu_preempt_qs()` to immediately record a quiescent state. Recall that any point in the code that is not in an RCU read-side critical section is potentially a quiescent state. Therefore, since someone is waiting, report the quiescent state immediately. 

Lines 16 through 18 take an early exit if we are executing in a hardware interrupt handler. This is appropriate given that hardware interrupt handlers cannot block, so it is not possible to preempt or to block within an RCU read-side critical section running within a hardware interrupt handler. (Of course, threaded interrupt handlers are another story altogether.) 

Finally, line 20 checks to see if we blocked or were preempted within the just-ended RCU read-side critical section, clearing the corresponding bit and cleaning up after blockage or preemption if so. The exact details of the cleanup are not important (and are therefore omitted from the code fragment above), although curious readers are referred to `kernel.org`. The important thing is what happens if this RCU read-side critical section was the last one blocking an expedited RCU grace period or if the just-ended RCU read-side critical section was priority-boosted. Either situation requires that RCU interact with the scheduler, which may require the scheduler to acquire its runqueue and priority-inheritance locks. 

Because the scheduler disables interrupts when acquiring the runqueue and the priority-inheritance locks, an RCU read-side critical section that lies entirely within one of these locks' critical sections cannot be interrupted, preempted, or blocked. Therefore, such an RCU read-side critical section should not enter `rcu_read_unlock_special()`, and should thus avoid what would otherwise be an obvious self-deadlock scenario. 

**Quick Quiz 1** : But what about RCU read-side critical sections that begin before a runqueue lock is acquired and end within that lock's critical section? Answer

As we will see later, a number of self-deadlock scenarios can be avoided via the `in_irq()` early exit from `rcu_read_unlock_special()`. Keep the critical importance of this early exit firmly in mind as we steam down the tracks towards the RCU/scheduler/threaded-irq trainwreck. 

####  Steaming towards the trainwreck

Before we leave the station, please keep in mind that `in_irq()` can return inaccurate results because it consults the `preempt_count()` bitmask, which is updated in software. At the start of the interrupt, there is therefore a period of time before `preempt_count()` is updated to record the start of the interrupt, during which time the interrupt handler has started executing, but `in_irq()` returns false. Similarly, at the end of the interrupt, there is a period of time after `preempt_count()` is updated to record the end of the interrupt, during which time the interrupt handler has not completed executing, but again `in_irq()` returns false. This last is most emphatically the case when the end-of-interrupt processing kicks off softirq handling. 

With that background, the sequence of commits leading to the trainwreck is as follows: 

  1. In March of 2009, [commit `a18b83b7ef`](http://git.kernel.org/?p=linux/kernel/git/torvalds/linux-2.6.git;a=commitdiff;h=a18b83b7ef) added the first known `rcu_read_unlock()` to be called while holding a runqueue lock. 

**Quick Quiz 2** : Suppose that an RCU read-side critical section is enclosed within a runqueue-lock critical section. Why couldn't that RCU read-side critical section be the last RCU read-side critical section blocking a `TREE_PREEMPT_RCU` expedited grace period? Answer

**Quick Quiz 3** : Why can't we avoid this whole mess by treating interrupt-disabled segments of code as if they were RCU read-side critical sections? Answer



