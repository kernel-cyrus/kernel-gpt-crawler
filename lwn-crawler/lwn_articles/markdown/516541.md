# KS2012: memcg/mm: Shared-memory accounting in memory cgroups [LWN.net]

> **We're bad at marketing**
> 
> We can admit it, marketing is not our strong suit. Our strength is writing the kind of articles that developers, administrators, and free-software supporters depend on to know what is going on in the Linux world. Please [subscribe today](/Promo/nsn-bad/subscribe) to help us keep doing that, and so we don’t have to get good at marketing. 

By **Michael Kerrisk**  
September 17, 2012 

* * *

[2012 Kernel Summit](/Articles/KernelSummit2012/)

During the 2012 Kernel Summit [memcg/mm minisummit](/Articles/516439/), Greg Thelen and Michel Lespinasse led a session on improving shared-memory accounting for memory cgroups. Currently, memory cgroups memory usage accounting is done by charging each page to the first user to touch each page. In practice, this is usually okay, but it is potentially unfair when that page is shared. The process that faults the page could exit, but the cgroup is still charged for the page. Greg described another example where a pair of processes that are implemented as a producer/consumer, with each process running in separate cgroups, may be unfairly charged if they share the data using shared memory. His idea is that both producer and consumer have to touch the memory, and they should be charged equally for the resource. With the current implementation, the process that is charged is the one that first touches the memory—i.e., the producer. He would like to do something about this problem. 

One suggestion was to extend the `madvise()` system call to allow a process to claim ownership of any pages currently mapped in a specified range, but this would solve only some specific subsets of the problem. It may also be required that all future charges in an address range get charged to a specific cgroup, although it was unclear how this could be specified in a sensible fashion. Minimally, using `madvise()` would assume that the data was `mmap()`ed. Greg was asked to post a patch series for comment, and discussion will continue on the mailing list. 

[Next: NUMA scheduling](/Articles/516542/)  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Memory management/Control groups](/Kernel/Index#Memory_management-Control_groups)  
  


* * *

to post comments 
