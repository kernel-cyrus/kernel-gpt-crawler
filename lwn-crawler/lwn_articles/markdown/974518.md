# Two sessions on CXL memory [LWN.net]

> **Benefits for LWN subscribers**
> 
> The primary benefit from [subscribing to LWN](/Promo/nst-nag5/subscribe) is helping to keep us publishing, but, beyond that, subscribers get immediate access to all site content and access to a number of extra site features. Please sign up today! 

By **Jonathan Corbet**  
May 22, 2024 

* * *

[LSFMM+BPF](/Articles/lsfmmbpf2024/)

[Compute Express Link (CXL)](https://en.wikipedia.org/wiki/Compute_Express_Link) is a data-center-oriented memory solution that, according to some in the industry, will yield large cost savings and performance improvements. Others are more skeptical. At the [2024 Linux Storage, Filesystem, Memory-Management and BPF Summit](https://events.linuxfoundation.org/lsfmmbpf/), two sessions covered CXL and how it will be supported in future kernels.   


#### CXL development

The first session, led by Adam Manzanares, covered the kernel's support for CXL in general. He started by saying that CXL is often mentioned in connection with memory tiering, but there is more to it than that. He would like to see more attention given to some of the other CXL-related code, such as the driver layer. CXL development is [using the kernel.org patchwork server](https://patchwork.kernel.org/bundle/cxllinux/cxl-next/) now, so it is easy for interested developers to see where the work stands. 

[![\[Adam Manzanares\]](https://static.lwn.net/images/conf/2024/lsfmm/AdamManzanares-sm.png)](/Articles/974520/) Manzanares would especially like some help from developers with an understanding of the PCI bus. CXL, he said, is a bit of an awkward fit with the PCI core, so some effort is needed to make the pieces work well together. He is also interested in reliability, availability, and serviceability (RAS) issues, and would like to talk with developers from other subsystems with experience in dealing with memory errors. Having a memory controller on a device complicates things, he said. He wondered why the CXL code does its own event handling rather than using the existing [error detection and correction (EDAC)](https://docs.kernel.org/driver-api/edac.html) code. 

Dan Williams answered that EDAC was invented to abstract the information about the memory controller; CXL is a standardization of that abstraction. So, in the future, the kernel will only need to understand CXL rather than EDAC; and other vendors will find themselves having to make their devices look more like CXL. He has been working on translating CXL events into the EDAC subsystem, which [RAS Daemon](https://github.com/mchehab/rasdaemon), which is used to collect and report on error notifications, knows how to deal with. RAS Daemon may be a legacy tool, but there is value in its ability to handle errors; there is, however, no desire to modify it to handle a new interface. 

Hannes Reinecke pointed out that RAS Daemon is running _in memory_ ; what happens if a memory problem affects it? Williams answered that "if it kills the daemon, you lose". The result of the killing of the RAS Daemon will be a machine-check error, Manzanares said. 

Williams said that there is ongoing work in defining a new scrub subsystem that is designed to proactively find memory problems. There is always a tradeoff between scrubbing frequency and performance. Both ACPI and CXL have mechanisms to handle scrubbing; EDAC does too. There are a lot of people independently solving the same problems, he said; it would be better if they worked together. 

Turning to benchmarks, Manzanares said that it would be good to have a general agreement on a few workloads to run for performance measurements. Since he works for a CXL vendor, he said, he might not be the best person to be doing benchmarking; end users are better suited to that sort of task. The [Open Compute Project](https://www.opencompute.org/) might be a good home for this work; the newly formed tiering working group might be another. Williams echoed the need for good benchmarks; touching the memory-management code is hard, and developers never know when they are regressing somebody's workload. 

The session concluded with a note that CXL is moving quickly. Hardware is currently hard to get, which does not make life easier for developers who are trying to support it. It would be good, Manzanares said, to have a central site where developers could report information about specific devices. 

#### CXL compression

Normally, CXL memory is thought of as being voluminous and cheap, but with higher latency than normal DRAM. There is potential for other types of CXL memory as well, though. Presenting remotely, Yiannis Nikolakopoulos described the use of compression within CXL devices and how it might work with Linux. 

In a conventional tiered layout, the top tier of memory lives in the host, while a lower tier is stored on a CXL device. The "densemem" concept extends that design by adding yet another CXL box, adding a third tier to the system. The address space on that box is oversubscribed — the box claims to have more memory than is actually installed. When data is written to that memory, it is compressed by the densemem box and mapped accordingly. The host is charged with managing this space and reacting to notifications about capacity changes; it can configure the size of the address space and the oversubscription factor. 

Making this work requires the addition of a "backpressure" API that will inform the host about how much free space actually remains on the device. There are four watermark levels that can be established, and the host will be interrupted whenever usage passes one of them. The host can respond by delaying writes, but it can also take actions like changing the compression algorithm for better (but presumably slower) compression. The host can also defragment the device, or simply free memory. 

Most of the upstream support for this hardware will run in user space, but there will be some kernel components too. Nikolakopoulos is working on a driver to expose the control knobs and give user space control over the device. 

Davidlohr Bueso asked why developers should care about compression; it seemed to him to be a way to add latency to a technology that is already slow. Manzanares answered that compression is in the Open Compute Project specification; it is a desired feature, and it is not up to the kernel community to fight it. It is, in the end, a cost-saving measure, he said. 

David Hildenbrand said that the only reasonable use for densemem is [zswap](https://docs.kernel.org/admin-guide/mm/zswap.html); the kernel could be configured to use it while avoiding the overhead of `page` structures. The kernel would not have to manage the compression, it could just swap to the device. Williams agreed that it would be like zswap, but it could provide an additional advantage: since it is directly addressable, there would be no need to swap data back in to access it. 

Matthew Wilcox repeated the complaint that CXL already has high latency, and that compression will make it worse. Williams answered that densemem is intended for cold memory; it is better to move that memory there than to swap it out to disk. Wilcox said that the PCI bus is intended for storage, not memory access; he agreed that using PCI-attached CXL memory as a swap device might be workable, though. 

The session wound down at that point with Williams asking CXL vendors for a couple of features from this technology. One current problem is the lack of a good promotion signal — an indication that memory is being accessed and should be moved to faster storage. He also requested an interface to identify the least-compressible pages stored on the device; those could be migrated back to faster memory to free space on the densemem device.  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Compute Express Link (CXL)](/Kernel/Index#Compute_Express_Link_CXL)  
[Conference](/Archives/ConferenceIndex/)| [Storage, Filesystem, Memory-Management and BPF Summit/2024](/Archives/ConferenceIndex/#Storage_Filesystem_Memory-Management_and_BPF_Summit-2024)  
  


* * *

to post comments 
