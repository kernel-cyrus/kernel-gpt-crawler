# I/O scheduling for single-queue devices [LWN.net]

> **Please consider subscribing to LWN**
> 
> Subscriptions are the lifeblood of LWN.net. If you appreciate this content and would like to see more of it, your subscription will help to ensure that LWN continues to thrive. Please visit [this page](/Promo/nst-nag1/subscribe) to join up and keep LWN on the net. 

By **Jonathan Corbet**  
October 12, 2018 

Block I/O performance can be one of the determining factors for the performance of a system as a whole, especially on systems with slower drives. The need to optimize I/O patterns has led to the development of a long series of I/O schedulers over the years; one of the most recent of those is [BFQ](/Articles/674308/), which was merged during the 4.12 development cycle. BFQ incorporates an impressive set of heuristics designed to improve interactive performance, but it has, thus far, seen relatively little uptake in deployed systems. An attempt to make BFQ the default I/O scheduler for some types of storage devices has raised some interesting questions, though, on how such decisions should be made. 

A bit of review for those who haven't been following the block layer closely may be in order. There are two generations of the internal API used between the block layer and the underlying device drivers, which we can call "legacy" and "multiqueue". Unsurprisingly, the legacy API is older, while [the multiqueue API](/Articles/552904/) was first merged in 3.13. The conversion of block drivers to the multiqueue API has been ongoing since then, with the SCSI subsystem only [switching over](https://git.kernel.org/linus/d5038a13eca72fb216c07eb717169092e92284f1), after a false start, in the upcoming 4.19 release. Most of the remaining holdout legacy drivers [will be converted](/ml/linux-block/20181011165909.32615-1-axboe@kernel.dk/) to multiqueue in the near future, at which point the legacy API can be expected to go away. 

Several I/O schedulers exist for the legacy interface but, in practice, only two are in common use: `cfq` for slower drives and `none` for fast, solid-state devices. The multiqueue interface was aimed at fast devices from the outset; it was not able to support an I/O scheduler at all initially. That capability was added later, along with the `mq-deadline` scheduler, which was essentially a forward port of one of the simpler legacy schedulers (`deadline`). BFQ, which came later, is also a multiqueue-API scheduler. 

In early October Linus Walleij [posted a patch](/ml/linux-block/20181002124329.21248-1-linus.walleij@linaro.org/) making BFQ the default I/O scheduler for single-queue devices driven by way of the multiqueue API. The idea of a single-queue multiqueue device may seem a bit contradictory at a first encounter, but one should remember that "multiqueue" refers to the API which, unlike the legacy API, is capable of handling block devices that implement more than one queue in hardware (but does not require multiple queues). As more drivers move to this API, more single-queue devices will be driven using it. In this particular case, Walleij is concerned with SD cards and similar devices, the kind often found in mobile systems. The expectation is that devices with a single hardware queue can be expected to be relatively slow, and that BFQ will extract better performance from those devices. 

The initial [response](/ml/linux-block/05fdbe23-ec01-895f-e67e-abff85c1ece2@kernel.dk/) from block subsystem maintainer Jens Axboe was not entirely positive: ""I think this should just be done with udev rules, and I'd prefer if the distros would lead the way on this"". This approach is not inconsistent with how the kernel tries to do things in general, leaving policy decisions to user space. But, of course, current kernels, by selecting `mq-deadline` for such devices, are already implementing a specific policy. 

There were a few objections made to Axboe's position. Paolo Valente, the creator of BFQ, [asserted](/ml/linux-block/B5321A64-8A1A-42FF-A337-F4524BDA179B@linaro.org/) that almost nobody understands I/O schedulers or how to choose one, so almost everybody will stick with whatever default the system gives them. And `mq-deadline`, as a default, is far worse than BFQ for such devices, he said. Walleij [added](/ml/linux-block/CACRpkdZFeGy2e9OiDYaq_8XwMTjCd+oeGiJ1HXzNifqZyxPkKw@mail.gmail.com/) that there are quite a few systems out there that do not use `udev` at all, so a rule-based approach will not work for them. On embedded systems where `initramfs` is not in use, it's currently not possible to mount the root filesystem using BFQ at all. As an additional practical difficulty, the number of hardware queues provided by a device is currently not made available to `udev`, so it could not effect this particular policy choice in any case (though that would be relatively straightforward to fix). 

Oleksandr Natalenko [was not impressed](/ml/linux-block/1eca41df95ff660eb247a3de666adeb4@natalenko.name/) by the embedded-systems argument; he said that the people building such systems know which I/O scheduler they should use and can build their systems accordingly. Mark Brown [took issue](/ml/linux-block/20181003145150.GC7132@sirena.org.uk/) with that view of things, though: 

That's not an entirely realistic assessment of a lot of practical embedded development - while people *can* go in and tweak things to their heart's content and some will have the time to do that there's a lot of small teams pulling together entire systems who rely fairly heavily on defaults, focusing most of their effort on the bits of code they directly wrote. 

Walleij [echoed](/ml/linux-block/CACRpkdb1YSw3WWpGSqwxORaMSAeemiaP4i6cWJcY5VtADfzorQ@mail.gmail.com/) that view, and added that there have been many times in kernel history where the decision was made to try to do the right thing automatically when possible, without requiring intervention from user space. 

Bart Van Assche, instead, questioned the superiority of the BFQ scheduler. He initially [claimed](/ml/linux-block/1538683746.230807.9.camel@acm.org/) that it would slow down kernel builds (a sure way to prevent your code from being merged), but Valente [challenged](/ml/linux-block/A017CAC7-41E6-4BB3-AC31-5EB524276138@linaro.org/) that assessment. Van Assche's other [concern](/ml/linux-block/20bfa679-3131-e0af-f69d-2fbec32fbced@acm.org/), though, had to do with fast solid-state SATA drives. Once SCSI switches over to the multiqueue API, those drives will show up with a single queue, and will thus be affected by this change. He questioned whether BFQ could be as fast as `mq-deadline` in that situation, but did not present any test results. 

One other potential problem, as [pointed out](/ml/linux-block/BN3PR0401MB164003C967C45F2681F503A7E7E90@BN3PR0401MB1640.namprd04.prod.outlook.com/) by Damien Le Moal, is shingled magnetic recording (SMR) disks, which often require that write operations arrive in a specific order. BFQ does not provide the same ordering guarantees that `mq-deadline` does, so attempts to use it with SMR drives are unlikely to lead to a high level of user satisfaction. Valente [has a plan](/ml/linux-block/44614924-FF19-4C99-B04F-93C032B74395@linaro.org/) for how to support those drives in BFQ, but he acknowledged that they will not work correctly now. 

The discussion wound down without reaching any sort of clear conclusion. It would appear that, before being merged, a patch of this nature would need to gain some additional checks to ensure, at a minimum, that BFQ is not selected for hardware that it cannot schedule properly. No such revision has been posted as of this writing. The proponents of BFQ seem unlikely to give up in the near future, though, so this topic seems like one that can be expected to arise again.  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Block layer/I/O scheduling](/Kernel/Index#Block_layer-IO_scheduling)  
  


* * *

to post comments 
