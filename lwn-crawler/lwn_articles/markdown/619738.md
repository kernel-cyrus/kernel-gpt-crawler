# Transparent huge page reference counting [LWN.net]

By **Jonathan Corbet**  
November 11, 2014 

While most architectures supported by Linux use a 4KB page size, most of them also are able to work with much larger pages, varying from 2MB to 1GB in size. These "huge pages" offer significant performance advantages for many workloads, the biggest of which is usually the reduction of pressure on the translation lookaside buffer which short-circuits the process of turning a virtual address into a physical address. The kernel's [transparent huge pages](/Articles/423584/) (THP) feature enables the use of huge pages without the need for any sort of developer or user intervention. THP suffers from some limitations that prevent it from being used as fully as one might like, though; a [patch set](/Articles/619191/) from Kirill A. Shutemov aims to reduce those limitations, at the cost of making changes to some fairly complex code. 

#### Transparent huge pages

THP works by quietly substituting huge pages into a process's address space when (1) those page are available and (2) it appears that the process would benefit from the change. When Andrea Arcangeli first added this feature to the 2.6.38 kernel, he had a formidable problem to face: there were many places in the kernel's memory-management code that were not prepared to cope with huge pages scattered randomly in a process's address space. Andrea dealt with some of those problems by avoiding them entirely; that is why, for example, page-cache pages (those backed by files on disk) cannot be huge pages in current kernels. 

In many other situations, Andrea placed a call to `split_huge_page()`, a function which breaks a huge page down into its component small pages. Whenever he encountered code that could not cope with a huge page, and that he wasn't able to fix at the time, he put in a `split_huge_page()` call to simply make the huge page go away. These calls have a clear performance cost, since they undo the work that put the huge page into place to begin with. But they reduced the problem to something more tractable; `split_huge_page()` can thus be thought of as a crutch similar to the big kernel lock. It is almost never the optimal solution to the problem, but it is a solution that can be made to work now, deferring a hard problem for a later time. 

> **`$ sudo subscribe today`**
> 
> Subscribe today and elevate your LWN privileges. You’ll have access to all of LWN’s high-quality articles as soon as they’re published, and help support LWN in the process. [Act now](https://lwn.net/Promo/nst-sudo/claim) and you can start with a free trial subscription. 

Over time, some of the `split_huge_page()` calls in the kernel have been replaced with code that can handle huge pages. But they still exist in the page migration code, in the implementation of [kernel samepage merging](/Articles/330589/) (KSM), in the [bad-memory poisoning code](/Articles/348886/), in the implementation of `mprotect()` and `mlock()`, and in the swap code, among other places. Some of these cases are unlikely to change; KSM will probably never be able to merge duplicate pages if it cannot split them out of huge pages. Others might seem just as resistant to change; what does it mean to change the protection of, say, half of a huge page with `mprotect()`? It turns out that this latter case can be optimized, though, as part of a bigger effort to rework and simplify how the management of transparent huge pages and their references counts is done. 

#### PMD- and PTE-level mappings

To understand Kirill's patch set, it is good to remember that huge pages are represented in the kernel as compound pages. Readers who are unfamiliar with how compound pages work may want to take a look at [this article](/Articles/619514/) for some basic background and terminology. 

Kirill's ultimate goal is to enable the use of transparent huge pages with the page cache. Currently, only anonymous pages can be replaced with huge pages, limiting their use to only a fraction of total memory. That is an ambitious goal, and the current patch set does not even try to approach it. Instead, Kirill has worked to simplify the management of transparent huge pages and make them more flexible. 

In particular, he has eliminated the hard separation between normal and huge pages in the system. In current kernels, a specific 4KB page can be treated as an individual page, or it can be part of a huge page, but not both. If a huge page must be split into individual pages, it is split completely for all users, the compound page structure is torn down, and the huge page no longer exists. The fundamental change in Kirill's patch set is to allow a huge page to be split in one process's address space, while remaining a huge page in any other address space where it is found. 

Time for a quick reminder of how page tables are structured on Linux systems; this diagram was taken from [this 2005 LWN article](/Articles/117749/) on the subject: 

> ![\[Page tables\]](https://static.lwn.net/images/ns/kernel/four-level-pt.png)

A huge page is represented in a process's page table with a single entry at the page middle directory (PMD) level. Individual pages, instead, have entries at the bottom page-table entry (PTE) level, as shown in the diagram. But there is nothing that says that the same memory must be mapped in the same way in all processes; it is perfectly legitimate for one process to see a 2MB range as a single huge page while another has it mapped as 512 individual PTEs. If this type of different mapping were supported, one process could call `mprotect()` to change the protections on a portion of a huge page (causing the mapping to be split in that process's address space) while not disturbing the huge page mapping in other processes, which are not affected by the protection change. 

In other words, if `split_huge_page()` could be replaced by a new function, call it `split_huge_pmd()`, that would only split up a single process's mapping of a huge page, code needing to deal with individual pages could often be accommodated while preserving the benefits of the huge page for other processes. But, as noted above, the kernel currently does not support different mappings of huge pages; all processes must map the memory in the same way. This restriction comes down to how various parameters — reference counts in particular — are represented in huge pages. 

#### Huge-page reference counting

A reference count tracks the number of users an object (such as a page in memory) has, allowing the kernel to determine when the object is free and can be deleted. There are actually two types of reference counts for a normal page. The first, stored in the `_count` field of `struct page`, is the total number of references held to the page. The second, kept in `_mapcount`, is the number of page table entries referring to this page. A page-table mapping is a reference, so every such reference counted in `_mapcount` is also tracked in `_count`; the latter should thus always be greater than or equal to the former. Situations where `_count` can exceed `_mapcount` include pages mapped for DMA and pages mapped into the kernel's address space with a function like `get_user_pages()`. Locking a page into memory with `mlock()` will also increase `_count`. The relative value of these two counters is important; if `_count` equals `_mapcount`, the page can be reclaimed by locating and removing all of the page table entries. But if `_count` is greater than `_mapcount`, the page is "pinned" and cannot be reclaimed until the extra references are removed. 

The reference count rules change with compound pages, though. In that case, the value of `_count` is held at zero for all tail pages to avoid confusing other parts of the memory management code. Reference counts are, as a rule, tracked in the head page for the compound page as a whole, but it is still necessary to track references on individual (small) pages within the compound page. Imagine a situation where part of such a page is used for an I/O operation, for example. The trick that is used is to keep that reference in `_mapcount` instead and to have the various helper functions that access reference counts pick the right field depending on whether a given page is a tail page or not. 

This trick works because huge pages are mapped and unmapped as a unit, so there is no need to track the mapping of tail pages separately. If, however, one wants to allow the mapping of individual pages within a huge page, things fall apart, because it will become necessary to track the mappings to those individual pages. So this trick must go; it must be replaced by a scheme that can track both the mappings to the huge page as a whole and the individual pages that make up that huge page. 

The first part, tracking mappings to the huge page as a whole, is done with another increasingly familiar (to those who read the article on compound pages) trick: this count is placed into the `mapping` field of the first _tail_ page in the huge page. This field is normally used to track the file that has been mapped into this page of memory; since huge pages are not used in the page cache, there is no file mapping and this space is available for other uses. The count is an `atomic_t`, while `mapping` is a pointer to `struct address_space`, so yet another cast is used. One might argue that another union field should be added to make the overloading explicit, but that was not done here. 

There is still the matter of tracking non-mapping reference counts to tail pages — the reason `_mapcount` was used in those pages to begin with. This tracking is done so that, should the huge page be split, the individual pages with elevated reference counts can be properly marked. Kirill's approach is to give up on that objective and, in the process, change how non-mapping references to tail pages are tracked. Instead, a call to `get_page()` on a tail page will increment the reference count on the _head_ page. So the knowledge that a specific tail page is pinned is replaced with the knowledge that some page, somewhere within the compound page, is pinned. 

That, naturally, will destroy the ability to mark individual pages as being pinned if the huge page is split. Kirill's answer to that is to simply cause `split_huge_page()` to fail if any pages within the huge page are pinned. Note that it will _not_ cause `split_huge_pmd()`, which just splits the mapping for one process, to fail. So most cases that formerly had to call `split_huge_page()` will still work since the huge page is no longer truly being split. The cases where a split is absolutely necessary will simply fail, but, Kirill says, the code is prepared for that eventuality in all cases. 

Removing tail page reference counting enables the removal of a surprising amount of special-case code. It also frees up `_mapcount` for its original purpose: to track the number of page-table mappings to the page. At that point, it becomes possible for one process to map a huge page as a single page, while another maps it as a set of individual pages. 

Kirill claims that the simplification of the code yields performance improvements on their own, though no benchmark results have been posted. Allowing some processes to retain a huge-page mapping when others need a split view should also make things go faster in cases where memory is shared. This feature should make a bigger difference, though, in the future when THP can be used with the page cache, where sharing of pages happens rather more often. That day will not come soon, though; first this set of patches must find its way into the mainline. Given that the work is being done in low-level memory-management code, and given that Kirill thinks there are probably a few surprises (code that doesn't expect an individual page to be a tail page, for example) yet to be found, that probably is not going to happen in the immediate future.  
Index entries for this article  
---  
[Kernel](/Kernel/Index)| [Huge pages](/Kernel/Index#Huge_pages)  
[Kernel](/Kernel/Index)| [Memory management/Huge pages](/Kernel/Index#Memory_management-Huge_pages)  
  


* * *

to post comments 
